{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"THEANO_FLAGS\"] = 'cuda.root=/usr/local/cuda,floatX=float32,device=gpu1,force_device=False,lib.cnmem=.75'\n",
    "\n",
    "import theano\n",
    "print(theano.config.device)\n",
    "\n",
    "import mhcflurry, seaborn, numpy, pandas, pickle, sklearn, collections, scipy, time\n",
    "import mhcflurry.dataset\n",
    "import fancyimpute, locale\n",
    "\n",
    "import sklearn.metrics\n",
    "import sklearn.cross_validation\n",
    "\n",
    "def print_full(x):\n",
    "    pandas.set_option('display.max_rows', len(x))\n",
    "    print(x)\n",
    "    pandas.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_ic50 = 50000\n",
    "min_peptides_to_consider_allele = 10\n",
    "data_dir = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_train_data = mhcflurry.dataset.Dataset.from_csv(data_dir + \"bdata.2009.mhci.public.1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 12235 peptides with <2 observations\n",
      "Dropping 9 alleles with <10 observations: ['ELA-A1', 'HLA-B2701', 'HLA-B3508', 'HLA-B44', 'HLA-E0101', 'Mamu-B04', 'Patr-A0602', 'Patr-B0901', 'Patr-B1701']\n",
      "[MICE] Completing matrix with shape (19304, 97)\n",
      "[MICE] Starting imputation round 1/30, elapsed time 0.048\n",
      "[MICE] Starting imputation round 2/30, elapsed time 4.002\n",
      "[MICE] Starting imputation round 3/30, elapsed time 7.712\n",
      "[MICE] Starting imputation round 4/30, elapsed time 11.453\n",
      "[MICE] Starting imputation round 5/30, elapsed time 15.168\n",
      "[MICE] Starting imputation round 6/30, elapsed time 18.849\n",
      "[MICE] Starting imputation round 7/30, elapsed time 22.609\n",
      "[MICE] Starting imputation round 8/30, elapsed time 26.438\n",
      "[MICE] Starting imputation round 9/30, elapsed time 30.400\n",
      "[MICE] Starting imputation round 10/30, elapsed time 34.912\n",
      "[MICE] Starting imputation round 11/30, elapsed time 39.564\n",
      "[MICE] Starting imputation round 12/30, elapsed time 47.226\n",
      "[MICE] Starting imputation round 13/30, elapsed time 52.296\n",
      "[MICE] Starting imputation round 14/30, elapsed time 58.151\n",
      "[MICE] Starting imputation round 15/30, elapsed time 62.641\n",
      "[MICE] Starting imputation round 16/30, elapsed time 67.735\n",
      "[MICE] Starting imputation round 17/30, elapsed time 72.156\n",
      "[MICE] Starting imputation round 18/30, elapsed time 77.619\n",
      "[MICE] Starting imputation round 19/30, elapsed time 82.302\n",
      "[MICE] Starting imputation round 20/30, elapsed time 87.681\n",
      "[MICE] Starting imputation round 21/30, elapsed time 91.895\n",
      "[MICE] Starting imputation round 22/30, elapsed time 96.147\n",
      "[MICE] Starting imputation round 23/30, elapsed time 100.867\n",
      "[MICE] Starting imputation round 24/30, elapsed time 104.924\n",
      "[MICE] Starting imputation round 25/30, elapsed time 109.085\n",
      "[MICE] Starting imputation round 26/30, elapsed time 114.314\n",
      "[MICE] Starting imputation round 27/30, elapsed time 119.328\n",
      "[MICE] Starting imputation round 28/30, elapsed time 124.641\n",
      "[MICE] Starting imputation round 29/30, elapsed time 130.772\n",
      "[MICE] Starting imputation round 30/30, elapsed time 136.220\n"
     ]
    }
   ],
   "source": [
    "imputed_train_data = all_train_data.impute_missing_values(\n",
    "    fancyimpute.MICE(n_imputations=25, n_burn_in=5),\n",
    "    min_observations_per_peptide=2,\n",
    "    min_observations_per_allele=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>allele</th>\n",
       "      <th>peptide</th>\n",
       "      <th>length</th>\n",
       "      <th>meas</th>\n",
       "      <th>netmhc</th>\n",
       "      <th>netmhcpan</th>\n",
       "      <th>smmpmbec_cpp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAACNVATA</td>\n",
       "      <td>9</td>\n",
       "      <td>657.657837</td>\n",
       "      <td>154.881662</td>\n",
       "      <td>711.213514</td>\n",
       "      <td>438.530698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAFEFVYV</td>\n",
       "      <td>8</td>\n",
       "      <td>30831.879502</td>\n",
       "      <td>6456.542290</td>\n",
       "      <td>785.235635</td>\n",
       "      <td>10351.421667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAFVNDYSL</td>\n",
       "      <td>9</td>\n",
       "      <td>77.446180</td>\n",
       "      <td>17.458222</td>\n",
       "      <td>7.516229</td>\n",
       "      <td>28.054336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAIANQAAV</td>\n",
       "      <td>9</td>\n",
       "      <td>1.999862</td>\n",
       "      <td>9.638290</td>\n",
       "      <td>9.749896</td>\n",
       "      <td>25.703958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAIANQAVV</td>\n",
       "      <td>9</td>\n",
       "      <td>1.517050</td>\n",
       "      <td>8.550667</td>\n",
       "      <td>8.336812</td>\n",
       "      <td>28.773984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAIENYVRF</td>\n",
       "      <td>9</td>\n",
       "      <td>37.844258</td>\n",
       "      <td>252.348077</td>\n",
       "      <td>114.815362</td>\n",
       "      <td>187.068214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAINFITTM</td>\n",
       "      <td>9</td>\n",
       "      <td>3.155005</td>\n",
       "      <td>199.986187</td>\n",
       "      <td>389.045145</td>\n",
       "      <td>200.909281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAIPAPPPI</td>\n",
       "      <td>9</td>\n",
       "      <td>3243.396173</td>\n",
       "      <td>1059.253725</td>\n",
       "      <td>493.173804</td>\n",
       "      <td>295.120923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAKLNRPPL</td>\n",
       "      <td>9</td>\n",
       "      <td>654.636174</td>\n",
       "      <td>66.374307</td>\n",
       "      <td>77.268059</td>\n",
       "      <td>38.459178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AALDMVDAL</td>\n",
       "      <td>9</td>\n",
       "      <td>229.614865</td>\n",
       "      <td>547.015963</td>\n",
       "      <td>597.035287</td>\n",
       "      <td>225.423921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AALQHLRSI</td>\n",
       "      <td>9</td>\n",
       "      <td>905.732601</td>\n",
       "      <td>1686.553025</td>\n",
       "      <td>2032.357011</td>\n",
       "      <td>698.232404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AALVRLTAL</td>\n",
       "      <td>9</td>\n",
       "      <td>1106.623784</td>\n",
       "      <td>435.511874</td>\n",
       "      <td>214.783047</td>\n",
       "      <td>378.442585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAMVPTGSL</td>\n",
       "      <td>9</td>\n",
       "      <td>1836.538343</td>\n",
       "      <td>4055.085354</td>\n",
       "      <td>5176.068320</td>\n",
       "      <td>1545.254440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AANSPWAPV</td>\n",
       "      <td>9</td>\n",
       "      <td>4325.138310</td>\n",
       "      <td>903.649474</td>\n",
       "      <td>1023.292992</td>\n",
       "      <td>557.185749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAPSGAAPL</td>\n",
       "      <td>9</td>\n",
       "      <td>2.844461</td>\n",
       "      <td>97.948999</td>\n",
       "      <td>501.187234</td>\n",
       "      <td>822.242650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAPVSEPTV</td>\n",
       "      <td>9</td>\n",
       "      <td>106.905488</td>\n",
       "      <td>2654.605562</td>\n",
       "      <td>1918.668741</td>\n",
       "      <td>1870.682140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAVANGFAA</td>\n",
       "      <td>9</td>\n",
       "      <td>83.368118</td>\n",
       "      <td>205.116218</td>\n",
       "      <td>228.559880</td>\n",
       "      <td>200.447203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAVLLGAPV</td>\n",
       "      <td>9</td>\n",
       "      <td>373.250158</td>\n",
       "      <td>320.626932</td>\n",
       "      <td>623.734835</td>\n",
       "      <td>286.417797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAVQLLFPA</td>\n",
       "      <td>9</td>\n",
       "      <td>347.536161</td>\n",
       "      <td>2494.594727</td>\n",
       "      <td>5152.286446</td>\n",
       "      <td>679.203633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAVTAGVAL</td>\n",
       "      <td>9</td>\n",
       "      <td>229.614865</td>\n",
       "      <td>1552.387010</td>\n",
       "      <td>2172.701179</td>\n",
       "      <td>1927.524913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAVVNRSLV</td>\n",
       "      <td>9</td>\n",
       "      <td>8.609938</td>\n",
       "      <td>26.302680</td>\n",
       "      <td>29.580125</td>\n",
       "      <td>63.826349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAYVPADAV</td>\n",
       "      <td>9</td>\n",
       "      <td>331.894458</td>\n",
       "      <td>1757.923614</td>\n",
       "      <td>3206.269325</td>\n",
       "      <td>595.662144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>ACMVNLERL</td>\n",
       "      <td>9</td>\n",
       "      <td>737.904230</td>\n",
       "      <td>239.331576</td>\n",
       "      <td>679.203633</td>\n",
       "      <td>444.631267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>ADLVNHPPV</td>\n",
       "      <td>9</td>\n",
       "      <td>558.470195</td>\n",
       "      <td>1694.337800</td>\n",
       "      <td>1857.804455</td>\n",
       "      <td>325.087297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AEMEEALKG</td>\n",
       "      <td>9</td>\n",
       "      <td>78523.563461</td>\n",
       "      <td>41304.750199</td>\n",
       "      <td>46989.410861</td>\n",
       "      <td>440554.863507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AFFAFRYV</td>\n",
       "      <td>8</td>\n",
       "      <td>87902.251683</td>\n",
       "      <td>18967.059212</td>\n",
       "      <td>12882.495517</td>\n",
       "      <td>7816.278046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AGLDNKFYL</td>\n",
       "      <td>9</td>\n",
       "      <td>659.173895</td>\n",
       "      <td>357.272838</td>\n",
       "      <td>202.768272</td>\n",
       "      <td>260.015956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AGLLFVLL</td>\n",
       "      <td>8</td>\n",
       "      <td>87902.251683</td>\n",
       "      <td>14996.848355</td>\n",
       "      <td>9885.530947</td>\n",
       "      <td>26242.185434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AGLQVINL</td>\n",
       "      <td>8</td>\n",
       "      <td>59429.215862</td>\n",
       "      <td>25061.092530</td>\n",
       "      <td>12560.299637</td>\n",
       "      <td>35399.734108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AGLVSFNFL</td>\n",
       "      <td>9</td>\n",
       "      <td>210.862815</td>\n",
       "      <td>620.869034</td>\n",
       "      <td>2285.598803</td>\n",
       "      <td>1811.340093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27650</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YNTVCVIW</td>\n",
       "      <td>8</td>\n",
       "      <td>909.913273</td>\n",
       "      <td>734.513868</td>\n",
       "      <td>10023.052381</td>\n",
       "      <td>48.865236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27651</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YRHDGGNVL</td>\n",
       "      <td>9</td>\n",
       "      <td>78342.964277</td>\n",
       "      <td>1355.189412</td>\n",
       "      <td>22233.098907</td>\n",
       "      <td>1409.288798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27652</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YSEGQYMNTP</td>\n",
       "      <td>10</td>\n",
       "      <td>7362.070975</td>\n",
       "      <td>2958.012467</td>\n",
       "      <td>8709.635900</td>\n",
       "      <td>4477.133042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27653</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YSEVALNVTES</td>\n",
       "      <td>11</td>\n",
       "      <td>7709.034691</td>\n",
       "      <td>1682.674061</td>\n",
       "      <td>5105.050000</td>\n",
       "      <td>51.522864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27654</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YSLVFVILM</td>\n",
       "      <td>9</td>\n",
       "      <td>1256.029964</td>\n",
       "      <td>17.498467</td>\n",
       "      <td>49.431069</td>\n",
       "      <td>6.324119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27655</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YSQIGAGVY</td>\n",
       "      <td>9</td>\n",
       "      <td>1.999862</td>\n",
       "      <td>17.864876</td>\n",
       "      <td>15.346170</td>\n",
       "      <td>16.292960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27656</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YSRDLICEQS</td>\n",
       "      <td>10</td>\n",
       "      <td>1059.253725</td>\n",
       "      <td>1297.179271</td>\n",
       "      <td>3863.669771</td>\n",
       "      <td>220.292646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27657</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YSYKAFIKY</td>\n",
       "      <td>9</td>\n",
       "      <td>2.317395</td>\n",
       "      <td>4.285485</td>\n",
       "      <td>3.090295</td>\n",
       "      <td>1.778279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27658</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YSYKAFIKYP</td>\n",
       "      <td>10</td>\n",
       "      <td>4830.588020</td>\n",
       "      <td>1199.499303</td>\n",
       "      <td>1753.880502</td>\n",
       "      <td>3111.716337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27659</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YSYKAFIKYPE</td>\n",
       "      <td>11</td>\n",
       "      <td>4897.788194</td>\n",
       "      <td>671.428853</td>\n",
       "      <td>2666.858665</td>\n",
       "      <td>42.559841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27660</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTAFTLPSV</td>\n",
       "      <td>9</td>\n",
       "      <td>101.624869</td>\n",
       "      <td>32.210688</td>\n",
       "      <td>52.844525</td>\n",
       "      <td>91.833260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27661</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTAFTLPSVN</td>\n",
       "      <td>10</td>\n",
       "      <td>2529.297996</td>\n",
       "      <td>236.591970</td>\n",
       "      <td>1406.047524</td>\n",
       "      <td>14.487719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27662</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTDGSCNKQS</td>\n",
       "      <td>10</td>\n",
       "      <td>11091.748153</td>\n",
       "      <td>4345.102242</td>\n",
       "      <td>11246.049740</td>\n",
       "      <td>588.843655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27663</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTGDFDSVI</td>\n",
       "      <td>9</td>\n",
       "      <td>215.278173</td>\n",
       "      <td>354.813389</td>\n",
       "      <td>42.461956</td>\n",
       "      <td>40.550854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27664</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTPKVVGG</td>\n",
       "      <td>8</td>\n",
       "      <td>70145.529842</td>\n",
       "      <td>17947.336268</td>\n",
       "      <td>13995.873226</td>\n",
       "      <td>3140.508694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27665</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTPKVVGGIGG</td>\n",
       "      <td>11</td>\n",
       "      <td>70145.529842</td>\n",
       "      <td>9418.895965</td>\n",
       "      <td>10046.157903</td>\n",
       "      <td>387.257645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27666</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTSGPGIR</td>\n",
       "      <td>8</td>\n",
       "      <td>16865.530254</td>\n",
       "      <td>619.441075</td>\n",
       "      <td>2349.632821</td>\n",
       "      <td>80.723503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27667</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTSGPGIRYP</td>\n",
       "      <td>10</td>\n",
       "      <td>7128.530301</td>\n",
       "      <td>783.429643</td>\n",
       "      <td>5176.068320</td>\n",
       "      <td>1270.574105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27668</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTSGPGTRYP</td>\n",
       "      <td>10</td>\n",
       "      <td>3097.419299</td>\n",
       "      <td>483.058802</td>\n",
       "      <td>5420.008904</td>\n",
       "      <td>797.994687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27669</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTSGPGTRYPM</td>\n",
       "      <td>11</td>\n",
       "      <td>25.941794</td>\n",
       "      <td>39.174188</td>\n",
       "      <td>7.744618</td>\n",
       "      <td>76.383578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27670</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTTGGTSR</td>\n",
       "      <td>8</td>\n",
       "      <td>70145.529842</td>\n",
       "      <td>357.272838</td>\n",
       "      <td>5701.642723</td>\n",
       "      <td>100.230524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27671</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTTGGTSRN</td>\n",
       "      <td>9</td>\n",
       "      <td>36897.759857</td>\n",
       "      <td>186.637969</td>\n",
       "      <td>6025.595861</td>\n",
       "      <td>119.674053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27672</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTTTGASRN</td>\n",
       "      <td>9</td>\n",
       "      <td>587.489353</td>\n",
       "      <td>135.518941</td>\n",
       "      <td>2844.461107</td>\n",
       "      <td>87.096359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27673</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTYEAYVRYP</td>\n",
       "      <td>10</td>\n",
       "      <td>2624.218543</td>\n",
       "      <td>1552.387010</td>\n",
       "      <td>3872.576449</td>\n",
       "      <td>1954.339456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27674</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTYEAYVRYPE</td>\n",
       "      <td>11</td>\n",
       "      <td>1905.460718</td>\n",
       "      <td>864.967919</td>\n",
       "      <td>6998.419960</td>\n",
       "      <td>32.734069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27675</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YVADALAAF</td>\n",
       "      <td>9</td>\n",
       "      <td>15.381546</td>\n",
       "      <td>453.941617</td>\n",
       "      <td>71.285303</td>\n",
       "      <td>108.143395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27676</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YVFPVIFSK</td>\n",
       "      <td>9</td>\n",
       "      <td>39264.493540</td>\n",
       "      <td>2098.939884</td>\n",
       "      <td>5610.479760</td>\n",
       "      <td>901.571138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27677</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YVFPVIFSR</td>\n",
       "      <td>9</td>\n",
       "      <td>36728.230050</td>\n",
       "      <td>2333.458062</td>\n",
       "      <td>10046.157903</td>\n",
       "      <td>2600.159563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27678</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YVPCHIRQI</td>\n",
       "      <td>9</td>\n",
       "      <td>10764.652136</td>\n",
       "      <td>21134.890398</td>\n",
       "      <td>6039.486294</td>\n",
       "      <td>10568.175092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27679</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YVVQMLARL</td>\n",
       "      <td>9</td>\n",
       "      <td>152.405275</td>\n",
       "      <td>232.273680</td>\n",
       "      <td>739.605275</td>\n",
       "      <td>105.681751</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27680 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         allele      peptide  length          meas        netmhc  \\\n",
       "0        H-2-DB    AAACNVATA       9    657.657837    154.881662   \n",
       "1        H-2-DB     AAFEFVYV       8  30831.879502   6456.542290   \n",
       "2        H-2-DB    AAFVNDYSL       9     77.446180     17.458222   \n",
       "3        H-2-DB    AAIANQAAV       9      1.999862      9.638290   \n",
       "4        H-2-DB    AAIANQAVV       9      1.517050      8.550667   \n",
       "5        H-2-DB    AAIENYVRF       9     37.844258    252.348077   \n",
       "6        H-2-DB    AAINFITTM       9      3.155005    199.986187   \n",
       "7        H-2-DB    AAIPAPPPI       9   3243.396173   1059.253725   \n",
       "8        H-2-DB    AAKLNRPPL       9    654.636174     66.374307   \n",
       "9        H-2-DB    AALDMVDAL       9    229.614865    547.015963   \n",
       "10       H-2-DB    AALQHLRSI       9    905.732601   1686.553025   \n",
       "11       H-2-DB    AALVRLTAL       9   1106.623784    435.511874   \n",
       "12       H-2-DB    AAMVPTGSL       9   1836.538343   4055.085354   \n",
       "13       H-2-DB    AANSPWAPV       9   4325.138310    903.649474   \n",
       "14       H-2-DB    AAPSGAAPL       9      2.844461     97.948999   \n",
       "15       H-2-DB    AAPVSEPTV       9    106.905488   2654.605562   \n",
       "16       H-2-DB    AAVANGFAA       9     83.368118    205.116218   \n",
       "17       H-2-DB    AAVLLGAPV       9    373.250158    320.626932   \n",
       "18       H-2-DB    AAVQLLFPA       9    347.536161   2494.594727   \n",
       "19       H-2-DB    AAVTAGVAL       9    229.614865   1552.387010   \n",
       "20       H-2-DB    AAVVNRSLV       9      8.609938     26.302680   \n",
       "21       H-2-DB    AAYVPADAV       9    331.894458   1757.923614   \n",
       "22       H-2-DB    ACMVNLERL       9    737.904230    239.331576   \n",
       "23       H-2-DB    ADLVNHPPV       9    558.470195   1694.337800   \n",
       "24       H-2-DB    AEMEEALKG       9  78523.563461  41304.750199   \n",
       "25       H-2-DB     AFFAFRYV       8  87902.251683  18967.059212   \n",
       "26       H-2-DB    AGLDNKFYL       9    659.173895    357.272838   \n",
       "27       H-2-DB     AGLLFVLL       8  87902.251683  14996.848355   \n",
       "28       H-2-DB     AGLQVINL       8  59429.215862  25061.092530   \n",
       "29       H-2-DB    AGLVSFNFL       9    210.862815    620.869034   \n",
       "...         ...          ...     ...           ...           ...   \n",
       "27650  Mamu-A02     YNTVCVIW       8    909.913273    734.513868   \n",
       "27651  Mamu-A02    YRHDGGNVL       9  78342.964277   1355.189412   \n",
       "27652  Mamu-A02   YSEGQYMNTP      10   7362.070975   2958.012467   \n",
       "27653  Mamu-A02  YSEVALNVTES      11   7709.034691   1682.674061   \n",
       "27654  Mamu-A02    YSLVFVILM       9   1256.029964     17.498467   \n",
       "27655  Mamu-A02    YSQIGAGVY       9      1.999862     17.864876   \n",
       "27656  Mamu-A02   YSRDLICEQS      10   1059.253725   1297.179271   \n",
       "27657  Mamu-A02    YSYKAFIKY       9      2.317395      4.285485   \n",
       "27658  Mamu-A02   YSYKAFIKYP      10   4830.588020   1199.499303   \n",
       "27659  Mamu-A02  YSYKAFIKYPE      11   4897.788194    671.428853   \n",
       "27660  Mamu-A02    YTAFTLPSV       9    101.624869     32.210688   \n",
       "27661  Mamu-A02   YTAFTLPSVN      10   2529.297996    236.591970   \n",
       "27662  Mamu-A02   YTDGSCNKQS      10  11091.748153   4345.102242   \n",
       "27663  Mamu-A02    YTGDFDSVI       9    215.278173    354.813389   \n",
       "27664  Mamu-A02     YTPKVVGG       8  70145.529842  17947.336268   \n",
       "27665  Mamu-A02  YTPKVVGGIGG      11  70145.529842   9418.895965   \n",
       "27666  Mamu-A02     YTSGPGIR       8  16865.530254    619.441075   \n",
       "27667  Mamu-A02   YTSGPGIRYP      10   7128.530301    783.429643   \n",
       "27668  Mamu-A02   YTSGPGTRYP      10   3097.419299    483.058802   \n",
       "27669  Mamu-A02  YTSGPGTRYPM      11     25.941794     39.174188   \n",
       "27670  Mamu-A02     YTTGGTSR       8  70145.529842    357.272838   \n",
       "27671  Mamu-A02    YTTGGTSRN       9  36897.759857    186.637969   \n",
       "27672  Mamu-A02    YTTTGASRN       9    587.489353    135.518941   \n",
       "27673  Mamu-A02   YTYEAYVRYP      10   2624.218543   1552.387010   \n",
       "27674  Mamu-A02  YTYEAYVRYPE      11   1905.460718    864.967919   \n",
       "27675  Mamu-A02    YVADALAAF       9     15.381546    453.941617   \n",
       "27676  Mamu-A02    YVFPVIFSK       9  39264.493540   2098.939884   \n",
       "27677  Mamu-A02    YVFPVIFSR       9  36728.230050   2333.458062   \n",
       "27678  Mamu-A02    YVPCHIRQI       9  10764.652136  21134.890398   \n",
       "27679  Mamu-A02    YVVQMLARL       9    152.405275    232.273680   \n",
       "\n",
       "          netmhcpan   smmpmbec_cpp  \n",
       "0        711.213514     438.530698  \n",
       "1        785.235635   10351.421667  \n",
       "2          7.516229      28.054336  \n",
       "3          9.749896      25.703958  \n",
       "4          8.336812      28.773984  \n",
       "5        114.815362     187.068214  \n",
       "6        389.045145     200.909281  \n",
       "7        493.173804     295.120923  \n",
       "8         77.268059      38.459178  \n",
       "9        597.035287     225.423921  \n",
       "10      2032.357011     698.232404  \n",
       "11       214.783047     378.442585  \n",
       "12      5176.068320    1545.254440  \n",
       "13      1023.292992     557.185749  \n",
       "14       501.187234     822.242650  \n",
       "15      1918.668741    1870.682140  \n",
       "16       228.559880     200.447203  \n",
       "17       623.734835     286.417797  \n",
       "18      5152.286446     679.203633  \n",
       "19      2172.701179    1927.524913  \n",
       "20        29.580125      63.826349  \n",
       "21      3206.269325     595.662144  \n",
       "22       679.203633     444.631267  \n",
       "23      1857.804455     325.087297  \n",
       "24     46989.410861  440554.863507  \n",
       "25     12882.495517    7816.278046  \n",
       "26       202.768272     260.015956  \n",
       "27      9885.530947   26242.185434  \n",
       "28     12560.299637   35399.734108  \n",
       "29      2285.598803    1811.340093  \n",
       "...             ...            ...  \n",
       "27650  10023.052381      48.865236  \n",
       "27651  22233.098907    1409.288798  \n",
       "27652   8709.635900    4477.133042  \n",
       "27653   5105.050000      51.522864  \n",
       "27654     49.431069       6.324119  \n",
       "27655     15.346170      16.292960  \n",
       "27656   3863.669771     220.292646  \n",
       "27657      3.090295       1.778279  \n",
       "27658   1753.880502    3111.716337  \n",
       "27659   2666.858665      42.559841  \n",
       "27660     52.844525      91.833260  \n",
       "27661   1406.047524      14.487719  \n",
       "27662  11246.049740     588.843655  \n",
       "27663     42.461956      40.550854  \n",
       "27664  13995.873226    3140.508694  \n",
       "27665  10046.157903     387.257645  \n",
       "27666   2349.632821      80.723503  \n",
       "27667   5176.068320    1270.574105  \n",
       "27668   5420.008904     797.994687  \n",
       "27669      7.744618      76.383578  \n",
       "27670   5701.642723     100.230524  \n",
       "27671   6025.595861     119.674053  \n",
       "27672   2844.461107      87.096359  \n",
       "27673   3872.576449    1954.339456  \n",
       "27674   6998.419960      32.734069  \n",
       "27675     71.285303     108.143395  \n",
       "27676   5610.479760     901.571138  \n",
       "27677  10046.157903    2600.159563  \n",
       "27678   6039.486294   10568.175092  \n",
       "27679    739.605275     105.681751  \n",
       "\n",
       "[27680 rows x 7 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df = pandas.read_csv(\"../data/combined_test_BLIND_dataset_from_kim2013.csv\")\n",
    "validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HLA-A0201     9565\n",
       "HLA-A0301     6141\n",
       "HLA-A0203     5542\n",
       "HLA-A1101     5399\n",
       "HLA-A0206     4827\n",
       "HLA-A3101     4796\n",
       "HLA-A6802     4768\n",
       "HLA-A0202     3919\n",
       "HLA-A0101     3725\n",
       "HLA-B0702     3412\n",
       "H-2-KB        3407\n",
       "H-2-DB        3216\n",
       "HLA-B1501     3213\n",
       "HLA-A6801     3184\n",
       "HLA-A3301     3040\n",
       "HLA-B2705     3028\n",
       "HLA-A2601     2894\n",
       "HLA-B4001     2718\n",
       "HLA-B5801     2564\n",
       "HLA-A2402     2533\n",
       "HLA-B3501     2397\n",
       "HLA-A2902     2397\n",
       "HLA-B0801     2267\n",
       "Mamu-A01      2264\n",
       "HLA-A6901     2079\n",
       "HLA-B1801     2052\n",
       "HLA-A3001     2040\n",
       "HLA-A2301     2021\n",
       "HLA-B5701     1857\n",
       "HLA-B5101     1734\n",
       "              ... \n",
       "HLA-B3801      136\n",
       "HLA-A0250      132\n",
       "HLA-B7301      115\n",
       "HLA-A11         74\n",
       "HLA-A0207       68\n",
       "HLA-B7          64\n",
       "HLA-A0205       56\n",
       "Mamu-A07        53\n",
       "Mamu-A2601      51\n",
       "HLA-A2          44\n",
       "HLA-B5802       42\n",
       "HLA-A0210       18\n",
       "Gogo-B0101      15\n",
       "ELA-A1          14\n",
       "HLA-A0302       10\n",
       "Patr-B1701       8\n",
       "HLA-B3503        7\n",
       "HLA-A6601        4\n",
       "HLA-B2702        4\n",
       "HLA-A26          4\n",
       "HLA-B1402        3\n",
       "HLA-B4201        3\n",
       "Mamu-B52         2\n",
       "HLA-B2701        2\n",
       "Mamu-B04         2\n",
       "Patr-B0901       1\n",
       "HLA-B3508        1\n",
       "HLA-B44          1\n",
       "HLA-E0101        1\n",
       "Patr-A0602       1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_allele_counts = validation_df.allele.value_counts()\n",
    "train_allele_counts = all_train_data._df.allele.value_counts()\n",
    "validation_allele_counts\n",
    "train_allele_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HLA-A0201',\n",
       " 'HLA-A0301',\n",
       " 'HLA-A0203',\n",
       " 'HLA-A1101',\n",
       " 'HLA-A0206',\n",
       " 'HLA-A3101',\n",
       " 'HLA-A6802',\n",
       " 'HLA-A0202',\n",
       " 'HLA-A0101',\n",
       " 'HLA-B0702',\n",
       " 'H-2-KB',\n",
       " 'H-2-DB',\n",
       " 'HLA-B1501',\n",
       " 'HLA-A6801',\n",
       " 'HLA-A3301',\n",
       " 'HLA-B2705',\n",
       " 'HLA-A2601',\n",
       " 'HLA-B4001',\n",
       " 'HLA-B5801',\n",
       " 'HLA-A2402',\n",
       " 'HLA-B3501',\n",
       " 'HLA-A2902',\n",
       " 'HLA-B0801',\n",
       " 'Mamu-A01',\n",
       " 'HLA-A6901',\n",
       " 'HLA-B1801',\n",
       " 'HLA-A3001',\n",
       " 'HLA-A2301',\n",
       " 'HLA-B5701',\n",
       " 'HLA-B5101',\n",
       " 'HLA-B4402',\n",
       " 'HLA-A3002',\n",
       " 'HLA-B4601',\n",
       " 'HLA-B5401',\n",
       " 'HLA-B5301',\n",
       " 'Mamu-A02',\n",
       " 'HLA-B4403',\n",
       " 'HLA-B4501',\n",
       " 'HLA-B3901',\n",
       " 'HLA-B4002',\n",
       " 'HLA-B1517',\n",
       " 'HLA-A8001',\n",
       " 'HLA-A3201',\n",
       " 'HLA-A2501',\n",
       " 'HLA-B0802',\n",
       " 'H-2-KD',\n",
       " 'HLA-B2703',\n",
       " 'HLA-B1503',\n",
       " 'HLA-B1509',\n",
       " 'HLA-B0803',\n",
       " 'HLA-A2603',\n",
       " 'HLA-A2602',\n",
       " 'HLA-B3801']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alleles = sorted(train_allele_counts.index[\n",
    "    (train_allele_counts >= min_peptides_to_consider_allele)\n",
    "    & (train_allele_counts.index.isin(validation_allele_counts.index))\n",
    "], key=lambda allele: -1 * train_allele_counts[allele])\n",
    "alleles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 models\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'activation',\n",
       " 'dropout_probability',\n",
       " 'embedding_output_dim',\n",
       " 'impute',\n",
       " 'layer_sizes'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout_probabilities = [0.0, 0.5]\n",
    "embedding_output_dims_and_layer_sizes_list = [(32, [64]), (8, [4])]\n",
    "activations = [\"tanh\"]\n",
    "\n",
    "models_params_list = []\n",
    "\n",
    "for model_num in range(1):\n",
    "    for impute in [False, True]:\n",
    "        for dropout_probability in dropout_probabilities:\n",
    "            for (embedding_output_dim, layer_sizes) in embedding_output_dims_and_layer_sizes_list:\n",
    "                for activation in activations:\n",
    "                    models_params_list.append(dict(\n",
    "                        impute=impute,\n",
    "                        dropout_probability=dropout_probability,  \n",
    "                        embedding_output_dim=embedding_output_dim,\n",
    "                        layer_sizes=layer_sizes,\n",
    "                        activation=activation))\n",
    "\n",
    "print(\"%d models\" % len(models_params_list))\n",
    "models_params_explored = set.union(*[set(x) for x in models_params_list])\n",
    "models_params_explored\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_to_ic50(log_value):\n",
    "        \"\"\"\n",
    "        Convert neural network output to IC50 values between 0.0 and\n",
    "        self.max_ic50 (typically 5000, 20000 or 50000)\n",
    "        \"\"\"\n",
    "        return max_ic50 ** (1.0 - log_value)\n",
    "\n",
    "def make_scores(ic50_y, ic50_y_pred, sample_weight=None, threshold_nm=500):     \n",
    "    y_pred = mhcflurry.regression_target.ic50_to_regression_target(ic50_y_pred, max_ic50)\n",
    "    try:\n",
    "        auc = sklearn.metrics.roc_auc_score(ic50_y <= threshold_nm, y_pred, sample_weight=sample_weight)\n",
    "    except ValueError:\n",
    "        auc = numpy.nan\n",
    "    try:\n",
    "        f1 = sklearn.metrics.f1_score(ic50_y <= threshold_nm, ic50_y_pred <= threshold_nm, sample_weight=sample_weight)\n",
    "    except ValueError:\n",
    "        f1 = numpy.nan\n",
    "    try:\n",
    "        tau = scipy.stats.kendalltau(ic50_y_pred, ic50_y)[0]\n",
    "    except ValueError:\n",
    "        tau = numpy.nan\n",
    "    \n",
    "    return dict(\n",
    "        auc=auc,\n",
    "        f1=f1,\n",
    "        tau=tau,\n",
    "    )    \n",
    "\n",
    "def mean_with_std(grouped_column, decimals=3):\n",
    "    pattern = \"%%0.%df\" % decimals\n",
    "    return pandas.Series([\n",
    "        (pattern + \" +/ \" + pattern) % (m, s) if not pandas.isnull(s) else pattern % m\n",
    "        for (m, s) in zip(grouped_column.mean(), grouped_column.std())\n",
    "    ], index = grouped_column.mean().index)\n",
    "\n",
    "def allele_data_to_df(data):\n",
    "    d = data._asdict()\n",
    "    d[\"X_index\"] = [x for x in d[\"X_index\"]]\n",
    "    d[\"X_binary\"] = [x for x in d[\"X_binary\"]]\n",
    "    df = pandas.DataFrame(d).set_index('peptides')\n",
    "    return df\n",
    "\n",
    "def make_2d_array(thing):\n",
    "    return numpy.array([list(x) for x in thing])\n",
    "\n",
    "def df_to_allele_data(df):\n",
    "    d = dict((col, df[col].values) for col in df)\n",
    "    d[\"X_index\"] = make_2d_array(d[\"X_index\"])\n",
    "    (d[\"max_ic50\"],) = list(df.max_ic50.unique())\n",
    "    return mhcflurry.data.AlleleData(peptides = df.index.values, **d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "models_and_scores = {}\n",
    "validation_df_with_mhcflurry = validation_df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping allele HLA-A0201: already done\n",
      "Allele 1 model 0\n",
      "Fitting model for allele HLA-A0301 (6141 + 19304): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 50 sec\n",
      "test set size: 811\n",
      "{'tau': 0.43438214718912999, 'auc': 0.82528648306426089, 'f1': 0.78222222222222226}\n",
      "Allele 1 model 1\n",
      "Fitting model for allele HLA-A0301 (6141 + 19304): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 22 sec\n",
      "test set size: 811\n",
      "{'tau': 0.58213187535133193, 'auc': 0.92022475466919906, 'f1': 0.86752136752136744}\n",
      "Allele 1 model 2\n",
      "Fitting model for allele HLA-A0301 (6141 + 19304): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 99 sec\n",
      "test set size: 811\n",
      "{'tau': 0.60037775507791202, 'auc': 0.92539411206077871, 'f1': 0.87272727272727268}\n",
      "Allele 1 model 3\n",
      "Fitting model for allele HLA-A0301 (6141 + 19304): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 32 sec\n",
      "test set size: 811\n",
      "{'tau': 0.57989650365770296, 'auc': 0.9175688509021841, 'f1': 0.77817319098457893}\n",
      "Allele 1 model 4\n",
      "Fitting model for allele HLA-A0301 (6141 + 19304): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 61 sec\n",
      "test set size: 811\n",
      "{'tau': 0.46559751152067874, 'auc': 0.86018360240582459, 'f1': 0.788950276243094}\n",
      "Allele 1 model 5\n",
      "Fitting model for allele HLA-A0301 (6141 + 19304): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 25 sec\n",
      "test set size: 811\n",
      "{'tau': 0.57476546282001517, 'auc': 0.91671415004748336, 'f1': 0.88704663212435231}\n",
      "Allele 1 model 6\n",
      "Fitting model for allele HLA-A0301 (6141 + 19304): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 106 sec\n",
      "test set size: 811\n",
      "{'tau': 0.60079461383175048, 'auc': 0.9261918328584996, 'f1': 0.88235294117647067}\n",
      "Allele 1 model 7\n",
      "Fitting model for allele HLA-A0301 (6141 + 19304): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 36 sec\n",
      "test set size: 811\n",
      "{'tau': 0.58568348400510717, 'auc': 0.91749287749287745, 'f1': 0.76328502415458943}\n",
      "Allele 2 model 0\n",
      "Fitting model for allele HLA-A0203 (5542 + 19304): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 46 sec\n",
      "test set size: 651\n",
      "{'tau': 0.46584576770970831, 'auc': 0.92388106416275428, 'f1': 0.88405797101449279}\n",
      "Allele 2 model 1\n",
      "Fitting model for allele HLA-A0203 (5542 + 19304): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 21 sec\n",
      "test set size: 651\n",
      "{'tau': 0.58095546666103737, 'auc': 0.97256129368805433, 'f1': 0.93964497041420125}\n",
      "Allele 2 model 2\n",
      "Fitting model for allele HLA-A0203 (5542 + 19304): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 106 sec\n",
      "test set size: 651\n",
      "{'tau': 0.59250563829765213, 'auc': 0.97913406364110589, 'f1': 0.9548693586698338}\n",
      "Allele 2 model 3\n",
      "Fitting model for allele HLA-A0203 (5542 + 19304): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 32 sec\n",
      "test set size: 651\n",
      "{'tau': 0.57935871280067752, 'auc': 0.97034950443401158, 'f1': 0.9522673031026252}\n",
      "Allele 2 model 4\n",
      "Fitting model for allele HLA-A0203 (5542 + 19304): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 55 sec\n",
      "test set size: 651\n",
      "{'tau': 0.48720593611715657, 'auc': 0.92133541992696921, 'f1': 0.88277511961722488}\n",
      "Allele 2 model 5\n",
      "Fitting model for allele HLA-A0203 (5542 + 19304): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 25 sec\n",
      "test set size: 651\n",
      "{'tau': 0.56513134906286067, 'auc': 0.97226917057902973, 'f1': 0.94862604540023898}\n",
      "Allele 2 model 6\n",
      "Fitting model for allele HLA-A0203 (5542 + 19304): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 109 sec\n",
      "test set size: 651\n",
      "{'tau': 0.59551747941150457, 'auc': 0.97815336463223779, 'f1': 0.94899169632265712}\n",
      "Allele 2 model 7\n",
      "Fitting model for allele HLA-A0203 (5542 + 19304): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 36 sec\n",
      "test set size: 651\n",
      "{'tau': 0.58511467581826204, 'auc': 0.97053729786124154, 'f1': 0.95124851367419738}\n",
      "Allele 3 model 0\n",
      "Fitting model for allele HLA-A1101 (5399 + 19304): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 45 sec\n",
      "test set size: 723\n",
      "{'tau': 0.50181379099839452, 'auc': 0.88094762862538589, 'f1': 0.82087447108603662}\n",
      "Allele 3 model 1\n",
      "Fitting model for allele HLA-A1101 (5399 + 19304): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 20 sec\n",
      "test set size: 723\n",
      "{'tau': 0.61139161237118478, 'auc': 0.93030200672490826, 'f1': 0.87500000000000011}\n",
      "Allele 3 model 2\n",
      "Fitting model for allele HLA-A1101 (5399 + 19304): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 91 sec\n",
      "test set size: 723\n",
      "{'tau': 0.61506335433132431, 'auc': 0.94106493067817176, 'f1': 0.88736263736263743}\n",
      "Allele 3 model 3\n",
      "Fitting model for allele HLA-A1101 (5399 + 19304): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 29 sec\n",
      "test set size: 723\n",
      "{'tau': 0.58206425269801942, 'auc': 0.92508175830249795, 'f1': 0.87899860917941586}\n",
      "Allele 3 model 4\n",
      "Fitting model for allele HLA-A1101 (5399 + 19304): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 52 sec\n",
      "test set size: 723\n",
      "{'tau': 0.49629453405619744, 'auc': 0.88918487356251252, 'f1': 0.83626522327469544}\n",
      "Allele 3 model 5\n",
      "Fitting model for allele HLA-A1101 (5399 + 19304): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 24 sec\n",
      "test set size: 723\n",
      "{'tau': 0.603357250999632, 'auc': 0.93612872518462797, 'f1': 0.8794520547945206}\n",
      "Allele 3 model 6\n",
      "Fitting model for allele HLA-A1101 (5399 + 19304): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 103 sec\n",
      "test set size: 723\n",
      "{'tau': 0.61569989310665507, 'auc': 0.93989037478312942, 'f1': 0.88888888888888884}\n",
      "Allele 3 model 7\n",
      "Fitting model for allele HLA-A1101 (5399 + 19304): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 34 sec\n",
      "test set size: 723\n",
      "{'tau': 0.59286212387466664, 'auc': 0.93116181234742279, 'f1': 0.88055555555555554}\n",
      "Allele 4 model 0\n",
      "Fitting model for allele HLA-A0206 (4827 + 19304): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 45 sec\n",
      "test set size: 682\n",
      "{'tau': 0.41557911042240381, 'auc': 0.82984110275234346, 'f1': 0.77835051546391754}\n",
      "Allele 4 model 1\n",
      "Fitting model for allele HLA-A0206 (4827 + 19304): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 20 sec\n",
      "test set size: 682\n",
      "{'tau': 0.51093666373963109, 'auc': 0.90092289263773062, 'f1': 0.86691086691086683}\n",
      "Allele 4 model 2\n",
      "Fitting model for allele HLA-A0206 (4827 + 19304): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 90 sec\n",
      "test set size: 682\n",
      "{'tau': 0.52859034677442507, 'auc': 0.90676697187763733, 'f1': 0.86977886977886976}\n",
      "Allele 4 model 3\n",
      "Fitting model for allele HLA-A0206 (4827 + 19304): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 29 sec\n",
      "test set size: 682\n",
      "{'tau': 0.49319652780065099, 'auc': 0.88205668030890139, 'f1': 0.86806411837237973}\n",
      "Allele 4 model 4\n",
      "Fitting model for allele HLA-A0206 (4827 + 19304): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 52 sec\n",
      "test set size: 682\n",
      "{'tau': 0.40064869386947077, 'auc': 0.81625634091672183, 'f1': 0.77935483870967748}\n",
      "Allele 4 model 5\n",
      "Fitting model for allele HLA-A0206 (4827 + 19304): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 24 sec\n",
      "test set size: 682\n",
      "{'tau': 0.5014614254408386, 'auc': 0.8978102852164761, 'f1': 0.86893203883495151}\n",
      "Allele 4 model 6\n",
      "Fitting model for allele HLA-A0206 (4827 + 19304): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 108 sec\n",
      "test set size: 682\n",
      "{'tau': 0.52897073955284379, 'auc': 0.90605914861566106, 'f1': 0.87053020961775585}\n",
      "Allele 4 model 7\n",
      "Fitting model for allele HLA-A0206 (4827 + 19304): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 35 sec\n",
      "test set size: 682\n",
      "{'tau': 0.49361150174074409, 'auc': 0.8828371008285163, 'f1': 0.8681592039800996}\n",
      "Allele 5 model 0\n",
      "Fitting model for allele HLA-A3101 (4796 + 19304): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 44 sec\n",
      "test set size: 724\n",
      "{'tau': 0.38588544261318314, 'auc': 0.79505319312015199, 'f1': 0.76923076923076938}\n",
      "Allele 5 model 1\n",
      "Fitting model for allele HLA-A3101 (4796 + 19304): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 19 sec\n",
      "test set size: 724\n",
      "{'tau': 0.48115284068552577, 'auc': 0.84961063249225866, 'f1': 0.82191780821917815}\n",
      "Allele 5 model 2\n",
      "Fitting model for allele HLA-A3101 (4796 + 19304): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 92 sec\n",
      "test set size: 724\n",
      "{'tau': 0.50402812972363531, 'auc': 0.85130453444522791, 'f1': 0.81954887218045103}\n",
      "Allele 5 model 3\n",
      "Fitting model for allele HLA-A3101 (4796 + 19304): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 31 sec\n",
      "test set size: 724\n",
      "{'tau': 0.48535899201000948, 'auc': 0.83940123248612697, 'f1': 0.79172056921086675}\n",
      "Allele 5 model 4\n",
      "Fitting model for allele HLA-A3101 (4796 + 19304): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 51 sec\n",
      "test set size: 724\n",
      "{'tau': 0.39576410994408062, 'auc': 0.80080172915964076, 'f1': 0.74329501915708807}\n",
      "Allele 5 model 5\n",
      "Fitting model for allele HLA-A3101 (4796 + 19304): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 24 sec\n",
      "test set size: 724\n",
      "{'tau': 0.49688153132643914, 'auc': 0.85808014225710505, 'f1': 0.82571075401730532}\n",
      "Allele 5 model 6\n",
      "Fitting model for allele HLA-A3101 (4796 + 19304): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 104 sec\n",
      "test set size: 724\n",
      "{'tau': 0.50155074518205867, 'auc': 0.85151914645736881, 'f1': 0.81715006305170235}\n",
      "Allele 5 model 7\n",
      "Fitting model for allele HLA-A3101 (4796 + 19304): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 41 sec\n",
      "test set size: 724\n",
      "{'tau': 0.4878981182224042, 'auc': 0.83849679614924733, 'f1': 0.78543563068920674}\n",
      "Allele 6 model 0\n",
      "Fitting model for allele HLA-A6802 (4768 + 19304): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 54 sec\n",
      "test set size: 669\n",
      "{'tau': 0.53776972589564898, 'auc': 0.90185700188391493, 'f1': 0.79503105590062118}\n",
      "Allele 6 model 1\n",
      "Fitting model for allele HLA-A6802 (4768 + 19304): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}"
     ]
    }
   ],
   "source": [
    "# train and test models, adding columns to validation_df_with_mhcflurry\n",
    "pandas.DataFrame(models_params_list).to_csv(\"../data/validation_models.csv\", index=False)\n",
    "\n",
    "def make_and_fit_model(allele, original_params):\n",
    "    params = dict(original_params)\n",
    "    impute = params[\"impute\"]\n",
    "    del params[\"impute\"]\n",
    "    model = mhcflurry.Class1BindingPredictor.from_hyperparameters(max_ic50=max_ic50, **params)\n",
    "    print(\"Fitting model for allele %s (%d + %d): %s\" % (\n",
    "            allele,\n",
    "            len(all_train_data.groupby_allele_dictionary()[allele]),\n",
    "            len(imputed_train_data.groupby_allele_dictionary()[allele]),\n",
    "            str(original_params)))\n",
    "    t = -time.time()\n",
    "    model.fit_dataset(\n",
    "        all_train_data.groupby_allele_dictionary()[allele],\n",
    "        pretraining_dataset=imputed_train_data.groupby_allele_dictionary()[allele] if impute else None,\n",
    "        verbose=False,\n",
    "        batch_size=128,\n",
    "        n_training_epochs=250)\n",
    "    t += time.time()\n",
    "    print(\"Trained in %d sec\" % t)\n",
    "    return model\n",
    "\n",
    "for (i, allele) in enumerate(alleles):\n",
    "    if allele not in validation_df_with_mhcflurry.allele.unique():\n",
    "        print(\"Skipping allele %s: not in test set\" % allele)\n",
    "        continue\n",
    "    if allele in models_and_scores:\n",
    "        print(\"Skipping allele %s: already done\" % allele)\n",
    "        continue\n",
    "    values_for_allele = []\n",
    "    for (j, params) in enumerate(models_params_list):\n",
    "        print(\"Allele %d model %d\" % (i, j))\n",
    "        model = make_and_fit_model(allele, params)\n",
    "        predictions = model.predict(\n",
    "            list(validation_df_with_mhcflurry.ix[validation_df_with_mhcflurry.allele == allele].peptide))\n",
    "        print(\"test set size: %d\" % len(predictions))\n",
    "        validation_df_with_mhcflurry.loc[(validation_df_with_mhcflurry.allele == allele),\n",
    "                                         (\"mhcflurry %d\" % j)] = predictions\n",
    "        scores = make_scores(validation_df_with_mhcflurry.ix[validation_df.allele == allele].meas,\n",
    "                            predictions)\n",
    "        print(scores)\n",
    "        values_for_allele.append((params, scores))\n",
    "        \n",
    "    models_and_scores[allele] = values_for_allele\n",
    "    \n",
    "    # Write out all data after each allele.\n",
    "    validation_df_with_mhcflurry_results = validation_df_with_mhcflurry.ix[validation_df_with_mhcflurry.allele.isin(models_and_scores)]\n",
    "    validation_df_with_mhcflurry_results.to_csv(\"../data/validation_predictions_full.csv\", index=False)\n",
    "    \n",
    "    scores_df = collections.defaultdict(list)\n",
    "    predictors = validation_df_with_mhcflurry_results.columns[4:]\n",
    "\n",
    "    for (allele, grouped) in validation_df_with_mhcflurry_results.groupby(\"allele\"):\n",
    "        scores_df[\"allele\"].append(allele)\n",
    "        scores_df[\"test_size\"].append(len(grouped.meas))\n",
    "        for predictor in predictors:\n",
    "            scores = make_scores(grouped.meas, grouped[predictor])\n",
    "            for (key, value) in scores.items():\n",
    "                scores_df[\"%s_%s\" % (predictor, key)].append(value)\n",
    "\n",
    "    scores_df = pandas.DataFrame(scores_df)\n",
    "    scores_df[\"train_size\"] = [\n",
    "        len(all_train_data.groupby_allele_dictionary()[a])\n",
    "        for a in scores_df.allele\n",
    "    ]\n",
    "\n",
    "    scores_df.index = scores_df.allele\n",
    "    scores_df.to_csv(\"../data/validation_scores.csv\", index=False)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_full(scores_df[[\"train_size\", \"test_size\"]].sort(\"train_size\", inplace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
