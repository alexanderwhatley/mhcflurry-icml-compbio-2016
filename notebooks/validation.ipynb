{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"THEANO_FLAGS\"] = 'cuda.root=/usr/local/cuda,floatX=float32,device=gpu1,force_device=False,lib.cnmem=.75'\n",
    "\n",
    "import theano\n",
    "print(theano.config.device)\n",
    "\n",
    "import mhcflurry, seaborn, numpy, pandas, pickle, sklearn, collections, scipy, time\n",
    "import mhcflurry.dataset\n",
    "import fancyimpute, locale\n",
    "\n",
    "import sklearn.metrics\n",
    "import sklearn.cross_validation\n",
    "\n",
    "def print_full(x):\n",
    "    pandas.set_option('display.max_rows', len(x))\n",
    "    print(x)\n",
    "    pandas.reset_option('display.max_rows')\n",
    "    \n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_ic50 = 50000\n",
    "min_peptides_to_consider_allele = 10\n",
    "data_dir = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_train_data = mhcflurry.dataset.Dataset.from_csv(data_dir + \"bdata.2009.mhci.public.1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 12235 peptides with <2 observations\n",
      "Dropping 9 alleles with <2 observations: ['ELA-A1', 'HLA-B2701', 'HLA-B3508', 'HLA-B44', 'HLA-E0101', 'Mamu-B04', 'Patr-A0602', 'Patr-B0901', 'Patr-B1701']\n",
      "[MICE] Completing matrix with shape (19304, 97)\n",
      "[MICE] Starting imputation round 1/300, elapsed time 0.024\n",
      "[MICE] Starting imputation round 2/300, elapsed time 1.564\n",
      "[MICE] Starting imputation round 3/300, elapsed time 2.855\n",
      "[MICE] Starting imputation round 4/300, elapsed time 4.131\n",
      "[MICE] Starting imputation round 5/300, elapsed time 5.399\n",
      "[MICE] Starting imputation round 6/300, elapsed time 6.674\n",
      "[MICE] Starting imputation round 7/300, elapsed time 7.941\n",
      "[MICE] Starting imputation round 8/300, elapsed time 9.213\n",
      "[MICE] Starting imputation round 9/300, elapsed time 10.482\n",
      "[MICE] Starting imputation round 10/300, elapsed time 11.755\n",
      "[MICE] Starting imputation round 11/300, elapsed time 13.023\n",
      "[MICE] Starting imputation round 12/300, elapsed time 14.301\n",
      "[MICE] Starting imputation round 13/300, elapsed time 15.569\n",
      "[MICE] Starting imputation round 14/300, elapsed time 16.843\n",
      "[MICE] Starting imputation round 15/300, elapsed time 18.113\n",
      "[MICE] Starting imputation round 16/300, elapsed time 19.389\n",
      "[MICE] Starting imputation round 17/300, elapsed time 20.659\n",
      "[MICE] Starting imputation round 18/300, elapsed time 21.934\n",
      "[MICE] Starting imputation round 19/300, elapsed time 23.202\n",
      "[MICE] Starting imputation round 20/300, elapsed time 24.476\n",
      "[MICE] Starting imputation round 21/300, elapsed time 25.743\n",
      "[MICE] Starting imputation round 22/300, elapsed time 27.020\n",
      "[MICE] Starting imputation round 23/300, elapsed time 28.287\n",
      "[MICE] Starting imputation round 24/300, elapsed time 29.562\n",
      "[MICE] Starting imputation round 25/300, elapsed time 30.827\n",
      "[MICE] Starting imputation round 26/300, elapsed time 32.106\n",
      "[MICE] Starting imputation round 27/300, elapsed time 33.370\n",
      "[MICE] Starting imputation round 28/300, elapsed time 34.645\n",
      "[MICE] Starting imputation round 29/300, elapsed time 35.913\n",
      "[MICE] Starting imputation round 30/300, elapsed time 37.188\n",
      "[MICE] Starting imputation round 31/300, elapsed time 38.456\n",
      "[MICE] Starting imputation round 32/300, elapsed time 39.730\n",
      "[MICE] Starting imputation round 33/300, elapsed time 41.005\n",
      "[MICE] Starting imputation round 34/300, elapsed time 42.282\n",
      "[MICE] Starting imputation round 35/300, elapsed time 43.549\n",
      "[MICE] Starting imputation round 36/300, elapsed time 44.822\n",
      "[MICE] Starting imputation round 37/300, elapsed time 46.087\n",
      "[MICE] Starting imputation round 38/300, elapsed time 47.362\n",
      "[MICE] Starting imputation round 39/300, elapsed time 48.628\n",
      "[MICE] Starting imputation round 40/300, elapsed time 49.901\n",
      "[MICE] Starting imputation round 41/300, elapsed time 51.169\n",
      "[MICE] Starting imputation round 42/300, elapsed time 52.442\n",
      "[MICE] Starting imputation round 43/300, elapsed time 53.713\n",
      "[MICE] Starting imputation round 44/300, elapsed time 54.988\n",
      "[MICE] Starting imputation round 45/300, elapsed time 56.258\n",
      "[MICE] Starting imputation round 46/300, elapsed time 57.530\n",
      "[MICE] Starting imputation round 47/300, elapsed time 58.802\n",
      "[MICE] Starting imputation round 48/300, elapsed time 60.078\n",
      "[MICE] Starting imputation round 49/300, elapsed time 61.351\n",
      "[MICE] Starting imputation round 50/300, elapsed time 62.624\n",
      "[MICE] Starting imputation round 51/300, elapsed time 63.894\n",
      "[MICE] Starting imputation round 52/300, elapsed time 65.174\n",
      "[MICE] Starting imputation round 53/300, elapsed time 66.452\n",
      "[MICE] Starting imputation round 54/300, elapsed time 67.729\n",
      "[MICE] Starting imputation round 55/300, elapsed time 69.007\n",
      "[MICE] Starting imputation round 56/300, elapsed time 70.284\n",
      "[MICE] Starting imputation round 57/300, elapsed time 71.565\n",
      "[MICE] Starting imputation round 58/300, elapsed time 72.841\n",
      "[MICE] Starting imputation round 59/300, elapsed time 74.117\n",
      "[MICE] Starting imputation round 60/300, elapsed time 75.395\n",
      "[MICE] Starting imputation round 61/300, elapsed time 76.673\n",
      "[MICE] Starting imputation round 62/300, elapsed time 77.950\n",
      "[MICE] Starting imputation round 63/300, elapsed time 79.223\n",
      "[MICE] Starting imputation round 64/300, elapsed time 80.500\n",
      "[MICE] Starting imputation round 65/300, elapsed time 81.772\n",
      "[MICE] Starting imputation round 66/300, elapsed time 83.054\n",
      "[MICE] Starting imputation round 67/300, elapsed time 84.333\n",
      "[MICE] Starting imputation round 68/300, elapsed time 85.605\n",
      "[MICE] Starting imputation round 69/300, elapsed time 86.880\n",
      "[MICE] Starting imputation round 70/300, elapsed time 88.163\n",
      "[MICE] Starting imputation round 71/300, elapsed time 89.439\n",
      "[MICE] Starting imputation round 72/300, elapsed time 90.721\n",
      "[MICE] Starting imputation round 73/300, elapsed time 91.999\n",
      "[MICE] Starting imputation round 74/300, elapsed time 93.277\n",
      "[MICE] Starting imputation round 75/300, elapsed time 94.559\n",
      "[MICE] Starting imputation round 76/300, elapsed time 95.840\n",
      "[MICE] Starting imputation round 77/300, elapsed time 97.114\n",
      "[MICE] Starting imputation round 78/300, elapsed time 98.392\n",
      "[MICE] Starting imputation round 79/300, elapsed time 99.673\n",
      "[MICE] Starting imputation round 80/300, elapsed time 100.951\n",
      "[MICE] Starting imputation round 81/300, elapsed time 102.229\n",
      "[MICE] Starting imputation round 82/300, elapsed time 103.507\n",
      "[MICE] Starting imputation round 83/300, elapsed time 104.786\n",
      "[MICE] Starting imputation round 84/300, elapsed time 106.069\n",
      "[MICE] Starting imputation round 85/300, elapsed time 107.348\n",
      "[MICE] Starting imputation round 86/300, elapsed time 108.623\n",
      "[MICE] Starting imputation round 87/300, elapsed time 109.902\n",
      "[MICE] Starting imputation round 88/300, elapsed time 111.183\n",
      "[MICE] Starting imputation round 89/300, elapsed time 112.462\n",
      "[MICE] Starting imputation round 90/300, elapsed time 113.742\n",
      "[MICE] Starting imputation round 91/300, elapsed time 115.024\n",
      "[MICE] Starting imputation round 92/300, elapsed time 116.300\n",
      "[MICE] Starting imputation round 93/300, elapsed time 117.584\n",
      "[MICE] Starting imputation round 94/300, elapsed time 118.869\n",
      "[MICE] Starting imputation round 95/300, elapsed time 120.148\n",
      "[MICE] Starting imputation round 96/300, elapsed time 121.433\n",
      "[MICE] Starting imputation round 97/300, elapsed time 122.716\n",
      "[MICE] Starting imputation round 98/300, elapsed time 123.997\n",
      "[MICE] Starting imputation round 99/300, elapsed time 125.276\n",
      "[MICE] Starting imputation round 100/300, elapsed time 126.561\n",
      "[MICE] Starting imputation round 101/300, elapsed time 127.839\n",
      "[MICE] Starting imputation round 102/300, elapsed time 129.121\n",
      "[MICE] Starting imputation round 103/300, elapsed time 130.405\n",
      "[MICE] Starting imputation round 104/300, elapsed time 131.690\n",
      "[MICE] Starting imputation round 105/300, elapsed time 132.972\n",
      "[MICE] Starting imputation round 106/300, elapsed time 134.256\n",
      "[MICE] Starting imputation round 107/300, elapsed time 135.534\n",
      "[MICE] Starting imputation round 108/300, elapsed time 136.814\n",
      "[MICE] Starting imputation round 109/300, elapsed time 138.098\n",
      "[MICE] Starting imputation round 110/300, elapsed time 139.386\n",
      "[MICE] Starting imputation round 111/300, elapsed time 140.667\n",
      "[MICE] Starting imputation round 112/300, elapsed time 141.957\n",
      "[MICE] Starting imputation round 113/300, elapsed time 143.236\n",
      "[MICE] Starting imputation round 114/300, elapsed time 144.523\n",
      "[MICE] Starting imputation round 115/300, elapsed time 145.810\n",
      "[MICE] Starting imputation round 116/300, elapsed time 147.100\n",
      "[MICE] Starting imputation round 117/300, elapsed time 148.383\n",
      "[MICE] Starting imputation round 118/300, elapsed time 149.669\n",
      "[MICE] Starting imputation round 119/300, elapsed time 150.951\n",
      "[MICE] Starting imputation round 120/300, elapsed time 152.234\n",
      "[MICE] Starting imputation round 121/300, elapsed time 153.518\n",
      "[MICE] Starting imputation round 122/300, elapsed time 154.797\n",
      "[MICE] Starting imputation round 123/300, elapsed time 156.080\n",
      "[MICE] Starting imputation round 124/300, elapsed time 157.362\n",
      "[MICE] Starting imputation round 125/300, elapsed time 158.642\n",
      "[MICE] Starting imputation round 126/300, elapsed time 159.921\n",
      "[MICE] Starting imputation round 127/300, elapsed time 161.202\n",
      "[MICE] Starting imputation round 128/300, elapsed time 162.474\n",
      "[MICE] Starting imputation round 129/300, elapsed time 163.750\n",
      "[MICE] Starting imputation round 130/300, elapsed time 165.032\n",
      "[MICE] Starting imputation round 131/300, elapsed time 166.313\n",
      "[MICE] Starting imputation round 132/300, elapsed time 167.595\n",
      "[MICE] Starting imputation round 133/300, elapsed time 168.880\n",
      "[MICE] Starting imputation round 134/300, elapsed time 170.155\n",
      "[MICE] Starting imputation round 135/300, elapsed time 171.438\n",
      "[MICE] Starting imputation round 136/300, elapsed time 172.716\n",
      "[MICE] Starting imputation round 137/300, elapsed time 173.989\n",
      "[MICE] Starting imputation round 138/300, elapsed time 175.267\n",
      "[MICE] Starting imputation round 139/300, elapsed time 176.542\n",
      "[MICE] Starting imputation round 140/300, elapsed time 177.814\n",
      "[MICE] Starting imputation round 141/300, elapsed time 179.094\n",
      "[MICE] Starting imputation round 142/300, elapsed time 180.370\n",
      "[MICE] Starting imputation round 143/300, elapsed time 181.646\n",
      "[MICE] Starting imputation round 144/300, elapsed time 182.924\n",
      "[MICE] Starting imputation round 145/300, elapsed time 184.201\n",
      "[MICE] Starting imputation round 146/300, elapsed time 185.474\n",
      "[MICE] Starting imputation round 147/300, elapsed time 186.760\n",
      "[MICE] Starting imputation round 148/300, elapsed time 188.039\n",
      "[MICE] Starting imputation round 149/300, elapsed time 189.320\n",
      "[MICE] Starting imputation round 150/300, elapsed time 190.601\n",
      "[MICE] Starting imputation round 151/300, elapsed time 191.879\n",
      "[MICE] Starting imputation round 152/300, elapsed time 193.158\n",
      "[MICE] Starting imputation round 153/300, elapsed time 194.439\n",
      "[MICE] Starting imputation round 154/300, elapsed time 195.726\n",
      "[MICE] Starting imputation round 155/300, elapsed time 197.017\n",
      "[MICE] Starting imputation round 156/300, elapsed time 198.300\n",
      "[MICE] Starting imputation round 157/300, elapsed time 199.579\n",
      "[MICE] Starting imputation round 158/300, elapsed time 200.863\n",
      "[MICE] Starting imputation round 159/300, elapsed time 202.146\n",
      "[MICE] Starting imputation round 160/300, elapsed time 203.422\n",
      "[MICE] Starting imputation round 161/300, elapsed time 204.699\n",
      "[MICE] Starting imputation round 162/300, elapsed time 205.979\n",
      "[MICE] Starting imputation round 163/300, elapsed time 207.258\n",
      "[MICE] Starting imputation round 164/300, elapsed time 208.547\n",
      "[MICE] Starting imputation round 165/300, elapsed time 209.829\n",
      "[MICE] Starting imputation round 166/300, elapsed time 211.108\n",
      "[MICE] Starting imputation round 167/300, elapsed time 212.387\n",
      "[MICE] Starting imputation round 168/300, elapsed time 213.666\n",
      "[MICE] Starting imputation round 169/300, elapsed time 214.943\n",
      "[MICE] Starting imputation round 170/300, elapsed time 216.225\n",
      "[MICE] Starting imputation round 171/300, elapsed time 217.504\n",
      "[MICE] Starting imputation round 172/300, elapsed time 218.788\n",
      "[MICE] Starting imputation round 173/300, elapsed time 220.065\n",
      "[MICE] Starting imputation round 174/300, elapsed time 221.346\n",
      "[MICE] Starting imputation round 175/300, elapsed time 222.631\n",
      "[MICE] Starting imputation round 176/300, elapsed time 223.906\n",
      "[MICE] Starting imputation round 177/300, elapsed time 225.187\n",
      "[MICE] Starting imputation round 178/300, elapsed time 226.464\n",
      "[MICE] Starting imputation round 179/300, elapsed time 227.743\n",
      "[MICE] Starting imputation round 180/300, elapsed time 229.021\n",
      "[MICE] Starting imputation round 181/300, elapsed time 230.294\n",
      "[MICE] Starting imputation round 182/300, elapsed time 231.571\n",
      "[MICE] Starting imputation round 183/300, elapsed time 232.854\n",
      "[MICE] Starting imputation round 184/300, elapsed time 234.137\n",
      "[MICE] Starting imputation round 185/300, elapsed time 235.418\n",
      "[MICE] Starting imputation round 186/300, elapsed time 236.701\n",
      "[MICE] Starting imputation round 187/300, elapsed time 237.974\n",
      "[MICE] Starting imputation round 188/300, elapsed time 239.253\n",
      "[MICE] Starting imputation round 189/300, elapsed time 240.533\n",
      "[MICE] Starting imputation round 190/300, elapsed time 241.816\n",
      "[MICE] Starting imputation round 191/300, elapsed time 243.095\n",
      "[MICE] Starting imputation round 192/300, elapsed time 244.379\n",
      "[MICE] Starting imputation round 193/300, elapsed time 245.668\n",
      "[MICE] Starting imputation round 194/300, elapsed time 246.949\n",
      "[MICE] Starting imputation round 195/300, elapsed time 248.228\n",
      "[MICE] Starting imputation round 196/300, elapsed time 249.508\n",
      "[MICE] Starting imputation round 197/300, elapsed time 250.784\n",
      "[MICE] Starting imputation round 198/300, elapsed time 252.072\n",
      "[MICE] Starting imputation round 199/300, elapsed time 253.353\n",
      "[MICE] Starting imputation round 200/300, elapsed time 254.633\n",
      "[MICE] Starting imputation round 201/300, elapsed time 255.919\n",
      "[MICE] Starting imputation round 202/300, elapsed time 257.205\n",
      "[MICE] Starting imputation round 203/300, elapsed time 258.492\n",
      "[MICE] Starting imputation round 204/300, elapsed time 259.784\n",
      "[MICE] Starting imputation round 205/300, elapsed time 261.063\n",
      "[MICE] Starting imputation round 206/300, elapsed time 262.357\n",
      "[MICE] Starting imputation round 207/300, elapsed time 263.636\n",
      "[MICE] Starting imputation round 208/300, elapsed time 264.921\n",
      "[MICE] Starting imputation round 209/300, elapsed time 266.197\n",
      "[MICE] Starting imputation round 210/300, elapsed time 267.489\n",
      "[MICE] Starting imputation round 211/300, elapsed time 268.769\n",
      "[MICE] Starting imputation round 212/300, elapsed time 270.051\n",
      "[MICE] Starting imputation round 213/300, elapsed time 271.333\n",
      "[MICE] Starting imputation round 214/300, elapsed time 272.607\n",
      "[MICE] Starting imputation round 215/300, elapsed time 273.888\n",
      "[MICE] Starting imputation round 216/300, elapsed time 275.174\n",
      "[MICE] Starting imputation round 217/300, elapsed time 276.457\n",
      "[MICE] Starting imputation round 218/300, elapsed time 277.746\n",
      "[MICE] Starting imputation round 219/300, elapsed time 279.023\n",
      "[MICE] Starting imputation round 220/300, elapsed time 280.309\n",
      "[MICE] Starting imputation round 221/300, elapsed time 281.594\n",
      "[MICE] Starting imputation round 222/300, elapsed time 282.881\n",
      "[MICE] Starting imputation round 223/300, elapsed time 284.164\n",
      "[MICE] Starting imputation round 224/300, elapsed time 285.452\n",
      "[MICE] Starting imputation round 225/300, elapsed time 286.732\n",
      "[MICE] Starting imputation round 226/300, elapsed time 288.014\n",
      "[MICE] Starting imputation round 227/300, elapsed time 289.295\n",
      "[MICE] Starting imputation round 228/300, elapsed time 290.578\n",
      "[MICE] Starting imputation round 229/300, elapsed time 291.857\n",
      "[MICE] Starting imputation round 230/300, elapsed time 293.142\n",
      "[MICE] Starting imputation round 231/300, elapsed time 294.422\n",
      "[MICE] Starting imputation round 232/300, elapsed time 295.710\n",
      "[MICE] Starting imputation round 233/300, elapsed time 296.989\n",
      "[MICE] Starting imputation round 234/300, elapsed time 298.280\n",
      "[MICE] Starting imputation round 235/300, elapsed time 299.560\n",
      "[MICE] Starting imputation round 236/300, elapsed time 300.848\n",
      "[MICE] Starting imputation round 237/300, elapsed time 302.125\n",
      "[MICE] Starting imputation round 238/300, elapsed time 303.414\n",
      "[MICE] Starting imputation round 239/300, elapsed time 304.694\n",
      "[MICE] Starting imputation round 240/300, elapsed time 305.982\n",
      "[MICE] Starting imputation round 241/300, elapsed time 307.265\n",
      "[MICE] Starting imputation round 242/300, elapsed time 308.550\n",
      "[MICE] Starting imputation round 243/300, elapsed time 309.835\n",
      "[MICE] Starting imputation round 244/300, elapsed time 311.123\n",
      "[MICE] Starting imputation round 245/300, elapsed time 312.404\n",
      "[MICE] Starting imputation round 246/300, elapsed time 313.687\n",
      "[MICE] Starting imputation round 247/300, elapsed time 314.965\n",
      "[MICE] Starting imputation round 248/300, elapsed time 316.240\n",
      "[MICE] Starting imputation round 249/300, elapsed time 317.770\n",
      "[MICE] Starting imputation round 250/300, elapsed time 319.286\n",
      "[MICE] Starting imputation round 251/300, elapsed time 320.800\n",
      "[MICE] Starting imputation round 252/300, elapsed time 322.318\n",
      "[MICE] Starting imputation round 253/300, elapsed time 323.822\n",
      "[MICE] Starting imputation round 254/300, elapsed time 325.331\n",
      "[MICE] Starting imputation round 255/300, elapsed time 326.842\n",
      "[MICE] Starting imputation round 256/300, elapsed time 328.355\n",
      "[MICE] Starting imputation round 257/300, elapsed time 329.870\n",
      "[MICE] Starting imputation round 258/300, elapsed time 331.378\n",
      "[MICE] Starting imputation round 259/300, elapsed time 332.883\n",
      "[MICE] Starting imputation round 260/300, elapsed time 334.390\n",
      "[MICE] Starting imputation round 261/300, elapsed time 335.897\n",
      "[MICE] Starting imputation round 262/300, elapsed time 337.407\n",
      "[MICE] Starting imputation round 263/300, elapsed time 338.917\n",
      "[MICE] Starting imputation round 264/300, elapsed time 340.437\n",
      "[MICE] Starting imputation round 265/300, elapsed time 341.946\n",
      "[MICE] Starting imputation round 266/300, elapsed time 343.460\n",
      "[MICE] Starting imputation round 267/300, elapsed time 344.978\n",
      "[MICE] Starting imputation round 268/300, elapsed time 346.489\n",
      "[MICE] Starting imputation round 269/300, elapsed time 348.003\n",
      "[MICE] Starting imputation round 270/300, elapsed time 349.518\n",
      "[MICE] Starting imputation round 271/300, elapsed time 351.027\n",
      "[MICE] Starting imputation round 272/300, elapsed time 352.533\n",
      "[MICE] Starting imputation round 273/300, elapsed time 354.052\n",
      "[MICE] Starting imputation round 274/300, elapsed time 355.570\n",
      "[MICE] Starting imputation round 275/300, elapsed time 357.089\n",
      "[MICE] Starting imputation round 276/300, elapsed time 358.632\n",
      "[MICE] Starting imputation round 277/300, elapsed time 360.153\n",
      "[MICE] Starting imputation round 278/300, elapsed time 361.679\n",
      "[MICE] Starting imputation round 279/300, elapsed time 363.279\n",
      "[MICE] Starting imputation round 280/300, elapsed time 364.812\n",
      "[MICE] Starting imputation round 281/300, elapsed time 366.337\n",
      "[MICE] Starting imputation round 282/300, elapsed time 367.853\n",
      "[MICE] Starting imputation round 283/300, elapsed time 369.371\n",
      "[MICE] Starting imputation round 284/300, elapsed time 370.899\n",
      "[MICE] Starting imputation round 285/300, elapsed time 372.424\n",
      "[MICE] Starting imputation round 286/300, elapsed time 373.949\n",
      "[MICE] Starting imputation round 287/300, elapsed time 375.479\n",
      "[MICE] Starting imputation round 288/300, elapsed time 377.009\n",
      "[MICE] Starting imputation round 289/300, elapsed time 378.538\n",
      "[MICE] Starting imputation round 290/300, elapsed time 380.066\n",
      "[MICE] Starting imputation round 291/300, elapsed time 381.591\n",
      "[MICE] Starting imputation round 292/300, elapsed time 383.119\n",
      "[MICE] Starting imputation round 293/300, elapsed time 384.636\n",
      "[MICE] Starting imputation round 294/300, elapsed time 386.160\n",
      "[MICE] Starting imputation round 295/300, elapsed time 387.675\n",
      "[MICE] Starting imputation round 296/300, elapsed time 389.198\n",
      "[MICE] Starting imputation round 297/300, elapsed time 390.715\n",
      "[MICE] Starting imputation round 298/300, elapsed time 392.233\n",
      "[MICE] Starting imputation round 299/300, elapsed time 393.796\n",
      "[MICE] Starting imputation round 300/300, elapsed time 395.331\n"
     ]
    }
   ],
   "source": [
    "imputed_train_data = all_train_data.impute_missing_values(\n",
    "    fancyimpute.MICE(n_imputations=250, n_burn_in=50),\n",
    "    min_observations_per_peptide=2,\n",
    "    min_observations_per_allele=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>allele</th>\n",
       "      <th>peptide</th>\n",
       "      <th>affinity</th>\n",
       "      <th>sample_weight</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>allele</th>\n",
       "      <th>peptide</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"30\" valign=\"top\">Mamu-A07</th>\n",
       "      <th>RELYLNSSNV</th>\n",
       "      <td>Mamu-A07</td>\n",
       "      <td>RELYLNSSNV</td>\n",
       "      <td>1615.211000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ICKAAMGLR</th>\n",
       "      <td>Mamu-A07</td>\n",
       "      <td>ICKAAMGLR</td>\n",
       "      <td>1684.829504</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SAVTDRETDV</th>\n",
       "      <td>Mamu-A07</td>\n",
       "      <td>SAVTDRETDV</td>\n",
       "      <td>1691.783274</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NSHQRSDSS</th>\n",
       "      <td>Mamu-A07</td>\n",
       "      <td>NSHQRSDSS</td>\n",
       "      <td>1694.957645</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STATLCLGHH</th>\n",
       "      <td>Mamu-A07</td>\n",
       "      <td>STATLCLGHH</td>\n",
       "      <td>1682.656229</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AIYVFCISLK</th>\n",
       "      <td>Mamu-A07</td>\n",
       "      <td>AIYVFCISLK</td>\n",
       "      <td>1659.680822</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KPVDTSNSF</th>\n",
       "      <td>Mamu-A07</td>\n",
       "      <td>KPVDTSNSF</td>\n",
       "      <td>1632.351678</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SIYIAVANCV</th>\n",
       "      <td>Mamu-A07</td>\n",
       "      <td>SIYIAVANCV</td>\n",
       "      <td>1631.715646</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PWLTEKEAM</th>\n",
       "      <td>Mamu-A07</td>\n",
       "      <td>PWLTEKEAM</td>\n",
       "      <td>1698.113597</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PSLPSPSR</th>\n",
       "      <td>Mamu-A07</td>\n",
       "      <td>PSLPSPSR</td>\n",
       "      <td>1678.871225</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EEILGTVSW</th>\n",
       "      <td>Mamu-A07</td>\n",
       "      <td>EEILGTVSW</td>\n",
       "      <td>1559.525566</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VRGGMVAPL</th>\n",
       "      <td>Mamu-A07</td>\n",
       "      <td>VRGGMVAPL</td>\n",
       "      <td>1704.502331</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LTQRRTTPR</th>\n",
       "      <td>Mamu-A07</td>\n",
       "      <td>LTQRRTTPR</td>\n",
       "      <td>1617.113308</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSIMPVLAY</th>\n",
       "      <td>Mamu-A07</td>\n",
       "      <td>MSIMPVLAY</td>\n",
       "      <td>1584.375149</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IVILFIMFM</th>\n",
       "      <td>Mamu-A07</td>\n",
       "      <td>IVILFIMFM</td>\n",
       "      <td>1658.351731</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GLYCYFTHL</th>\n",
       "      <td>Mamu-A07</td>\n",
       "      <td>GLYCYFTHL</td>\n",
       "      <td>1581.397127</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TILDDNLYK</th>\n",
       "      <td>Mamu-A07</td>\n",
       "      <td>TILDDNLYK</td>\n",
       "      <td>1667.000649</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LYLYALIYFL</th>\n",
       "      <td>Mamu-A07</td>\n",
       "      <td>LYLYALIYFL</td>\n",
       "      <td>1609.864718</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RGINDRNFW</th>\n",
       "      <td>Mamu-A07</td>\n",
       "      <td>RGINDRNFW</td>\n",
       "      <td>1694.400571</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QLLALADRI</th>\n",
       "      <td>Mamu-A07</td>\n",
       "      <td>QLLALADRI</td>\n",
       "      <td>1696.888851</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KTRMEDYYL</th>\n",
       "      <td>Mamu-A07</td>\n",
       "      <td>KTRMEDYYL</td>\n",
       "      <td>1697.055873</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TVKPGNFNK</th>\n",
       "      <td>Mamu-A07</td>\n",
       "      <td>TVKPGNFNK</td>\n",
       "      <td>1596.731368</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIVPDIKLDA</th>\n",
       "      <td>Mamu-A07</td>\n",
       "      <td>VIVPDIKLDA</td>\n",
       "      <td>1694.371904</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TISSESLVY</th>\n",
       "      <td>Mamu-A07</td>\n",
       "      <td>TISSESLVY</td>\n",
       "      <td>1699.968730</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RVYAELAAL</th>\n",
       "      <td>Mamu-A07</td>\n",
       "      <td>RVYAELAAL</td>\n",
       "      <td>1593.762818</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SQAELTSNCT</th>\n",
       "      <td>Mamu-A07</td>\n",
       "      <td>SQAELTSNCT</td>\n",
       "      <td>1664.570572</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VTPLLKIL</th>\n",
       "      <td>Mamu-A07</td>\n",
       "      <td>VTPLLKIL</td>\n",
       "      <td>1658.983060</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRGEFLYCKM</th>\n",
       "      <td>Mamu-A07</td>\n",
       "      <td>CRGEFLYCKM</td>\n",
       "      <td>1643.095810</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIGLIVILFI</th>\n",
       "      <td>Mamu-A07</td>\n",
       "      <td>VIGLIVILFI</td>\n",
       "      <td>1630.718529</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YTAVVKLVY</th>\n",
       "      <td>Mamu-A07</td>\n",
       "      <td>YTAVVKLVY</td>\n",
       "      <td>1568.170612</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"30\" valign=\"top\">HLA-A0219</th>\n",
       "      <th>PVSDLYTSMR</th>\n",
       "      <td>HLA-A0219</td>\n",
       "      <td>PVSDLYTSMR</td>\n",
       "      <td>281.847354</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YTDEVYDYL</th>\n",
       "      <td>HLA-A0219</td>\n",
       "      <td>YTDEVYDYL</td>\n",
       "      <td>71.905121</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AITDNGPMPY</th>\n",
       "      <td>HLA-A0219</td>\n",
       "      <td>AITDNGPMPY</td>\n",
       "      <td>5381.516144</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DTGNYILCY</th>\n",
       "      <td>HLA-A0219</td>\n",
       "      <td>DTGNYILCY</td>\n",
       "      <td>4483.233637</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GSFCTQLNR</th>\n",
       "      <td>HLA-A0219</td>\n",
       "      <td>GSFCTQLNR</td>\n",
       "      <td>848.997986</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SLVSSLWSMI</th>\n",
       "      <td>HLA-A0219</td>\n",
       "      <td>SLVSSLWSMI</td>\n",
       "      <td>4287.694356</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPRPEMQEF</th>\n",
       "      <td>HLA-A0219</td>\n",
       "      <td>SPRPEMQEF</td>\n",
       "      <td>61634.876926</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FTNNEFTLS</th>\n",
       "      <td>HLA-A0219</td>\n",
       "      <td>FTNNEFTLS</td>\n",
       "      <td>5519.317320</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HHIPNGVVW</th>\n",
       "      <td>HLA-A0219</td>\n",
       "      <td>HHIPNGVVW</td>\n",
       "      <td>21415.925021</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YCNYTRFWYI</th>\n",
       "      <td>HLA-A0219</td>\n",
       "      <td>YCNYTRFWYI</td>\n",
       "      <td>1222.106966</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAEMLANID</th>\n",
       "      <td>HLA-A0219</td>\n",
       "      <td>PAEMLANID</td>\n",
       "      <td>21797.092415</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NQFGSVPAL</th>\n",
       "      <td>HLA-A0219</td>\n",
       "      <td>NQFGSVPAL</td>\n",
       "      <td>6359.421750</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YLIPSVTSL</th>\n",
       "      <td>HLA-A0219</td>\n",
       "      <td>YLIPSVTSL</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TDSPETHHY</th>\n",
       "      <td>HLA-A0219</td>\n",
       "      <td>TDSPETHHY</td>\n",
       "      <td>42660.441140</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GFLNEDHWF</th>\n",
       "      <td>HLA-A0219</td>\n",
       "      <td>GFLNEDHWF</td>\n",
       "      <td>59.541082</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AATKRYPGVM</th>\n",
       "      <td>HLA-A0219</td>\n",
       "      <td>AATKRYPGVM</td>\n",
       "      <td>40131.279975</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KLQDLTLRC</th>\n",
       "      <td>HLA-A0219</td>\n",
       "      <td>KLQDLTLRC</td>\n",
       "      <td>847.943608</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SDLDMLTQS</th>\n",
       "      <td>HLA-A0219</td>\n",
       "      <td>SDLDMLTQS</td>\n",
       "      <td>15517.173699</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EFCVDHPFIY</th>\n",
       "      <td>HLA-A0219</td>\n",
       "      <td>EFCVDHPFIY</td>\n",
       "      <td>65254.538684</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APRELLQYI</th>\n",
       "      <td>HLA-A0219</td>\n",
       "      <td>APRELLQYI</td>\n",
       "      <td>9110.772894</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RRQWVLAFR</th>\n",
       "      <td>HLA-A0219</td>\n",
       "      <td>RRQWVLAFR</td>\n",
       "      <td>21702.989962</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ATRSHRPLI</th>\n",
       "      <td>HLA-A0219</td>\n",
       "      <td>ATRSHRPLI</td>\n",
       "      <td>935.816763</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TATPAWDAL</th>\n",
       "      <td>HLA-A0219</td>\n",
       "      <td>TATPAWDAL</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IEELFYSYAT</th>\n",
       "      <td>HLA-A0219</td>\n",
       "      <td>IEELFYSYAT</td>\n",
       "      <td>79.070485</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RSLKAFFSW</th>\n",
       "      <td>HLA-A0219</td>\n",
       "      <td>RSLKAFFSW</td>\n",
       "      <td>743.346827</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IRQAGVQYSRADEEQ</th>\n",
       "      <td>HLA-A0219</td>\n",
       "      <td>IRQAGVQYSRADEEQ</td>\n",
       "      <td>212135.864611</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YIYSEIKQGR</th>\n",
       "      <td>HLA-A0219</td>\n",
       "      <td>YIYSEIKQGR</td>\n",
       "      <td>304.356384</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TSSDTYACW</th>\n",
       "      <td>HLA-A0219</td>\n",
       "      <td>TSSDTYACW</td>\n",
       "      <td>10366.711084</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELRQLAQSL</th>\n",
       "      <td>HLA-A0219</td>\n",
       "      <td>ELRQLAQSL</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FIYTYDRV</th>\n",
       "      <td>HLA-A0219</td>\n",
       "      <td>FIYTYDRV</td>\n",
       "      <td>215.117838</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1872488 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              allele          peptide       affinity  \\\n",
       "allele    peptide                                                      \n",
       "Mamu-A07  RELYLNSSNV        Mamu-A07       RELYLNSSNV    1615.211000   \n",
       "          ICKAAMGLR         Mamu-A07        ICKAAMGLR    1684.829504   \n",
       "          SAVTDRETDV        Mamu-A07       SAVTDRETDV    1691.783274   \n",
       "          NSHQRSDSS         Mamu-A07        NSHQRSDSS    1694.957645   \n",
       "          STATLCLGHH        Mamu-A07       STATLCLGHH    1682.656229   \n",
       "          AIYVFCISLK        Mamu-A07       AIYVFCISLK    1659.680822   \n",
       "          KPVDTSNSF         Mamu-A07        KPVDTSNSF    1632.351678   \n",
       "          SIYIAVANCV        Mamu-A07       SIYIAVANCV    1631.715646   \n",
       "          PWLTEKEAM         Mamu-A07        PWLTEKEAM    1698.113597   \n",
       "          PSLPSPSR          Mamu-A07         PSLPSPSR    1678.871225   \n",
       "          EEILGTVSW         Mamu-A07        EEILGTVSW    1559.525566   \n",
       "          VRGGMVAPL         Mamu-A07        VRGGMVAPL    1704.502331   \n",
       "          LTQRRTTPR         Mamu-A07        LTQRRTTPR    1617.113308   \n",
       "          MSIMPVLAY         Mamu-A07        MSIMPVLAY    1584.375149   \n",
       "          IVILFIMFM         Mamu-A07        IVILFIMFM    1658.351731   \n",
       "          GLYCYFTHL         Mamu-A07        GLYCYFTHL    1581.397127   \n",
       "          TILDDNLYK         Mamu-A07        TILDDNLYK    1667.000649   \n",
       "          LYLYALIYFL        Mamu-A07       LYLYALIYFL    1609.864718   \n",
       "          RGINDRNFW         Mamu-A07        RGINDRNFW    1694.400571   \n",
       "          QLLALADRI         Mamu-A07        QLLALADRI    1696.888851   \n",
       "          KTRMEDYYL         Mamu-A07        KTRMEDYYL    1697.055873   \n",
       "          TVKPGNFNK         Mamu-A07        TVKPGNFNK    1596.731368   \n",
       "          VIVPDIKLDA        Mamu-A07       VIVPDIKLDA    1694.371904   \n",
       "          TISSESLVY         Mamu-A07        TISSESLVY    1699.968730   \n",
       "          RVYAELAAL         Mamu-A07        RVYAELAAL    1593.762818   \n",
       "          SQAELTSNCT        Mamu-A07       SQAELTSNCT    1664.570572   \n",
       "          VTPLLKIL          Mamu-A07         VTPLLKIL    1658.983060   \n",
       "          CRGEFLYCKM        Mamu-A07       CRGEFLYCKM    1643.095810   \n",
       "          VIGLIVILFI        Mamu-A07       VIGLIVILFI    1630.718529   \n",
       "          YTAVVKLVY         Mamu-A07        YTAVVKLVY    1568.170612   \n",
       "...                              ...              ...            ...   \n",
       "HLA-A0219 PVSDLYTSMR       HLA-A0219       PVSDLYTSMR     281.847354   \n",
       "          YTDEVYDYL        HLA-A0219        YTDEVYDYL      71.905121   \n",
       "          AITDNGPMPY       HLA-A0219       AITDNGPMPY    5381.516144   \n",
       "          DTGNYILCY        HLA-A0219        DTGNYILCY    4483.233637   \n",
       "          GSFCTQLNR        HLA-A0219        GSFCTQLNR     848.997986   \n",
       "          SLVSSLWSMI       HLA-A0219       SLVSSLWSMI    4287.694356   \n",
       "          SPRPEMQEF        HLA-A0219        SPRPEMQEF   61634.876926   \n",
       "          FTNNEFTLS        HLA-A0219        FTNNEFTLS    5519.317320   \n",
       "          HHIPNGVVW        HLA-A0219        HHIPNGVVW   21415.925021   \n",
       "          YCNYTRFWYI       HLA-A0219       YCNYTRFWYI    1222.106966   \n",
       "          PAEMLANID        HLA-A0219        PAEMLANID   21797.092415   \n",
       "          NQFGSVPAL        HLA-A0219        NQFGSVPAL    6359.421750   \n",
       "          YLIPSVTSL        HLA-A0219        YLIPSVTSL   20000.000000   \n",
       "          TDSPETHHY        HLA-A0219        TDSPETHHY   42660.441140   \n",
       "          GFLNEDHWF        HLA-A0219        GFLNEDHWF      59.541082   \n",
       "          AATKRYPGVM       HLA-A0219       AATKRYPGVM   40131.279975   \n",
       "          KLQDLTLRC        HLA-A0219        KLQDLTLRC     847.943608   \n",
       "          SDLDMLTQS        HLA-A0219        SDLDMLTQS   15517.173699   \n",
       "          EFCVDHPFIY       HLA-A0219       EFCVDHPFIY   65254.538684   \n",
       "          APRELLQYI        HLA-A0219        APRELLQYI    9110.772894   \n",
       "          RRQWVLAFR        HLA-A0219        RRQWVLAFR   21702.989962   \n",
       "          ATRSHRPLI        HLA-A0219        ATRSHRPLI     935.816763   \n",
       "          TATPAWDAL        HLA-A0219        TATPAWDAL   20000.000000   \n",
       "          IEELFYSYAT       HLA-A0219       IEELFYSYAT      79.070485   \n",
       "          RSLKAFFSW        HLA-A0219        RSLKAFFSW     743.346827   \n",
       "          IRQAGVQYSRADEEQ  HLA-A0219  IRQAGVQYSRADEEQ  212135.864611   \n",
       "          YIYSEIKQGR       HLA-A0219       YIYSEIKQGR     304.356384   \n",
       "          TSSDTYACW        HLA-A0219        TSSDTYACW   10366.711084   \n",
       "          ELRQLAQSL        HLA-A0219        ELRQLAQSL   20000.000000   \n",
       "          FIYTYDRV         HLA-A0219         FIYTYDRV     215.117838   \n",
       "\n",
       "                           sample_weight  \n",
       "allele    peptide                         \n",
       "Mamu-A07  RELYLNSSNV                 1.0  \n",
       "          ICKAAMGLR                  1.0  \n",
       "          SAVTDRETDV                 1.0  \n",
       "          NSHQRSDSS                  1.0  \n",
       "          STATLCLGHH                 1.0  \n",
       "          AIYVFCISLK                 1.0  \n",
       "          KPVDTSNSF                  1.0  \n",
       "          SIYIAVANCV                 1.0  \n",
       "          PWLTEKEAM                  1.0  \n",
       "          PSLPSPSR                   1.0  \n",
       "          EEILGTVSW                  1.0  \n",
       "          VRGGMVAPL                  1.0  \n",
       "          LTQRRTTPR                  1.0  \n",
       "          MSIMPVLAY                  1.0  \n",
       "          IVILFIMFM                  1.0  \n",
       "          GLYCYFTHL                  1.0  \n",
       "          TILDDNLYK                  1.0  \n",
       "          LYLYALIYFL                 1.0  \n",
       "          RGINDRNFW                  1.0  \n",
       "          QLLALADRI                  1.0  \n",
       "          KTRMEDYYL                  1.0  \n",
       "          TVKPGNFNK                  1.0  \n",
       "          VIVPDIKLDA                 1.0  \n",
       "          TISSESLVY                  1.0  \n",
       "          RVYAELAAL                  1.0  \n",
       "          SQAELTSNCT                 1.0  \n",
       "          VTPLLKIL                   1.0  \n",
       "          CRGEFLYCKM                 1.0  \n",
       "          VIGLIVILFI                 1.0  \n",
       "          YTAVVKLVY                  1.0  \n",
       "...                                  ...  \n",
       "HLA-A0219 PVSDLYTSMR                 1.0  \n",
       "          YTDEVYDYL                  1.0  \n",
       "          AITDNGPMPY                 1.0  \n",
       "          DTGNYILCY                  1.0  \n",
       "          GSFCTQLNR                  1.0  \n",
       "          SLVSSLWSMI                 1.0  \n",
       "          SPRPEMQEF                  1.0  \n",
       "          FTNNEFTLS                  1.0  \n",
       "          HHIPNGVVW                  1.0  \n",
       "          YCNYTRFWYI                 1.0  \n",
       "          PAEMLANID                  1.0  \n",
       "          NQFGSVPAL                  1.0  \n",
       "          YLIPSVTSL                  1.0  \n",
       "          TDSPETHHY                  1.0  \n",
       "          GFLNEDHWF                  1.0  \n",
       "          AATKRYPGVM                 1.0  \n",
       "          KLQDLTLRC                  1.0  \n",
       "          SDLDMLTQS                  1.0  \n",
       "          EFCVDHPFIY                 1.0  \n",
       "          APRELLQYI                  1.0  \n",
       "          RRQWVLAFR                  1.0  \n",
       "          ATRSHRPLI                  1.0  \n",
       "          TATPAWDAL                  1.0  \n",
       "          IEELFYSYAT                 1.0  \n",
       "          RSLKAFFSW                  1.0  \n",
       "          IRQAGVQYSRADEEQ            1.0  \n",
       "          YIYSEIKQGR                 1.0  \n",
       "          TSSDTYACW                  1.0  \n",
       "          ELRQLAQSL                  1.0  \n",
       "          FIYTYDRV                   1.0  \n",
       "\n",
       "[1872488 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputed_train_data.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>allele</th>\n",
       "      <th>peptide</th>\n",
       "      <th>length</th>\n",
       "      <th>meas</th>\n",
       "      <th>netmhc</th>\n",
       "      <th>netmhcpan</th>\n",
       "      <th>smmpmbec_cpp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAACNVATA</td>\n",
       "      <td>9</td>\n",
       "      <td>657.657837</td>\n",
       "      <td>154.881662</td>\n",
       "      <td>711.213514</td>\n",
       "      <td>438.530698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAFEFVYV</td>\n",
       "      <td>8</td>\n",
       "      <td>30831.879502</td>\n",
       "      <td>6456.542290</td>\n",
       "      <td>785.235635</td>\n",
       "      <td>10351.421667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAFVNDYSL</td>\n",
       "      <td>9</td>\n",
       "      <td>77.446180</td>\n",
       "      <td>17.458222</td>\n",
       "      <td>7.516229</td>\n",
       "      <td>28.054336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAIANQAAV</td>\n",
       "      <td>9</td>\n",
       "      <td>1.999862</td>\n",
       "      <td>9.638290</td>\n",
       "      <td>9.749896</td>\n",
       "      <td>25.703958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAIANQAVV</td>\n",
       "      <td>9</td>\n",
       "      <td>1.517050</td>\n",
       "      <td>8.550667</td>\n",
       "      <td>8.336812</td>\n",
       "      <td>28.773984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAIENYVRF</td>\n",
       "      <td>9</td>\n",
       "      <td>37.844258</td>\n",
       "      <td>252.348077</td>\n",
       "      <td>114.815362</td>\n",
       "      <td>187.068214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAINFITTM</td>\n",
       "      <td>9</td>\n",
       "      <td>3.155005</td>\n",
       "      <td>199.986187</td>\n",
       "      <td>389.045145</td>\n",
       "      <td>200.909281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAIPAPPPI</td>\n",
       "      <td>9</td>\n",
       "      <td>3243.396173</td>\n",
       "      <td>1059.253725</td>\n",
       "      <td>493.173804</td>\n",
       "      <td>295.120923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAKLNRPPL</td>\n",
       "      <td>9</td>\n",
       "      <td>654.636174</td>\n",
       "      <td>66.374307</td>\n",
       "      <td>77.268059</td>\n",
       "      <td>38.459178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AALDMVDAL</td>\n",
       "      <td>9</td>\n",
       "      <td>229.614865</td>\n",
       "      <td>547.015963</td>\n",
       "      <td>597.035287</td>\n",
       "      <td>225.423921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AALQHLRSI</td>\n",
       "      <td>9</td>\n",
       "      <td>905.732601</td>\n",
       "      <td>1686.553025</td>\n",
       "      <td>2032.357011</td>\n",
       "      <td>698.232404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AALVRLTAL</td>\n",
       "      <td>9</td>\n",
       "      <td>1106.623784</td>\n",
       "      <td>435.511874</td>\n",
       "      <td>214.783047</td>\n",
       "      <td>378.442585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAMVPTGSL</td>\n",
       "      <td>9</td>\n",
       "      <td>1836.538343</td>\n",
       "      <td>4055.085354</td>\n",
       "      <td>5176.068320</td>\n",
       "      <td>1545.254440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AANSPWAPV</td>\n",
       "      <td>9</td>\n",
       "      <td>4325.138310</td>\n",
       "      <td>903.649474</td>\n",
       "      <td>1023.292992</td>\n",
       "      <td>557.185749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAPSGAAPL</td>\n",
       "      <td>9</td>\n",
       "      <td>2.844461</td>\n",
       "      <td>97.948999</td>\n",
       "      <td>501.187234</td>\n",
       "      <td>822.242650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAPVSEPTV</td>\n",
       "      <td>9</td>\n",
       "      <td>106.905488</td>\n",
       "      <td>2654.605562</td>\n",
       "      <td>1918.668741</td>\n",
       "      <td>1870.682140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAVANGFAA</td>\n",
       "      <td>9</td>\n",
       "      <td>83.368118</td>\n",
       "      <td>205.116218</td>\n",
       "      <td>228.559880</td>\n",
       "      <td>200.447203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAVLLGAPV</td>\n",
       "      <td>9</td>\n",
       "      <td>373.250158</td>\n",
       "      <td>320.626932</td>\n",
       "      <td>623.734835</td>\n",
       "      <td>286.417797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAVQLLFPA</td>\n",
       "      <td>9</td>\n",
       "      <td>347.536161</td>\n",
       "      <td>2494.594727</td>\n",
       "      <td>5152.286446</td>\n",
       "      <td>679.203633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAVTAGVAL</td>\n",
       "      <td>9</td>\n",
       "      <td>229.614865</td>\n",
       "      <td>1552.387010</td>\n",
       "      <td>2172.701179</td>\n",
       "      <td>1927.524913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAVVNRSLV</td>\n",
       "      <td>9</td>\n",
       "      <td>8.609938</td>\n",
       "      <td>26.302680</td>\n",
       "      <td>29.580125</td>\n",
       "      <td>63.826349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAYVPADAV</td>\n",
       "      <td>9</td>\n",
       "      <td>331.894458</td>\n",
       "      <td>1757.923614</td>\n",
       "      <td>3206.269325</td>\n",
       "      <td>595.662144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>ACMVNLERL</td>\n",
       "      <td>9</td>\n",
       "      <td>737.904230</td>\n",
       "      <td>239.331576</td>\n",
       "      <td>679.203633</td>\n",
       "      <td>444.631267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>ADLVNHPPV</td>\n",
       "      <td>9</td>\n",
       "      <td>558.470195</td>\n",
       "      <td>1694.337800</td>\n",
       "      <td>1857.804455</td>\n",
       "      <td>325.087297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AEMEEALKG</td>\n",
       "      <td>9</td>\n",
       "      <td>78523.563461</td>\n",
       "      <td>41304.750199</td>\n",
       "      <td>46989.410861</td>\n",
       "      <td>440554.863507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AFFAFRYV</td>\n",
       "      <td>8</td>\n",
       "      <td>87902.251683</td>\n",
       "      <td>18967.059212</td>\n",
       "      <td>12882.495517</td>\n",
       "      <td>7816.278046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AGLDNKFYL</td>\n",
       "      <td>9</td>\n",
       "      <td>659.173895</td>\n",
       "      <td>357.272838</td>\n",
       "      <td>202.768272</td>\n",
       "      <td>260.015956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AGLLFVLL</td>\n",
       "      <td>8</td>\n",
       "      <td>87902.251683</td>\n",
       "      <td>14996.848355</td>\n",
       "      <td>9885.530947</td>\n",
       "      <td>26242.185434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AGLQVINL</td>\n",
       "      <td>8</td>\n",
       "      <td>59429.215862</td>\n",
       "      <td>25061.092530</td>\n",
       "      <td>12560.299637</td>\n",
       "      <td>35399.734108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AGLVSFNFL</td>\n",
       "      <td>9</td>\n",
       "      <td>210.862815</td>\n",
       "      <td>620.869034</td>\n",
       "      <td>2285.598803</td>\n",
       "      <td>1811.340093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27650</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YNTVCVIW</td>\n",
       "      <td>8</td>\n",
       "      <td>909.913273</td>\n",
       "      <td>734.513868</td>\n",
       "      <td>10023.052381</td>\n",
       "      <td>48.865236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27651</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YRHDGGNVL</td>\n",
       "      <td>9</td>\n",
       "      <td>78342.964277</td>\n",
       "      <td>1355.189412</td>\n",
       "      <td>22233.098907</td>\n",
       "      <td>1409.288798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27652</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YSEGQYMNTP</td>\n",
       "      <td>10</td>\n",
       "      <td>7362.070975</td>\n",
       "      <td>2958.012467</td>\n",
       "      <td>8709.635900</td>\n",
       "      <td>4477.133042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27653</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YSEVALNVTES</td>\n",
       "      <td>11</td>\n",
       "      <td>7709.034691</td>\n",
       "      <td>1682.674061</td>\n",
       "      <td>5105.050000</td>\n",
       "      <td>51.522864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27654</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YSLVFVILM</td>\n",
       "      <td>9</td>\n",
       "      <td>1256.029964</td>\n",
       "      <td>17.498467</td>\n",
       "      <td>49.431069</td>\n",
       "      <td>6.324119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27655</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YSQIGAGVY</td>\n",
       "      <td>9</td>\n",
       "      <td>1.999862</td>\n",
       "      <td>17.864876</td>\n",
       "      <td>15.346170</td>\n",
       "      <td>16.292960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27656</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YSRDLICEQS</td>\n",
       "      <td>10</td>\n",
       "      <td>1059.253725</td>\n",
       "      <td>1297.179271</td>\n",
       "      <td>3863.669771</td>\n",
       "      <td>220.292646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27657</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YSYKAFIKY</td>\n",
       "      <td>9</td>\n",
       "      <td>2.317395</td>\n",
       "      <td>4.285485</td>\n",
       "      <td>3.090295</td>\n",
       "      <td>1.778279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27658</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YSYKAFIKYP</td>\n",
       "      <td>10</td>\n",
       "      <td>4830.588020</td>\n",
       "      <td>1199.499303</td>\n",
       "      <td>1753.880502</td>\n",
       "      <td>3111.716337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27659</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YSYKAFIKYPE</td>\n",
       "      <td>11</td>\n",
       "      <td>4897.788194</td>\n",
       "      <td>671.428853</td>\n",
       "      <td>2666.858665</td>\n",
       "      <td>42.559841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27660</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTAFTLPSV</td>\n",
       "      <td>9</td>\n",
       "      <td>101.624869</td>\n",
       "      <td>32.210688</td>\n",
       "      <td>52.844525</td>\n",
       "      <td>91.833260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27661</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTAFTLPSVN</td>\n",
       "      <td>10</td>\n",
       "      <td>2529.297996</td>\n",
       "      <td>236.591970</td>\n",
       "      <td>1406.047524</td>\n",
       "      <td>14.487719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27662</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTDGSCNKQS</td>\n",
       "      <td>10</td>\n",
       "      <td>11091.748153</td>\n",
       "      <td>4345.102242</td>\n",
       "      <td>11246.049740</td>\n",
       "      <td>588.843655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27663</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTGDFDSVI</td>\n",
       "      <td>9</td>\n",
       "      <td>215.278173</td>\n",
       "      <td>354.813389</td>\n",
       "      <td>42.461956</td>\n",
       "      <td>40.550854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27664</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTPKVVGG</td>\n",
       "      <td>8</td>\n",
       "      <td>70145.529842</td>\n",
       "      <td>17947.336268</td>\n",
       "      <td>13995.873226</td>\n",
       "      <td>3140.508694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27665</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTPKVVGGIGG</td>\n",
       "      <td>11</td>\n",
       "      <td>70145.529842</td>\n",
       "      <td>9418.895965</td>\n",
       "      <td>10046.157903</td>\n",
       "      <td>387.257645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27666</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTSGPGIR</td>\n",
       "      <td>8</td>\n",
       "      <td>16865.530254</td>\n",
       "      <td>619.441075</td>\n",
       "      <td>2349.632821</td>\n",
       "      <td>80.723503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27667</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTSGPGIRYP</td>\n",
       "      <td>10</td>\n",
       "      <td>7128.530301</td>\n",
       "      <td>783.429643</td>\n",
       "      <td>5176.068320</td>\n",
       "      <td>1270.574105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27668</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTSGPGTRYP</td>\n",
       "      <td>10</td>\n",
       "      <td>3097.419299</td>\n",
       "      <td>483.058802</td>\n",
       "      <td>5420.008904</td>\n",
       "      <td>797.994687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27669</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTSGPGTRYPM</td>\n",
       "      <td>11</td>\n",
       "      <td>25.941794</td>\n",
       "      <td>39.174188</td>\n",
       "      <td>7.744618</td>\n",
       "      <td>76.383578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27670</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTTGGTSR</td>\n",
       "      <td>8</td>\n",
       "      <td>70145.529842</td>\n",
       "      <td>357.272838</td>\n",
       "      <td>5701.642723</td>\n",
       "      <td>100.230524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27671</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTTGGTSRN</td>\n",
       "      <td>9</td>\n",
       "      <td>36897.759857</td>\n",
       "      <td>186.637969</td>\n",
       "      <td>6025.595861</td>\n",
       "      <td>119.674053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27672</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTTTGASRN</td>\n",
       "      <td>9</td>\n",
       "      <td>587.489353</td>\n",
       "      <td>135.518941</td>\n",
       "      <td>2844.461107</td>\n",
       "      <td>87.096359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27673</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTYEAYVRYP</td>\n",
       "      <td>10</td>\n",
       "      <td>2624.218543</td>\n",
       "      <td>1552.387010</td>\n",
       "      <td>3872.576449</td>\n",
       "      <td>1954.339456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27674</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTYEAYVRYPE</td>\n",
       "      <td>11</td>\n",
       "      <td>1905.460718</td>\n",
       "      <td>864.967919</td>\n",
       "      <td>6998.419960</td>\n",
       "      <td>32.734069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27675</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YVADALAAF</td>\n",
       "      <td>9</td>\n",
       "      <td>15.381546</td>\n",
       "      <td>453.941617</td>\n",
       "      <td>71.285303</td>\n",
       "      <td>108.143395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27676</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YVFPVIFSK</td>\n",
       "      <td>9</td>\n",
       "      <td>39264.493540</td>\n",
       "      <td>2098.939884</td>\n",
       "      <td>5610.479760</td>\n",
       "      <td>901.571138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27677</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YVFPVIFSR</td>\n",
       "      <td>9</td>\n",
       "      <td>36728.230050</td>\n",
       "      <td>2333.458062</td>\n",
       "      <td>10046.157903</td>\n",
       "      <td>2600.159563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27678</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YVPCHIRQI</td>\n",
       "      <td>9</td>\n",
       "      <td>10764.652136</td>\n",
       "      <td>21134.890398</td>\n",
       "      <td>6039.486294</td>\n",
       "      <td>10568.175092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27679</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YVVQMLARL</td>\n",
       "      <td>9</td>\n",
       "      <td>152.405275</td>\n",
       "      <td>232.273680</td>\n",
       "      <td>739.605275</td>\n",
       "      <td>105.681751</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27680 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         allele      peptide  length          meas        netmhc  \\\n",
       "0        H-2-DB    AAACNVATA       9    657.657837    154.881662   \n",
       "1        H-2-DB     AAFEFVYV       8  30831.879502   6456.542290   \n",
       "2        H-2-DB    AAFVNDYSL       9     77.446180     17.458222   \n",
       "3        H-2-DB    AAIANQAAV       9      1.999862      9.638290   \n",
       "4        H-2-DB    AAIANQAVV       9      1.517050      8.550667   \n",
       "5        H-2-DB    AAIENYVRF       9     37.844258    252.348077   \n",
       "6        H-2-DB    AAINFITTM       9      3.155005    199.986187   \n",
       "7        H-2-DB    AAIPAPPPI       9   3243.396173   1059.253725   \n",
       "8        H-2-DB    AAKLNRPPL       9    654.636174     66.374307   \n",
       "9        H-2-DB    AALDMVDAL       9    229.614865    547.015963   \n",
       "10       H-2-DB    AALQHLRSI       9    905.732601   1686.553025   \n",
       "11       H-2-DB    AALVRLTAL       9   1106.623784    435.511874   \n",
       "12       H-2-DB    AAMVPTGSL       9   1836.538343   4055.085354   \n",
       "13       H-2-DB    AANSPWAPV       9   4325.138310    903.649474   \n",
       "14       H-2-DB    AAPSGAAPL       9      2.844461     97.948999   \n",
       "15       H-2-DB    AAPVSEPTV       9    106.905488   2654.605562   \n",
       "16       H-2-DB    AAVANGFAA       9     83.368118    205.116218   \n",
       "17       H-2-DB    AAVLLGAPV       9    373.250158    320.626932   \n",
       "18       H-2-DB    AAVQLLFPA       9    347.536161   2494.594727   \n",
       "19       H-2-DB    AAVTAGVAL       9    229.614865   1552.387010   \n",
       "20       H-2-DB    AAVVNRSLV       9      8.609938     26.302680   \n",
       "21       H-2-DB    AAYVPADAV       9    331.894458   1757.923614   \n",
       "22       H-2-DB    ACMVNLERL       9    737.904230    239.331576   \n",
       "23       H-2-DB    ADLVNHPPV       9    558.470195   1694.337800   \n",
       "24       H-2-DB    AEMEEALKG       9  78523.563461  41304.750199   \n",
       "25       H-2-DB     AFFAFRYV       8  87902.251683  18967.059212   \n",
       "26       H-2-DB    AGLDNKFYL       9    659.173895    357.272838   \n",
       "27       H-2-DB     AGLLFVLL       8  87902.251683  14996.848355   \n",
       "28       H-2-DB     AGLQVINL       8  59429.215862  25061.092530   \n",
       "29       H-2-DB    AGLVSFNFL       9    210.862815    620.869034   \n",
       "...         ...          ...     ...           ...           ...   \n",
       "27650  Mamu-A02     YNTVCVIW       8    909.913273    734.513868   \n",
       "27651  Mamu-A02    YRHDGGNVL       9  78342.964277   1355.189412   \n",
       "27652  Mamu-A02   YSEGQYMNTP      10   7362.070975   2958.012467   \n",
       "27653  Mamu-A02  YSEVALNVTES      11   7709.034691   1682.674061   \n",
       "27654  Mamu-A02    YSLVFVILM       9   1256.029964     17.498467   \n",
       "27655  Mamu-A02    YSQIGAGVY       9      1.999862     17.864876   \n",
       "27656  Mamu-A02   YSRDLICEQS      10   1059.253725   1297.179271   \n",
       "27657  Mamu-A02    YSYKAFIKY       9      2.317395      4.285485   \n",
       "27658  Mamu-A02   YSYKAFIKYP      10   4830.588020   1199.499303   \n",
       "27659  Mamu-A02  YSYKAFIKYPE      11   4897.788194    671.428853   \n",
       "27660  Mamu-A02    YTAFTLPSV       9    101.624869     32.210688   \n",
       "27661  Mamu-A02   YTAFTLPSVN      10   2529.297996    236.591970   \n",
       "27662  Mamu-A02   YTDGSCNKQS      10  11091.748153   4345.102242   \n",
       "27663  Mamu-A02    YTGDFDSVI       9    215.278173    354.813389   \n",
       "27664  Mamu-A02     YTPKVVGG       8  70145.529842  17947.336268   \n",
       "27665  Mamu-A02  YTPKVVGGIGG      11  70145.529842   9418.895965   \n",
       "27666  Mamu-A02     YTSGPGIR       8  16865.530254    619.441075   \n",
       "27667  Mamu-A02   YTSGPGIRYP      10   7128.530301    783.429643   \n",
       "27668  Mamu-A02   YTSGPGTRYP      10   3097.419299    483.058802   \n",
       "27669  Mamu-A02  YTSGPGTRYPM      11     25.941794     39.174188   \n",
       "27670  Mamu-A02     YTTGGTSR       8  70145.529842    357.272838   \n",
       "27671  Mamu-A02    YTTGGTSRN       9  36897.759857    186.637969   \n",
       "27672  Mamu-A02    YTTTGASRN       9    587.489353    135.518941   \n",
       "27673  Mamu-A02   YTYEAYVRYP      10   2624.218543   1552.387010   \n",
       "27674  Mamu-A02  YTYEAYVRYPE      11   1905.460718    864.967919   \n",
       "27675  Mamu-A02    YVADALAAF       9     15.381546    453.941617   \n",
       "27676  Mamu-A02    YVFPVIFSK       9  39264.493540   2098.939884   \n",
       "27677  Mamu-A02    YVFPVIFSR       9  36728.230050   2333.458062   \n",
       "27678  Mamu-A02    YVPCHIRQI       9  10764.652136  21134.890398   \n",
       "27679  Mamu-A02    YVVQMLARL       9    152.405275    232.273680   \n",
       "\n",
       "          netmhcpan   smmpmbec_cpp  \n",
       "0        711.213514     438.530698  \n",
       "1        785.235635   10351.421667  \n",
       "2          7.516229      28.054336  \n",
       "3          9.749896      25.703958  \n",
       "4          8.336812      28.773984  \n",
       "5        114.815362     187.068214  \n",
       "6        389.045145     200.909281  \n",
       "7        493.173804     295.120923  \n",
       "8         77.268059      38.459178  \n",
       "9        597.035287     225.423921  \n",
       "10      2032.357011     698.232404  \n",
       "11       214.783047     378.442585  \n",
       "12      5176.068320    1545.254440  \n",
       "13      1023.292992     557.185749  \n",
       "14       501.187234     822.242650  \n",
       "15      1918.668741    1870.682140  \n",
       "16       228.559880     200.447203  \n",
       "17       623.734835     286.417797  \n",
       "18      5152.286446     679.203633  \n",
       "19      2172.701179    1927.524913  \n",
       "20        29.580125      63.826349  \n",
       "21      3206.269325     595.662144  \n",
       "22       679.203633     444.631267  \n",
       "23      1857.804455     325.087297  \n",
       "24     46989.410861  440554.863507  \n",
       "25     12882.495517    7816.278046  \n",
       "26       202.768272     260.015956  \n",
       "27      9885.530947   26242.185434  \n",
       "28     12560.299637   35399.734108  \n",
       "29      2285.598803    1811.340093  \n",
       "...             ...            ...  \n",
       "27650  10023.052381      48.865236  \n",
       "27651  22233.098907    1409.288798  \n",
       "27652   8709.635900    4477.133042  \n",
       "27653   5105.050000      51.522864  \n",
       "27654     49.431069       6.324119  \n",
       "27655     15.346170      16.292960  \n",
       "27656   3863.669771     220.292646  \n",
       "27657      3.090295       1.778279  \n",
       "27658   1753.880502    3111.716337  \n",
       "27659   2666.858665      42.559841  \n",
       "27660     52.844525      91.833260  \n",
       "27661   1406.047524      14.487719  \n",
       "27662  11246.049740     588.843655  \n",
       "27663     42.461956      40.550854  \n",
       "27664  13995.873226    3140.508694  \n",
       "27665  10046.157903     387.257645  \n",
       "27666   2349.632821      80.723503  \n",
       "27667   5176.068320    1270.574105  \n",
       "27668   5420.008904     797.994687  \n",
       "27669      7.744618      76.383578  \n",
       "27670   5701.642723     100.230524  \n",
       "27671   6025.595861     119.674053  \n",
       "27672   2844.461107      87.096359  \n",
       "27673   3872.576449    1954.339456  \n",
       "27674   6998.419960      32.734069  \n",
       "27675     71.285303     108.143395  \n",
       "27676   5610.479760     901.571138  \n",
       "27677  10046.157903    2600.159563  \n",
       "27678   6039.486294   10568.175092  \n",
       "27679    739.605275     105.681751  \n",
       "\n",
       "[27680 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df = pandas.read_csv(\"../data/combined_test_BLIND_dataset_from_kim2013.csv\")\n",
    "validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HLA-A0201    2126\n",
      "HLA-A2601    1333\n",
      "HLA-B0801     940\n",
      "HLA-B5101     854\n",
      "HLA-B5701     815\n",
      "HLA-B0702     813\n",
      "HLA-A0301     811\n",
      "HLA-A3101     724\n",
      "HLA-A1101     723\n",
      "HLA-A0101     696\n",
      "HLA-A0206     682\n",
      "HLA-A6802     669\n",
      "HLA-A3001     660\n",
      "HLA-A0203     651\n",
      "HLA-B3901     641\n",
      "HLA-B1501     633\n",
      "HLA-B1517     582\n",
      "HLA-A2402     573\n",
      "H-2-DB        564\n",
      "H-2-KB        558\n",
      "HLA-B3501     542\n",
      "HLA-A6801     527\n",
      "HLA-B0802     509\n",
      "HLA-B1801     503\n",
      "HLA-B5301     485\n",
      "HLA-A3301     473\n",
      "HLA-A6901     470\n",
      "HLA-B1509     466\n",
      "HLA-A3201     449\n",
      "HLA-B5801     445\n",
      "HLA-B2703     441\n",
      "HLA-A3002     420\n",
      "HLA-A2501     416\n",
      "HLA-A2602     413\n",
      "HLA-B4402     411\n",
      "HLA-B4001     407\n",
      "HLA-A2301     391\n",
      "Mamu-A02      388\n",
      "HLA-A8001     379\n",
      "HLA-B4601     378\n",
      "HLA-B4403     378\n",
      "HLA-B3801     351\n",
      "HLA-B2705     314\n",
      "HLA-A2603     312\n",
      "Mamu-A01      274\n",
      "HLA-B0803     234\n",
      "H-2-KD        229\n",
      "HLA-B1503     165\n",
      "HLA-A0202     126\n",
      "HLA-A2902     118\n",
      "HLA-B5401      79\n",
      "HLA-B4002      74\n",
      "HLA-B4501      65\n",
      "Name: allele, dtype: int64\n",
      "HLA-A0201     9565\n",
      "HLA-A0301     6141\n",
      "HLA-A0203     5542\n",
      "HLA-A1101     5399\n",
      "HLA-A0206     4827\n",
      "HLA-A3101     4796\n",
      "HLA-A6802     4768\n",
      "HLA-A0202     3919\n",
      "HLA-A0101     3725\n",
      "HLA-B0702     3412\n",
      "H-2-KB        3407\n",
      "H-2-DB        3216\n",
      "HLA-B1501     3213\n",
      "HLA-A6801     3184\n",
      "HLA-A3301     3040\n",
      "HLA-B2705     3028\n",
      "HLA-A2601     2894\n",
      "HLA-B4001     2718\n",
      "HLA-B5801     2564\n",
      "HLA-A2402     2533\n",
      "HLA-B3501     2397\n",
      "HLA-A2902     2397\n",
      "HLA-B0801     2267\n",
      "Mamu-A01      2264\n",
      "HLA-A6901     2079\n",
      "HLA-B1801     2052\n",
      "HLA-A3001     2040\n",
      "HLA-A2301     2021\n",
      "HLA-B5701     1857\n",
      "HLA-B5101     1734\n",
      "              ... \n",
      "HLA-B3801      136\n",
      "HLA-A0250      132\n",
      "HLA-B7301      115\n",
      "HLA-A11         74\n",
      "HLA-A0207       68\n",
      "HLA-B7          64\n",
      "HLA-A0205       56\n",
      "Mamu-A07        53\n",
      "Mamu-A2601      51\n",
      "HLA-A2          44\n",
      "HLA-B5802       42\n",
      "HLA-A0210       18\n",
      "Gogo-B0101      15\n",
      "ELA-A1          14\n",
      "HLA-A0302       10\n",
      "Patr-B1701       8\n",
      "HLA-B3503        7\n",
      "HLA-A6601        4\n",
      "HLA-B2702        4\n",
      "HLA-A26          4\n",
      "HLA-B1402        3\n",
      "HLA-B4201        3\n",
      "Mamu-B52         2\n",
      "HLA-B2701        2\n",
      "Mamu-B04         2\n",
      "Patr-B0901       1\n",
      "HLA-B3508        1\n",
      "HLA-B44          1\n",
      "HLA-E0101        1\n",
      "Patr-A0602       1\n",
      "Name: allele, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "validation_allele_counts = validation_df.allele.value_counts()\n",
    "train_allele_counts = all_train_data._df.allele.value_counts()\n",
    "print(validation_allele_counts)\n",
    "print(train_allele_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping allele HLA-B2703\n",
      "Dropping allele HLA-B3801\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alleles = sorted(train_allele_counts.index[\n",
    "    (train_allele_counts >= min_peptides_to_consider_allele)\n",
    "    & (train_allele_counts.index.isin(validation_allele_counts.index))\n",
    "], key=lambda allele: -1 * train_allele_counts[allele])\n",
    "alleles\n",
    "not_dropped = []\n",
    "for allele in alleles:\n",
    "    sub = all_train_data.get_allele(allele)._df\n",
    "    if (sub.affinity < 500).sum() < 5 or (sub.affinity > 500).sum() < 5:\n",
    "        print(\"Dropping allele %s\" % allele)\n",
    "    else:\n",
    "        not_dropped.append(allele)\n",
    "alleles = not_dropped\n",
    "len(alleles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 models\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'activation',\n",
       " 'dropout_probability',\n",
       " 'embedding_output_dim',\n",
       " 'fraction_negative',\n",
       " 'impute',\n",
       " 'layer_sizes'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout_probabilities = [0.5]\n",
    "embedding_output_dims_and_layer_sizes_list = [(32, [64])] # , (8, [4])]\n",
    "activations = [\"tanh\"]\n",
    "\n",
    "models_params_list = []\n",
    "\n",
    "for model_num in range(10):\n",
    "    for fraction_negative in [.2]:\n",
    "        for impute in [True]:\n",
    "            for dropout_probability in dropout_probabilities:\n",
    "                for (embedding_output_dim, layer_sizes) in embedding_output_dims_and_layer_sizes_list:\n",
    "                    for activation in activations:\n",
    "                        models_params_list.append(dict(\n",
    "                            fraction_negative=fraction_negative,\n",
    "                            impute=impute,\n",
    "                            dropout_probability=dropout_probability,  \n",
    "                            embedding_output_dim=embedding_output_dim,\n",
    "                            layer_sizes=layer_sizes,\n",
    "                            activation=activation))\n",
    "\n",
    "print(\"%d models\" % len(models_params_list))\n",
    "models_params_explored = set.union(*[set(x) for x in models_params_list])\n",
    "models_params_explored\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_scores(ic50_y, ic50_y_pred, sample_weight=None, threshold_nm=500):     \n",
    "    y_pred = mhcflurry.regression_target.ic50_to_regression_target(ic50_y_pred, max_ic50)\n",
    "    try:\n",
    "        auc = sklearn.metrics.roc_auc_score(ic50_y <= threshold_nm, y_pred, sample_weight=sample_weight)\n",
    "    except ValueError:\n",
    "        auc = numpy.nan\n",
    "    try:\n",
    "        f1 = sklearn.metrics.f1_score(ic50_y <= threshold_nm, ic50_y_pred <= threshold_nm, sample_weight=sample_weight)\n",
    "    except ValueError:\n",
    "        f1 = numpy.nan\n",
    "    try:\n",
    "        tau = scipy.stats.kendalltau(ic50_y_pred, ic50_y)[0]\n",
    "    except ValueError:\n",
    "        tau = numpy.nan\n",
    "    \n",
    "    return dict(\n",
    "        auc=auc,\n",
    "        f1=f1,\n",
    "        tau=tau,\n",
    "    )    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "models_and_scores = {}\n",
    "validation_df_with_mhcflurry = validation_df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allele 0 model 0\n",
      "Fitting model for allele HLA-A0201 (9565 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 52 sec\n",
      "test set size: 2126\n",
      "{'tau': 0.62557849460708048, 'auc': 0.92894540014958871, 'f1': 0.87652173913043485}\n",
      "Allele 0 model 1\n",
      "Fitting model for allele HLA-A0201 (9565 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 52 sec\n",
      "test set size: 2126\n",
      "{'tau': 0.62636919613143582, 'auc': 0.9298901235887026, 'f1': 0.867288060363959}\n",
      "Allele 0 model 2\n",
      "Fitting model for allele HLA-A0201 (9565 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 52 sec\n",
      "test set size: 2126\n",
      "{'tau': 0.62753539061712893, 'auc': 0.92956245325355269, 'f1': 0.87570867858700396}\n",
      "Allele 0 model 3\n",
      "Fitting model for allele HLA-A0201 (9565 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 52 sec\n",
      "test set size: 2126\n",
      "{'tau': 0.62756576754319193, 'auc': 0.93028145813299146, 'f1': 0.87732986562635462}\n",
      "Allele 0 model 4\n",
      "Fitting model for allele HLA-A0201 (9565 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 51 sec\n",
      "test set size: 2126\n",
      "{'tau': 0.62718215085114282, 'auc': 0.93135840723724039, 'f1': 0.87385421213443915}\n",
      "Allele 0 model 5\n",
      "Fitting model for allele HLA-A0201 (9565 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 52 sec\n",
      "test set size: 2126\n",
      "{'tau': 0.62688549895327517, 'auc': 0.93049916301599167, 'f1': 0.87667676330592825}\n",
      "Allele 0 model 6\n",
      "Fitting model for allele HLA-A0201 (9565 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 52 sec\n",
      "test set size: 2126\n",
      "{'tau': 0.62933342422045746, 'auc': 0.93057929978274023, 'f1': 0.87716262975778547}\n",
      "Allele 0 model 7\n",
      "Fitting model for allele HLA-A0201 (9565 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 54 sec\n",
      "test set size: 2126\n",
      "{'tau': 0.62778993151880491, 'auc': 0.92982779499234247, 'f1': 0.87619877942458591}\n",
      "Allele 0 model 8\n",
      "Fitting model for allele HLA-A0201 (9565 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 55 sec\n",
      "test set size: 2126\n",
      "{'tau': 0.62974592490154235, 'auc': 0.93042882074295685, 'f1': 0.87220026350461144}\n",
      "Allele 0 model 9\n",
      "Fitting model for allele HLA-A0201 (9565 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 52 sec\n",
      "test set size: 2126\n",
      "{'tau': 0.62697756693898776, 'auc': 0.92947786444420699, 'f1': 0.87341217696014017}\n",
      "Allele 1 model 0\n",
      "Fitting model for allele HLA-A0301 (6141 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 34 sec\n",
      "test set size: 811\n",
      "{'tau': 0.59500763348434627, 'auc': 0.92570433681544784, 'f1': 0.87086446104589121}\n",
      "Allele 1 model 1\n",
      "Fitting model for allele HLA-A0301 (6141 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 37 sec\n",
      "test set size: 811\n",
      "{'tau': 0.59506893624226365, 'auc': 0.9233175055397278, 'f1': 0.86723768736616702}\n",
      "Allele 1 model 2\n",
      "Fitting model for allele HLA-A0301 (6141 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 35 sec\n",
      "test set size: 811\n",
      "{'tau': 0.59307046633415594, 'auc': 0.92271604938271601, 'f1': 0.87299893276414087}\n",
      "Allele 1 model 3\n",
      "Fitting model for allele HLA-A0301 (6141 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 34 sec\n",
      "test set size: 811\n",
      "{'tau': 0.59261069564977531, 'auc': 0.92302627413738514, 'f1': 0.8654467168998925}\n",
      "Allele 1 model 4\n",
      "Fitting model for allele HLA-A0301 (6141 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 35 sec\n",
      "test set size: 811\n",
      "{'tau': 0.59312563881628155, 'auc': 0.92041785375118701, 'f1': 0.87315010570824525}\n",
      "Allele 1 model 5\n",
      "Fitting model for allele HLA-A0301 (6141 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 34 sec\n",
      "test set size: 811\n",
      "{'tau': 0.59800533834650793, 'auc': 0.92495726495726494, 'f1': 0.87856388595564927}\n",
      "Allele 1 model 6\n",
      "Fitting model for allele HLA-A0301 (6141 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 34 sec\n",
      "test set size: 811\n",
      "{'tau': 0.59157467904097094, 'auc': 0.92364672364672362, 'f1': 0.87021276595744679}\n",
      "Allele 1 model 7\n",
      "Fitting model for allele HLA-A0301 (6141 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 34 sec\n",
      "test set size: 811\n",
      "{'tau': 0.59071644043012705, 'auc': 0.91896169673947448, 'f1': 0.86532343584305405}\n",
      "Allele 1 model 8\n",
      "Fitting model for allele HLA-A0301 (6141 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 34 sec\n",
      "test set size: 811\n",
      "{'tau': 0.58935551920436047, 'auc': 0.92127255460588797, 'f1': 0.86329386437029065}\n",
      "Allele 1 model 9\n",
      "Fitting model for allele HLA-A0301 (6141 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 33 sec\n",
      "test set size: 811\n",
      "{'tau': 0.59127429552717559, 'auc': 0.92041152263374482, 'f1': 0.86236559139784941}\n",
      "Allele 2 model 0\n",
      "Fitting model for allele HLA-A0203 (5542 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 35 sec\n",
      "test set size: 651\n",
      "{'tau': 0.59225704188825479, 'auc': 0.97718309859154928, 'f1': 0.95203836930455632}\n",
      "Allele 2 model 1\n",
      "Fitting model for allele HLA-A0203 (5542 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 35 sec\n",
      "test set size: 651\n",
      "{'tau': 0.59009616540657028, 'auc': 0.97743348982785605, 'f1': 0.95089820359281441}\n",
      "Allele 2 model 2\n",
      "Fitting model for allele HLA-A0203 (5542 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 33 sec\n",
      "test set size: 651\n",
      "{'tau': 0.59028739341379899, 'auc': 0.97742305685967656, 'f1': 0.94471153846153844}\n",
      "Allele 2 model 3\n",
      "Fitting model for allele HLA-A0203 (5542 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 35 sec\n",
      "test set size: 651\n",
      "{'tau': 0.58287730813368599, 'auc': 0.97644235785080868, 'f1': 0.94988066825775652}\n",
      "Allele 2 model 4\n",
      "Fitting model for allele HLA-A0203 (5542 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 34 sec\n",
      "test set size: 651\n",
      "{'tau': 0.59146344565825559, 'auc': 0.9778612415232133, 'f1': 0.95340501792114707}\n",
      "Allele 2 model 5\n",
      "Fitting model for allele HLA-A0203 (5542 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 33 sec\n",
      "test set size: 651\n",
      "{'tau': 0.58909221836861947, 'auc': 0.97610850286906625, 'f1': 0.95351609058402853}\n",
      "Allele 2 model 6\n",
      "Fitting model for allele HLA-A0203 (5542 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 34 sec\n",
      "test set size: 651\n",
      "{'tau': 0.59035432321632908, 'auc': 0.97732916014606164, 'f1': 0.95362663495838285}\n",
      "Allele 2 model 7\n",
      "Fitting model for allele HLA-A0203 (5542 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 35 sec\n",
      "test set size: 651\n",
      "{'tau': 0.5942266903627107, 'auc': 0.97608763693270739, 'f1': 0.95340501792114707}\n",
      "Allele 2 model 8\n",
      "Fitting model for allele HLA-A0203 (5542 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 36 sec\n",
      "test set size: 651\n",
      "{'tau': 0.59026827061307618, 'auc': 0.97683881064162759, 'f1': 0.95600475624256831}\n",
      "Allele 2 model 9\n",
      "Fitting model for allele HLA-A0203 (5542 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 34 sec\n",
      "test set size: 651\n",
      "{'tau': 0.5968369526613827, 'auc': 0.97634846113719354, 'f1': 0.94597839135654271}\n",
      "Allele 3 model 0\n",
      "Fitting model for allele HLA-A1101 (5399 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 33 sec\n",
      "test set size: 723\n",
      "{'tau': 0.60976145209289867, 'auc': 0.93884632509864729, 'f1': 0.88888888888888895}\n",
      "Allele 3 model 1\n",
      "Fitting model for allele HLA-A1101 (5399 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 34 sec\n",
      "test set size: 723\n",
      "{'tau': 0.60552303536935492, 'auc': 0.93604427998955952, 'f1': 0.89073305670816039}\n",
      "Allele 3 model 2\n",
      "Fitting model for allele HLA-A1101 (5399 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 32 sec\n",
      "test set size: 723\n",
      "{'tau': 0.60817786782256367, 'auc': 0.93594448112266049, 'f1': 0.89073305670816039}\n",
      "Allele 3 model 3\n",
      "Fitting model for allele HLA-A1101 (5399 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 32 sec\n",
      "test set size: 723\n",
      "{'tau': 0.60941213203326594, 'auc': 0.93787136693740325, 'f1': 0.88674033149171272}\n",
      "Allele 3 model 4\n",
      "Fitting model for allele HLA-A1101 (5399 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 32 sec\n",
      "test set size: 723\n",
      "{'tau': 0.60896189728973937, 'auc': 0.93770247654726635, 'f1': 0.89444444444444449}\n",
      "Allele 3 model 5\n",
      "Fitting model for allele HLA-A1101 (5399 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 33 sec\n",
      "test set size: 723\n",
      "{'tau': 0.61101124163958465, 'auc': 0.93676590256559855, 'f1': 0.88765603328710119}\n",
      "Allele 3 model 6\n",
      "Fitting model for allele HLA-A1101 (5399 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 35 sec\n",
      "test set size: 723\n",
      "{'tau': 0.61417841018025476, 'auc': 0.93863905052893404, 'f1': 0.8879551820728292}\n",
      "Allele 3 model 7\n",
      "Fitting model for allele HLA-A1101 (5399 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 34 sec\n",
      "test set size: 723\n",
      "{'tau': 0.61067744691593562, 'auc': 0.93721115904868657, 'f1': 0.88488210818307911}\n",
      "Allele 3 model 8\n",
      "Fitting model for allele HLA-A1101 (5399 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 36 sec\n",
      "test set size: 723\n",
      "{'tau': 0.60731621167546956, 'auc': 0.93691176244798946, 'f1': 0.89133425034387903}\n",
      "Allele 3 model 9\n",
      "Fitting model for allele HLA-A1101 (5399 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 33 sec\n",
      "test set size: 723\n",
      "{'tau': 0.60553079803734677, 'auc': 0.9347008337043804, 'f1': 0.88315217391304357}\n",
      "Allele 4 model 0\n",
      "Fitting model for allele HLA-A0206 (4827 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 32 sec\n",
      "test set size: 682\n",
      "{'tau': 0.54004535657907837, 'auc': 0.90621341778814302, 'f1': 0.87268232385661315}\n",
      "Allele 4 model 1\n",
      "Fitting model for allele HLA-A0206 (4827 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 32 sec\n",
      "test set size: 682\n",
      "{'tau': 0.53221272345982107, 'auc': 0.90329137816818972, 'f1': 0.87453416149068319}\n",
      "Allele 4 model 2\n",
      "Fitting model for allele HLA-A0206 (4827 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 32 sec\n",
      "test set size: 682\n",
      "{'tau': 0.53988109606112489, 'auc': 0.90685771844968555, 'f1': 0.8681592039800996}\n",
      "Allele 4 model 3\n",
      "Fitting model for allele HLA-A0206 (4827 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 32 sec\n",
      "test set size: 682\n",
      "{'tau': 0.53586968130689161, 'auc': 0.90343657268346689, 'f1': 0.86956521739130443}\n",
      "Allele 4 model 4\n",
      "Fitting model for allele HLA-A0206 (4827 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 32 sec\n",
      "test set size: 682\n",
      "{'tau': 0.5391289557947061, 'auc': 0.90556004246939581, 'f1': 0.86891385767790252}\n",
      "Allele 4 model 5\n",
      "Fitting model for allele HLA-A0206 (4827 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 32 sec\n",
      "test set size: 682\n",
      "{'tau': 0.53934508805517134, 'auc': 0.90535132535368479, 'f1': 0.8671679197994987}\n",
      "Allele 4 model 6\n",
      "Fitting model for allele HLA-A0206 (4827 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 33 sec\n",
      "test set size: 682\n",
      "{'tau': 0.53438269135489125, 'auc': 0.90467980072052789, 'f1': 0.86741016109045843}\n",
      "Allele 4 model 7\n",
      "Fitting model for allele HLA-A0206 (4827 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 33 sec\n",
      "test set size: 682\n",
      "{'tau': 0.53960444676772945, 'auc': 0.90497018975108223, 'f1': 0.86956521739130443}\n",
      "Allele 4 model 8\n",
      "Fitting model for allele HLA-A0206 (4827 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 33 sec\n",
      "test set size: 682\n",
      "{'tau': 0.53495328052251934, 'auc': 0.9040990226594191, 'f1': 0.86967418546365904}\n",
      "Allele 4 model 9\n",
      "Fitting model for allele HLA-A0206 (4827 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 34 sec\n",
      "test set size: 682\n",
      "{'tau': 0.54166202588735779, 'auc': 0.90707551022260136, 'f1': 0.87141073657927581}\n",
      "Allele 5 model 0\n",
      "Fitting model for allele HLA-A3101 (4796 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 31 sec\n",
      "test set size: 724\n",
      "{'tau': 0.50156618059976321, 'auc': 0.85233927093233586, 'f1': 0.81829733163913598}\n",
      "Allele 5 model 1\n",
      "Fitting model for allele HLA-A3101 (4796 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 31 sec\n",
      "test set size: 724\n",
      "{'tau': 0.49905792522277748, 'auc': 0.85206334120244043, 'f1': 0.81032258064516138}\n",
      "Allele 5 model 2\n",
      "Fitting model for allele HLA-A3101 (4796 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 31 sec\n",
      "test set size: 724\n",
      "{'tau': 0.50544818815245174, 'auc': 0.85272250666830174, 'f1': 0.80823680823680821}\n",
      "Allele 5 model 3\n",
      "Fitting model for allele HLA-A3101 (4796 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 36 sec\n",
      "test set size: 724\n",
      "{'tau': 0.50278268025835038, 'auc': 0.85298693932611835, 'f1': 0.81679389312977091}\n",
      "Allele 5 model 4\n",
      "Fitting model for allele HLA-A3101 (4796 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 31 sec\n",
      "test set size: 724\n",
      "{'tau': 0.50056287844896885, 'auc': 0.84869853144065976, 'f1': 0.81273885350318475}\n",
      "Allele 5 model 5\n",
      "Fitting model for allele HLA-A3101 (4796 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 31 sec\n",
      "test set size: 724\n",
      "{'tau': 0.50435999120428265, 'auc': 0.85255388294447676, 'f1': 0.8122605363984674}\n",
      "Allele 5 model 6\n",
      "Fitting model for allele HLA-A3101 (4796 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 31 sec\n",
      "test set size: 724\n",
      "{'tau': 0.50581863817736039, 'auc': 0.85309041297482913, 'f1': 0.80976863753213368}\n",
      "Allele 5 model 7\n",
      "Fitting model for allele HLA-A3101 (4796 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 31 sec\n",
      "test set size: 724\n",
      "{'tau': 0.5009333284738775, 'auc': 0.85280681853021434, 'f1': 0.81377551020408168}\n",
      "Allele 5 model 8\n",
      "Fitting model for allele HLA-A3101 (4796 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 31 sec\n",
      "test set size: 724\n",
      "{'tau': 0.50693770596093868, 'auc': 0.85374191372597108, 'f1': 0.81177976952624831}\n",
      "Allele 5 model 9\n",
      "Fitting model for allele HLA-A3101 (4796 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 31 sec\n",
      "test set size: 724\n",
      "{'tau': 0.5049774079124637, 'auc': 0.85384921973204153, 'f1': 0.80258064516129024}\n",
      "Allele 6 model 0\n",
      "Fitting model for allele HLA-A6802 (4768 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 32 sec\n",
      "test set size: 669\n",
      "{'tau': 0.63986662740622557, 'auc': 0.96729164797703415, 'f1': 0.87076923076923074}\n",
      "Allele 6 model 1\n",
      "Fitting model for allele HLA-A6802 (4768 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 32 sec\n",
      "test set size: 669\n",
      "{'tau': 0.63741615709275767, 'auc': 0.96660985018390599, 'f1': 0.87671232876712335}\n",
      "Allele 6 model 2\n",
      "Fitting model for allele HLA-A6802 (4768 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 32 sec\n",
      "test set size: 669\n",
      "{'tau': 0.63565290354616644, 'auc': 0.96775814120391135, 'f1': 0.87289433384379789}\n",
      "Allele 6 model 3\n",
      "Fitting model for allele HLA-A6802 (4768 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 32 sec\n",
      "test set size: 669\n",
      "{'tau': 0.63334711044677772, 'auc': 0.9654705301874944, 'f1': 0.87537993920972645}\n",
      "Allele 6 model 4\n",
      "Fitting model for allele HLA-A6802 (4768 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 32 sec\n",
      "test set size: 669\n",
      "{'tau': 0.63126737549438805, 'auc': 0.96524625459764957, 'f1': 0.86419753086419759}\n",
      "Allele 6 model 5\n",
      "Fitting model for allele HLA-A6802 (4768 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 32 sec\n",
      "test set size: 669\n",
      "{'tau': 0.63152056061902673, 'auc': 0.9652641966448372, 'f1': 0.87195121951219512}\n",
      "Allele 6 model 6\n",
      "Fitting model for allele HLA-A6802 (4768 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 32 sec\n",
      "test set size: 669\n",
      "{'tau': 0.63856453247951206, 'auc': 0.96680721270296943, 'f1': 0.87461773700305812}\n",
      "Allele 6 model 7\n",
      "Fitting model for allele HLA-A6802 (4768 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 32 sec\n",
      "test set size: 669\n",
      "{'tau': 0.63777785012795596, 'auc': 0.96809006907688167, 'f1': 0.88350983358547663}\n",
      "Allele 6 model 8\n",
      "Fitting model for allele HLA-A6802 (4768 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 32 sec\n",
      "test set size: 669\n",
      "{'tau': 0.63180991504718531, 'auc': 0.96566789270655784, 'f1': 0.87841945288753809}\n",
      "Allele 6 model 9\n",
      "Fitting model for allele HLA-A6802 (4768 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 32 sec\n",
      "test set size: 669\n",
      "{'tau': 0.63228915831882293, 'auc': 0.96666367632546868, 'f1': 0.87804878048780499}\n",
      "Allele 7 model 0\n",
      "Fitting model for allele HLA-A0202 (3919 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 31 sec\n",
      "test set size: 126\n",
      "{'tau': 0.61572630023288299, 'auc': 0.90604212860310429, 'f1': 0.79545454545454553}\n",
      "Allele 7 model 1\n",
      "Fitting model for allele HLA-A0202 (3919 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 31 sec\n",
      "test set size: 126\n",
      "{'tau': 0.62544896776201231, 'auc': 0.90576496674057638, 'f1': 0.77108433734939763}\n",
      "Allele 7 model 2\n",
      "Fitting model for allele HLA-A0202 (3919 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 31 sec\n",
      "test set size: 126\n",
      "{'tau': 0.61623801957652147, 'auc': 0.90687361419068746, 'f1': 0.7865168539325843}\n",
      "Allele 7 model 3\n",
      "Fitting model for allele HLA-A0202 (3919 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 31 sec\n",
      "test set size: 126\n",
      "{'tau': 0.62570482743383149, 'auc': 0.91241685144124185, 'f1': 0.8222222222222223}\n",
      "Allele 7 model 4\n",
      "Fitting model for allele HLA-A0202 (3919 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 31 sec\n",
      "test set size: 126\n",
      "{'tau': 0.61726145826379819, 'auc': 0.90853658536585369, 'f1': 0.77647058823529425}\n",
      "Allele 7 model 5\n",
      "Fitting model for allele HLA-A0202 (3919 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 31 sec\n",
      "test set size: 126\n",
      "{'tau': 0.61086496646831845, 'auc': 0.9007760532150777, 'f1': 0.73809523809523814}\n",
      "Allele 7 model 6\n",
      "Fitting model for allele HLA-A0202 (3919 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 32 sec\n",
      "test set size: 126\n",
      "{'tau': 0.61623801957652147, 'auc': 0.90964523281596454, 'f1': 0.78160919540229878}\n",
      "Allele 7 model 7\n",
      "Fitting model for allele HLA-A0202 (3919 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 32 sec\n",
      "test set size: 126\n",
      "{'tau': 0.62135521301290519, 'auc': 0.90327050997782699, 'f1': 0.78571428571428559}\n",
      "Allele 7 model 8\n",
      "Fitting model for allele HLA-A0202 (3919 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n",
      "Trained in 31 sec\n",
      "test set size: 126\n",
      "{'tau': 0.62084349366926683, 'auc': 0.908259423503326, 'f1': 0.79069767441860461}\n",
      "Allele 7 model 9\n",
      "Fitting model for allele HLA-A0202 (3919 + 19304): {'impute': True, 'dropout_probability': 0.5, 'embedding_output_dim': 32, 'layer_sizes': [64], 'fraction_negative': 0.2, 'activation': 'tanh'}\n"
     ]
    }
   ],
   "source": [
    "# train and test models, adding columns to validation_df_with_mhcflurry\n",
    "pandas.DataFrame(models_params_list).to_csv(\"../data/validation_models.csv\", index=False)\n",
    "\n",
    "def make_and_fit_model(allele, original_params):\n",
    "    params = dict(original_params)\n",
    "    impute = params[\"impute\"]\n",
    "    del params[\"impute\"]\n",
    "    \n",
    "    fraction_negative = params[\"fraction_negative\"]\n",
    "    del params[\"fraction_negative\"]\n",
    "    \n",
    "    model = mhcflurry.Class1BindingPredictor.from_hyperparameters(max_ic50=max_ic50, **params)\n",
    "    print(\"Fitting model for allele %s (%d + %d): %s\" % (\n",
    "            allele,\n",
    "            len(all_train_data.groupby_allele_dictionary()[allele]),\n",
    "            len(imputed_train_data.groupby_allele_dictionary()[allele]),\n",
    "            str(original_params)))\n",
    "    t = -time.time()\n",
    "    model.fit_dataset(\n",
    "        all_train_data.get_allele(allele),\n",
    "        pretraining_dataset=imputed_train_data.get_allele(allele) if impute else None,\n",
    "        verbose=False,\n",
    "        batch_size=128,\n",
    "        n_training_epochs=250,\n",
    "        n_random_negative_samples=int(fraction_negative * len(all_train_data.get_allele(allele))))\n",
    "    t += time.time()\n",
    "    print(\"Trained in %d sec\" % t)\n",
    "    return model\n",
    "\n",
    "for (i, allele) in enumerate(alleles):\n",
    "    if allele not in validation_df_with_mhcflurry.allele.unique():\n",
    "        print(\"Skipping allele %s: not in test set\" % allele)\n",
    "        continue\n",
    "    if allele in models_and_scores:\n",
    "        print(\"Skipping allele %s: already done\" % allele)\n",
    "        continue\n",
    "    values_for_allele = []\n",
    "    for (j, params) in enumerate(models_params_list):\n",
    "        print(\"Allele %d model %d\" % (i, j))\n",
    "        model = make_and_fit_model(allele, params)\n",
    "        predictions = model.predict(\n",
    "            list(validation_df_with_mhcflurry.ix[validation_df_with_mhcflurry.allele == allele].peptide))\n",
    "        print(\"test set size: %d\" % len(predictions))\n",
    "        validation_df_with_mhcflurry.loc[(validation_df_with_mhcflurry.allele == allele),\n",
    "                                         (\"mhcflurry %d\" % j)] = predictions\n",
    "        scores = make_scores(validation_df_with_mhcflurry.ix[validation_df.allele == allele].meas,\n",
    "                            predictions)\n",
    "        print(scores)\n",
    "        values_for_allele.append((params, scores))\n",
    "        \n",
    "    models_and_scores[allele] = values_for_allele\n",
    "    \n",
    "    # Write out all data after each allele.\n",
    "    validation_df_with_mhcflurry_results = validation_df_with_mhcflurry.ix[validation_df_with_mhcflurry.allele.isin(models_and_scores)]\n",
    "    validation_df_with_mhcflurry_results.to_csv(\"../data/validation_predictions_full.csv\", index=False)\n",
    "    \n",
    "    scores_df = collections.defaultdict(list)\n",
    "    predictors = validation_df_with_mhcflurry_results.columns[4:]\n",
    "\n",
    "    for (allele, grouped) in validation_df_with_mhcflurry_results.groupby(\"allele\"):\n",
    "        scores_df[\"allele\"].append(allele)\n",
    "        scores_df[\"test_size\"].append(len(grouped.meas))\n",
    "        for predictor in predictors:\n",
    "            scores = make_scores(grouped.meas, grouped[predictor])\n",
    "            for (key, value) in scores.items():\n",
    "                scores_df[\"%s_%s\" % (predictor, key)].append(value)\n",
    "\n",
    "    scores_df = pandas.DataFrame(scores_df)\n",
    "    scores_df[\"train_size\"] = [\n",
    "        len(all_train_data.groupby_allele_dictionary()[a])\n",
    "        for a in scores_df.allele\n",
    "    ]\n",
    "\n",
    "    scores_df.index = scores_df.allele\n",
    "    scores_df.to_csv(\"../data/validation_scores.csv\", index=False)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_full(scores_df[[\"train_size\", \"test_size\"]].sort(\"train_size\", inplace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
