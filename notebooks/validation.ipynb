{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 1: GeForce GTX TITAN X (CNMeM is enabled with initial size: 75.0% of memory, cuDNN 5004)\n",
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"THEANO_FLAGS\"] = 'cuda.root=/usr/local/cuda,floatX=float32,device=gpu1,force_device=False,lib.cnmem=.75'\n",
    "\n",
    "import theano\n",
    "print(theano.config.device)\n",
    "\n",
    "import mhcflurry, seaborn, numpy, pandas, pickle, sklearn, collections, scipy, time\n",
    "import mhcflurry.data\n",
    "import mhcflurry.imputation\n",
    "import fancyimpute, locale\n",
    "\n",
    "import sklearn.metrics\n",
    "import sklearn.cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_ic50 = 50000\n",
    "min_peptides_to_consider_allele = 10\n",
    "data_dir = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_train_data = mhcflurry.data.load_allele_datasets(data_dir + \"bdata.2009.mhci.public.1.txt\")\n",
    "#all_validation_data = mhcflurry.data.load_allele_datasets(data_dir + \"bdata.2013.mhci.public.blind.1.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 12235 peptides with <2 observations\n",
      "Dropping 9 alleles with <10 observations: ['ELA-A1', 'HLA-B2701', 'HLA-B3508', 'HLA-B44', 'HLA-E0101', 'Mamu-B04', 'Patr-A0602', 'Patr-B0901', 'Patr-B1701']\n",
      "[MICE] Completing matrix with shape (19304, 97)\n",
      "[MICE] Starting imputation round 1/110, elapsed time 0.023\n",
      "[MICE] Starting imputation round 2/110, elapsed time 1.659\n",
      "[MICE] Starting imputation round 3/110, elapsed time 3.079\n",
      "[MICE] Starting imputation round 4/110, elapsed time 4.573\n",
      "[MICE] Starting imputation round 5/110, elapsed time 5.993\n",
      "[MICE] Starting imputation round 6/110, elapsed time 7.489\n",
      "[MICE] Starting imputation round 7/110, elapsed time 9.023\n",
      "[MICE] Starting imputation round 8/110, elapsed time 10.403\n",
      "[MICE] Starting imputation round 9/110, elapsed time 11.871\n",
      "[MICE] Starting imputation round 10/110, elapsed time 13.326\n",
      "[MICE] Starting imputation round 11/110, elapsed time 14.847\n",
      "[MICE] Starting imputation round 12/110, elapsed time 16.409\n",
      "[MICE] Starting imputation round 13/110, elapsed time 18.009\n",
      "[MICE] Starting imputation round 14/110, elapsed time 19.608\n",
      "[MICE] Starting imputation round 15/110, elapsed time 21.210\n",
      "[MICE] Starting imputation round 16/110, elapsed time 22.803\n",
      "[MICE] Starting imputation round 17/110, elapsed time 24.418\n",
      "[MICE] Starting imputation round 18/110, elapsed time 25.873\n",
      "[MICE] Starting imputation round 19/110, elapsed time 27.365\n",
      "[MICE] Starting imputation round 20/110, elapsed time 28.772\n",
      "[MICE] Starting imputation round 21/110, elapsed time 30.200\n",
      "[MICE] Starting imputation round 22/110, elapsed time 31.690\n",
      "[MICE] Starting imputation round 23/110, elapsed time 33.255\n",
      "[MICE] Starting imputation round 24/110, elapsed time 34.803\n",
      "[MICE] Starting imputation round 25/110, elapsed time 36.280\n",
      "[MICE] Starting imputation round 26/110, elapsed time 37.710\n",
      "[MICE] Starting imputation round 27/110, elapsed time 39.165\n",
      "[MICE] Starting imputation round 28/110, elapsed time 40.658\n",
      "[MICE] Starting imputation round 29/110, elapsed time 42.126\n",
      "[MICE] Starting imputation round 30/110, elapsed time 43.649\n",
      "[MICE] Starting imputation round 31/110, elapsed time 45.144\n",
      "[MICE] Starting imputation round 32/110, elapsed time 46.617\n",
      "[MICE] Starting imputation round 33/110, elapsed time 48.154\n",
      "[MICE] Starting imputation round 34/110, elapsed time 49.621\n",
      "[MICE] Starting imputation round 35/110, elapsed time 51.118\n",
      "[MICE] Starting imputation round 36/110, elapsed time 52.594\n",
      "[MICE] Starting imputation round 37/110, elapsed time 54.057\n",
      "[MICE] Starting imputation round 38/110, elapsed time 55.516\n",
      "[MICE] Starting imputation round 39/110, elapsed time 57.027\n",
      "[MICE] Starting imputation round 40/110, elapsed time 58.467\n",
      "[MICE] Starting imputation round 41/110, elapsed time 59.926\n",
      "[MICE] Starting imputation round 42/110, elapsed time 61.408\n",
      "[MICE] Starting imputation round 43/110, elapsed time 62.885\n",
      "[MICE] Starting imputation round 44/110, elapsed time 64.415\n",
      "[MICE] Starting imputation round 45/110, elapsed time 65.860\n",
      "[MICE] Starting imputation round 46/110, elapsed time 67.341\n",
      "[MICE] Starting imputation round 47/110, elapsed time 68.783\n",
      "[MICE] Starting imputation round 48/110, elapsed time 70.170\n",
      "[MICE] Starting imputation round 49/110, elapsed time 71.629\n",
      "[MICE] Starting imputation round 50/110, elapsed time 73.095\n",
      "[MICE] Starting imputation round 51/110, elapsed time 74.553\n",
      "[MICE] Starting imputation round 52/110, elapsed time 76.010\n",
      "[MICE] Starting imputation round 53/110, elapsed time 77.442\n",
      "[MICE] Starting imputation round 54/110, elapsed time 78.830\n",
      "[MICE] Starting imputation round 55/110, elapsed time 80.283\n",
      "[MICE] Starting imputation round 56/110, elapsed time 81.748\n",
      "[MICE] Starting imputation round 57/110, elapsed time 83.171\n",
      "[MICE] Starting imputation round 58/110, elapsed time 84.609\n",
      "[MICE] Starting imputation round 59/110, elapsed time 86.055\n",
      "[MICE] Starting imputation round 60/110, elapsed time 87.590\n",
      "[MICE] Starting imputation round 61/110, elapsed time 89.106\n",
      "[MICE] Starting imputation round 62/110, elapsed time 90.513\n",
      "[MICE] Starting imputation round 63/110, elapsed time 92.019\n",
      "[MICE] Starting imputation round 64/110, elapsed time 93.594\n",
      "[MICE] Starting imputation round 65/110, elapsed time 95.159\n",
      "[MICE] Starting imputation round 66/110, elapsed time 96.725\n",
      "[MICE] Starting imputation round 67/110, elapsed time 98.172\n",
      "[MICE] Starting imputation round 68/110, elapsed time 99.618\n",
      "[MICE] Starting imputation round 69/110, elapsed time 101.093\n",
      "[MICE] Starting imputation round 70/110, elapsed time 102.662\n",
      "[MICE] Starting imputation round 71/110, elapsed time 104.223\n",
      "[MICE] Starting imputation round 72/110, elapsed time 105.784\n",
      "[MICE] Starting imputation round 73/110, elapsed time 107.349\n",
      "[MICE] Starting imputation round 74/110, elapsed time 108.934\n",
      "[MICE] Starting imputation round 75/110, elapsed time 110.386\n",
      "[MICE] Starting imputation round 76/110, elapsed time 111.804\n",
      "[MICE] Starting imputation round 77/110, elapsed time 113.300\n",
      "[MICE] Starting imputation round 78/110, elapsed time 114.869\n",
      "[MICE] Starting imputation round 79/110, elapsed time 116.437\n",
      "[MICE] Starting imputation round 80/110, elapsed time 118.025\n",
      "[MICE] Starting imputation round 81/110, elapsed time 119.528\n",
      "[MICE] Starting imputation round 82/110, elapsed time 121.004\n",
      "[MICE] Starting imputation round 83/110, elapsed time 122.479\n",
      "[MICE] Starting imputation round 84/110, elapsed time 123.950\n",
      "[MICE] Starting imputation round 85/110, elapsed time 125.423\n",
      "[MICE] Starting imputation round 86/110, elapsed time 126.889\n",
      "[MICE] Starting imputation round 87/110, elapsed time 128.357\n",
      "[MICE] Starting imputation round 88/110, elapsed time 129.835\n",
      "[MICE] Starting imputation round 89/110, elapsed time 131.298\n",
      "[MICE] Starting imputation round 90/110, elapsed time 132.766\n",
      "[MICE] Starting imputation round 91/110, elapsed time 134.237\n",
      "[MICE] Starting imputation round 92/110, elapsed time 135.714\n",
      "[MICE] Starting imputation round 93/110, elapsed time 137.181\n",
      "[MICE] Starting imputation round 94/110, elapsed time 138.640\n",
      "[MICE] Starting imputation round 95/110, elapsed time 140.103\n",
      "[MICE] Starting imputation round 96/110, elapsed time 141.564\n",
      "[MICE] Starting imputation round 97/110, elapsed time 143.025\n",
      "[MICE] Starting imputation round 98/110, elapsed time 144.481\n",
      "[MICE] Starting imputation round 99/110, elapsed time 145.945\n",
      "[MICE] Starting imputation round 100/110, elapsed time 147.405\n",
      "[MICE] Starting imputation round 101/110, elapsed time 148.861\n",
      "[MICE] Starting imputation round 102/110, elapsed time 150.326\n",
      "[MICE] Starting imputation round 103/110, elapsed time 151.791\n",
      "[MICE] Starting imputation round 104/110, elapsed time 153.258\n",
      "[MICE] Starting imputation round 105/110, elapsed time 154.722\n",
      "[MICE] Starting imputation round 106/110, elapsed time 156.189\n",
      "[MICE] Starting imputation round 107/110, elapsed time 157.655\n",
      "[MICE] Starting imputation round 108/110, elapsed time 159.118\n",
      "[MICE] Starting imputation round 109/110, elapsed time 160.577\n",
      "[MICE] Starting imputation round 110/110, elapsed time 162.034\n"
     ]
    }
   ],
   "source": [
    "imputed_train_data = mhcflurry.imputation.create_imputed_datasets(\n",
    "    all_train_data,\n",
    "    fancyimpute.MICE(),\n",
    "    min_observations_per_peptide=2,\n",
    "    min_observations_per_allele=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>allele</th>\n",
       "      <th>peptide</th>\n",
       "      <th>length</th>\n",
       "      <th>meas</th>\n",
       "      <th>netmhc</th>\n",
       "      <th>netmhcpan</th>\n",
       "      <th>smmpmbec_cpp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAACNVATA</td>\n",
       "      <td>9</td>\n",
       "      <td>657.657837</td>\n",
       "      <td>154.881662</td>\n",
       "      <td>711.213514</td>\n",
       "      <td>438.530698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAFEFVYV</td>\n",
       "      <td>8</td>\n",
       "      <td>30831.879502</td>\n",
       "      <td>6456.542290</td>\n",
       "      <td>785.235635</td>\n",
       "      <td>10351.421667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAFVNDYSL</td>\n",
       "      <td>9</td>\n",
       "      <td>77.446180</td>\n",
       "      <td>17.458222</td>\n",
       "      <td>7.516229</td>\n",
       "      <td>28.054336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAIANQAAV</td>\n",
       "      <td>9</td>\n",
       "      <td>1.999862</td>\n",
       "      <td>9.638290</td>\n",
       "      <td>9.749896</td>\n",
       "      <td>25.703958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAIANQAVV</td>\n",
       "      <td>9</td>\n",
       "      <td>1.517050</td>\n",
       "      <td>8.550667</td>\n",
       "      <td>8.336812</td>\n",
       "      <td>28.773984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAIENYVRF</td>\n",
       "      <td>9</td>\n",
       "      <td>37.844258</td>\n",
       "      <td>252.348077</td>\n",
       "      <td>114.815362</td>\n",
       "      <td>187.068214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAINFITTM</td>\n",
       "      <td>9</td>\n",
       "      <td>3.155005</td>\n",
       "      <td>199.986187</td>\n",
       "      <td>389.045145</td>\n",
       "      <td>200.909281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAIPAPPPI</td>\n",
       "      <td>9</td>\n",
       "      <td>3243.396173</td>\n",
       "      <td>1059.253725</td>\n",
       "      <td>493.173804</td>\n",
       "      <td>295.120923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAKLNRPPL</td>\n",
       "      <td>9</td>\n",
       "      <td>654.636174</td>\n",
       "      <td>66.374307</td>\n",
       "      <td>77.268059</td>\n",
       "      <td>38.459178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AALDMVDAL</td>\n",
       "      <td>9</td>\n",
       "      <td>229.614865</td>\n",
       "      <td>547.015963</td>\n",
       "      <td>597.035287</td>\n",
       "      <td>225.423921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AALQHLRSI</td>\n",
       "      <td>9</td>\n",
       "      <td>905.732601</td>\n",
       "      <td>1686.553025</td>\n",
       "      <td>2032.357011</td>\n",
       "      <td>698.232404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AALVRLTAL</td>\n",
       "      <td>9</td>\n",
       "      <td>1106.623784</td>\n",
       "      <td>435.511874</td>\n",
       "      <td>214.783047</td>\n",
       "      <td>378.442585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAMVPTGSL</td>\n",
       "      <td>9</td>\n",
       "      <td>1836.538343</td>\n",
       "      <td>4055.085354</td>\n",
       "      <td>5176.068320</td>\n",
       "      <td>1545.254440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AANSPWAPV</td>\n",
       "      <td>9</td>\n",
       "      <td>4325.138310</td>\n",
       "      <td>903.649474</td>\n",
       "      <td>1023.292992</td>\n",
       "      <td>557.185749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAPSGAAPL</td>\n",
       "      <td>9</td>\n",
       "      <td>2.844461</td>\n",
       "      <td>97.948999</td>\n",
       "      <td>501.187234</td>\n",
       "      <td>822.242650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAPVSEPTV</td>\n",
       "      <td>9</td>\n",
       "      <td>106.905488</td>\n",
       "      <td>2654.605562</td>\n",
       "      <td>1918.668741</td>\n",
       "      <td>1870.682140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAVANGFAA</td>\n",
       "      <td>9</td>\n",
       "      <td>83.368118</td>\n",
       "      <td>205.116218</td>\n",
       "      <td>228.559880</td>\n",
       "      <td>200.447203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAVLLGAPV</td>\n",
       "      <td>9</td>\n",
       "      <td>373.250158</td>\n",
       "      <td>320.626932</td>\n",
       "      <td>623.734835</td>\n",
       "      <td>286.417797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAVQLLFPA</td>\n",
       "      <td>9</td>\n",
       "      <td>347.536161</td>\n",
       "      <td>2494.594727</td>\n",
       "      <td>5152.286446</td>\n",
       "      <td>679.203633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAVTAGVAL</td>\n",
       "      <td>9</td>\n",
       "      <td>229.614865</td>\n",
       "      <td>1552.387010</td>\n",
       "      <td>2172.701179</td>\n",
       "      <td>1927.524913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAVVNRSLV</td>\n",
       "      <td>9</td>\n",
       "      <td>8.609938</td>\n",
       "      <td>26.302680</td>\n",
       "      <td>29.580125</td>\n",
       "      <td>63.826349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAYVPADAV</td>\n",
       "      <td>9</td>\n",
       "      <td>331.894458</td>\n",
       "      <td>1757.923614</td>\n",
       "      <td>3206.269325</td>\n",
       "      <td>595.662144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>ACMVNLERL</td>\n",
       "      <td>9</td>\n",
       "      <td>737.904230</td>\n",
       "      <td>239.331576</td>\n",
       "      <td>679.203633</td>\n",
       "      <td>444.631267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>ADLVNHPPV</td>\n",
       "      <td>9</td>\n",
       "      <td>558.470195</td>\n",
       "      <td>1694.337800</td>\n",
       "      <td>1857.804455</td>\n",
       "      <td>325.087297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AEMEEALKG</td>\n",
       "      <td>9</td>\n",
       "      <td>78523.563461</td>\n",
       "      <td>41304.750199</td>\n",
       "      <td>46989.410861</td>\n",
       "      <td>440554.863507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AFFAFRYV</td>\n",
       "      <td>8</td>\n",
       "      <td>87902.251683</td>\n",
       "      <td>18967.059212</td>\n",
       "      <td>12882.495517</td>\n",
       "      <td>7816.278046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AGLDNKFYL</td>\n",
       "      <td>9</td>\n",
       "      <td>659.173895</td>\n",
       "      <td>357.272838</td>\n",
       "      <td>202.768272</td>\n",
       "      <td>260.015956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AGLLFVLL</td>\n",
       "      <td>8</td>\n",
       "      <td>87902.251683</td>\n",
       "      <td>14996.848355</td>\n",
       "      <td>9885.530947</td>\n",
       "      <td>26242.185434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AGLQVINL</td>\n",
       "      <td>8</td>\n",
       "      <td>59429.215862</td>\n",
       "      <td>25061.092530</td>\n",
       "      <td>12560.299637</td>\n",
       "      <td>35399.734108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AGLVSFNFL</td>\n",
       "      <td>9</td>\n",
       "      <td>210.862815</td>\n",
       "      <td>620.869034</td>\n",
       "      <td>2285.598803</td>\n",
       "      <td>1811.340093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27650</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YNTVCVIW</td>\n",
       "      <td>8</td>\n",
       "      <td>909.913273</td>\n",
       "      <td>734.513868</td>\n",
       "      <td>10023.052381</td>\n",
       "      <td>48.865236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27651</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YRHDGGNVL</td>\n",
       "      <td>9</td>\n",
       "      <td>78342.964277</td>\n",
       "      <td>1355.189412</td>\n",
       "      <td>22233.098907</td>\n",
       "      <td>1409.288798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27652</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YSEGQYMNTP</td>\n",
       "      <td>10</td>\n",
       "      <td>7362.070975</td>\n",
       "      <td>2958.012467</td>\n",
       "      <td>8709.635900</td>\n",
       "      <td>4477.133042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27653</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YSEVALNVTES</td>\n",
       "      <td>11</td>\n",
       "      <td>7709.034691</td>\n",
       "      <td>1682.674061</td>\n",
       "      <td>5105.050000</td>\n",
       "      <td>51.522864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27654</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YSLVFVILM</td>\n",
       "      <td>9</td>\n",
       "      <td>1256.029964</td>\n",
       "      <td>17.498467</td>\n",
       "      <td>49.431069</td>\n",
       "      <td>6.324119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27655</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YSQIGAGVY</td>\n",
       "      <td>9</td>\n",
       "      <td>1.999862</td>\n",
       "      <td>17.864876</td>\n",
       "      <td>15.346170</td>\n",
       "      <td>16.292960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27656</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YSRDLICEQS</td>\n",
       "      <td>10</td>\n",
       "      <td>1059.253725</td>\n",
       "      <td>1297.179271</td>\n",
       "      <td>3863.669771</td>\n",
       "      <td>220.292646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27657</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YSYKAFIKY</td>\n",
       "      <td>9</td>\n",
       "      <td>2.317395</td>\n",
       "      <td>4.285485</td>\n",
       "      <td>3.090295</td>\n",
       "      <td>1.778279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27658</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YSYKAFIKYP</td>\n",
       "      <td>10</td>\n",
       "      <td>4830.588020</td>\n",
       "      <td>1199.499303</td>\n",
       "      <td>1753.880502</td>\n",
       "      <td>3111.716337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27659</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YSYKAFIKYPE</td>\n",
       "      <td>11</td>\n",
       "      <td>4897.788194</td>\n",
       "      <td>671.428853</td>\n",
       "      <td>2666.858665</td>\n",
       "      <td>42.559841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27660</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTAFTLPSV</td>\n",
       "      <td>9</td>\n",
       "      <td>101.624869</td>\n",
       "      <td>32.210688</td>\n",
       "      <td>52.844525</td>\n",
       "      <td>91.833260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27661</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTAFTLPSVN</td>\n",
       "      <td>10</td>\n",
       "      <td>2529.297996</td>\n",
       "      <td>236.591970</td>\n",
       "      <td>1406.047524</td>\n",
       "      <td>14.487719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27662</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTDGSCNKQS</td>\n",
       "      <td>10</td>\n",
       "      <td>11091.748153</td>\n",
       "      <td>4345.102242</td>\n",
       "      <td>11246.049740</td>\n",
       "      <td>588.843655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27663</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTGDFDSVI</td>\n",
       "      <td>9</td>\n",
       "      <td>215.278173</td>\n",
       "      <td>354.813389</td>\n",
       "      <td>42.461956</td>\n",
       "      <td>40.550854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27664</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTPKVVGG</td>\n",
       "      <td>8</td>\n",
       "      <td>70145.529842</td>\n",
       "      <td>17947.336268</td>\n",
       "      <td>13995.873226</td>\n",
       "      <td>3140.508694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27665</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTPKVVGGIGG</td>\n",
       "      <td>11</td>\n",
       "      <td>70145.529842</td>\n",
       "      <td>9418.895965</td>\n",
       "      <td>10046.157903</td>\n",
       "      <td>387.257645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27666</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTSGPGIR</td>\n",
       "      <td>8</td>\n",
       "      <td>16865.530254</td>\n",
       "      <td>619.441075</td>\n",
       "      <td>2349.632821</td>\n",
       "      <td>80.723503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27667</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTSGPGIRYP</td>\n",
       "      <td>10</td>\n",
       "      <td>7128.530301</td>\n",
       "      <td>783.429643</td>\n",
       "      <td>5176.068320</td>\n",
       "      <td>1270.574105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27668</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTSGPGTRYP</td>\n",
       "      <td>10</td>\n",
       "      <td>3097.419299</td>\n",
       "      <td>483.058802</td>\n",
       "      <td>5420.008904</td>\n",
       "      <td>797.994687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27669</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTSGPGTRYPM</td>\n",
       "      <td>11</td>\n",
       "      <td>25.941794</td>\n",
       "      <td>39.174188</td>\n",
       "      <td>7.744618</td>\n",
       "      <td>76.383578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27670</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTTGGTSR</td>\n",
       "      <td>8</td>\n",
       "      <td>70145.529842</td>\n",
       "      <td>357.272838</td>\n",
       "      <td>5701.642723</td>\n",
       "      <td>100.230524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27671</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTTGGTSRN</td>\n",
       "      <td>9</td>\n",
       "      <td>36897.759857</td>\n",
       "      <td>186.637969</td>\n",
       "      <td>6025.595861</td>\n",
       "      <td>119.674053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27672</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTTTGASRN</td>\n",
       "      <td>9</td>\n",
       "      <td>587.489353</td>\n",
       "      <td>135.518941</td>\n",
       "      <td>2844.461107</td>\n",
       "      <td>87.096359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27673</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTYEAYVRYP</td>\n",
       "      <td>10</td>\n",
       "      <td>2624.218543</td>\n",
       "      <td>1552.387010</td>\n",
       "      <td>3872.576449</td>\n",
       "      <td>1954.339456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27674</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTYEAYVRYPE</td>\n",
       "      <td>11</td>\n",
       "      <td>1905.460718</td>\n",
       "      <td>864.967919</td>\n",
       "      <td>6998.419960</td>\n",
       "      <td>32.734069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27675</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YVADALAAF</td>\n",
       "      <td>9</td>\n",
       "      <td>15.381546</td>\n",
       "      <td>453.941617</td>\n",
       "      <td>71.285303</td>\n",
       "      <td>108.143395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27676</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YVFPVIFSK</td>\n",
       "      <td>9</td>\n",
       "      <td>39264.493540</td>\n",
       "      <td>2098.939884</td>\n",
       "      <td>5610.479760</td>\n",
       "      <td>901.571138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27677</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YVFPVIFSR</td>\n",
       "      <td>9</td>\n",
       "      <td>36728.230050</td>\n",
       "      <td>2333.458062</td>\n",
       "      <td>10046.157903</td>\n",
       "      <td>2600.159563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27678</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YVPCHIRQI</td>\n",
       "      <td>9</td>\n",
       "      <td>10764.652136</td>\n",
       "      <td>21134.890398</td>\n",
       "      <td>6039.486294</td>\n",
       "      <td>10568.175092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27679</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YVVQMLARL</td>\n",
       "      <td>9</td>\n",
       "      <td>152.405275</td>\n",
       "      <td>232.273680</td>\n",
       "      <td>739.605275</td>\n",
       "      <td>105.681751</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27680 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         allele      peptide  length          meas        netmhc  \\\n",
       "0        H-2-DB    AAACNVATA       9    657.657837    154.881662   \n",
       "1        H-2-DB     AAFEFVYV       8  30831.879502   6456.542290   \n",
       "2        H-2-DB    AAFVNDYSL       9     77.446180     17.458222   \n",
       "3        H-2-DB    AAIANQAAV       9      1.999862      9.638290   \n",
       "4        H-2-DB    AAIANQAVV       9      1.517050      8.550667   \n",
       "5        H-2-DB    AAIENYVRF       9     37.844258    252.348077   \n",
       "6        H-2-DB    AAINFITTM       9      3.155005    199.986187   \n",
       "7        H-2-DB    AAIPAPPPI       9   3243.396173   1059.253725   \n",
       "8        H-2-DB    AAKLNRPPL       9    654.636174     66.374307   \n",
       "9        H-2-DB    AALDMVDAL       9    229.614865    547.015963   \n",
       "10       H-2-DB    AALQHLRSI       9    905.732601   1686.553025   \n",
       "11       H-2-DB    AALVRLTAL       9   1106.623784    435.511874   \n",
       "12       H-2-DB    AAMVPTGSL       9   1836.538343   4055.085354   \n",
       "13       H-2-DB    AANSPWAPV       9   4325.138310    903.649474   \n",
       "14       H-2-DB    AAPSGAAPL       9      2.844461     97.948999   \n",
       "15       H-2-DB    AAPVSEPTV       9    106.905488   2654.605562   \n",
       "16       H-2-DB    AAVANGFAA       9     83.368118    205.116218   \n",
       "17       H-2-DB    AAVLLGAPV       9    373.250158    320.626932   \n",
       "18       H-2-DB    AAVQLLFPA       9    347.536161   2494.594727   \n",
       "19       H-2-DB    AAVTAGVAL       9    229.614865   1552.387010   \n",
       "20       H-2-DB    AAVVNRSLV       9      8.609938     26.302680   \n",
       "21       H-2-DB    AAYVPADAV       9    331.894458   1757.923614   \n",
       "22       H-2-DB    ACMVNLERL       9    737.904230    239.331576   \n",
       "23       H-2-DB    ADLVNHPPV       9    558.470195   1694.337800   \n",
       "24       H-2-DB    AEMEEALKG       9  78523.563461  41304.750199   \n",
       "25       H-2-DB     AFFAFRYV       8  87902.251683  18967.059212   \n",
       "26       H-2-DB    AGLDNKFYL       9    659.173895    357.272838   \n",
       "27       H-2-DB     AGLLFVLL       8  87902.251683  14996.848355   \n",
       "28       H-2-DB     AGLQVINL       8  59429.215862  25061.092530   \n",
       "29       H-2-DB    AGLVSFNFL       9    210.862815    620.869034   \n",
       "...         ...          ...     ...           ...           ...   \n",
       "27650  Mamu-A02     YNTVCVIW       8    909.913273    734.513868   \n",
       "27651  Mamu-A02    YRHDGGNVL       9  78342.964277   1355.189412   \n",
       "27652  Mamu-A02   YSEGQYMNTP      10   7362.070975   2958.012467   \n",
       "27653  Mamu-A02  YSEVALNVTES      11   7709.034691   1682.674061   \n",
       "27654  Mamu-A02    YSLVFVILM       9   1256.029964     17.498467   \n",
       "27655  Mamu-A02    YSQIGAGVY       9      1.999862     17.864876   \n",
       "27656  Mamu-A02   YSRDLICEQS      10   1059.253725   1297.179271   \n",
       "27657  Mamu-A02    YSYKAFIKY       9      2.317395      4.285485   \n",
       "27658  Mamu-A02   YSYKAFIKYP      10   4830.588020   1199.499303   \n",
       "27659  Mamu-A02  YSYKAFIKYPE      11   4897.788194    671.428853   \n",
       "27660  Mamu-A02    YTAFTLPSV       9    101.624869     32.210688   \n",
       "27661  Mamu-A02   YTAFTLPSVN      10   2529.297996    236.591970   \n",
       "27662  Mamu-A02   YTDGSCNKQS      10  11091.748153   4345.102242   \n",
       "27663  Mamu-A02    YTGDFDSVI       9    215.278173    354.813389   \n",
       "27664  Mamu-A02     YTPKVVGG       8  70145.529842  17947.336268   \n",
       "27665  Mamu-A02  YTPKVVGGIGG      11  70145.529842   9418.895965   \n",
       "27666  Mamu-A02     YTSGPGIR       8  16865.530254    619.441075   \n",
       "27667  Mamu-A02   YTSGPGIRYP      10   7128.530301    783.429643   \n",
       "27668  Mamu-A02   YTSGPGTRYP      10   3097.419299    483.058802   \n",
       "27669  Mamu-A02  YTSGPGTRYPM      11     25.941794     39.174188   \n",
       "27670  Mamu-A02     YTTGGTSR       8  70145.529842    357.272838   \n",
       "27671  Mamu-A02    YTTGGTSRN       9  36897.759857    186.637969   \n",
       "27672  Mamu-A02    YTTTGASRN       9    587.489353    135.518941   \n",
       "27673  Mamu-A02   YTYEAYVRYP      10   2624.218543   1552.387010   \n",
       "27674  Mamu-A02  YTYEAYVRYPE      11   1905.460718    864.967919   \n",
       "27675  Mamu-A02    YVADALAAF       9     15.381546    453.941617   \n",
       "27676  Mamu-A02    YVFPVIFSK       9  39264.493540   2098.939884   \n",
       "27677  Mamu-A02    YVFPVIFSR       9  36728.230050   2333.458062   \n",
       "27678  Mamu-A02    YVPCHIRQI       9  10764.652136  21134.890398   \n",
       "27679  Mamu-A02    YVVQMLARL       9    152.405275    232.273680   \n",
       "\n",
       "          netmhcpan   smmpmbec_cpp  \n",
       "0        711.213514     438.530698  \n",
       "1        785.235635   10351.421667  \n",
       "2          7.516229      28.054336  \n",
       "3          9.749896      25.703958  \n",
       "4          8.336812      28.773984  \n",
       "5        114.815362     187.068214  \n",
       "6        389.045145     200.909281  \n",
       "7        493.173804     295.120923  \n",
       "8         77.268059      38.459178  \n",
       "9        597.035287     225.423921  \n",
       "10      2032.357011     698.232404  \n",
       "11       214.783047     378.442585  \n",
       "12      5176.068320    1545.254440  \n",
       "13      1023.292992     557.185749  \n",
       "14       501.187234     822.242650  \n",
       "15      1918.668741    1870.682140  \n",
       "16       228.559880     200.447203  \n",
       "17       623.734835     286.417797  \n",
       "18      5152.286446     679.203633  \n",
       "19      2172.701179    1927.524913  \n",
       "20        29.580125      63.826349  \n",
       "21      3206.269325     595.662144  \n",
       "22       679.203633     444.631267  \n",
       "23      1857.804455     325.087297  \n",
       "24     46989.410861  440554.863507  \n",
       "25     12882.495517    7816.278046  \n",
       "26       202.768272     260.015956  \n",
       "27      9885.530947   26242.185434  \n",
       "28     12560.299637   35399.734108  \n",
       "29      2285.598803    1811.340093  \n",
       "...             ...            ...  \n",
       "27650  10023.052381      48.865236  \n",
       "27651  22233.098907    1409.288798  \n",
       "27652   8709.635900    4477.133042  \n",
       "27653   5105.050000      51.522864  \n",
       "27654     49.431069       6.324119  \n",
       "27655     15.346170      16.292960  \n",
       "27656   3863.669771     220.292646  \n",
       "27657      3.090295       1.778279  \n",
       "27658   1753.880502    3111.716337  \n",
       "27659   2666.858665      42.559841  \n",
       "27660     52.844525      91.833260  \n",
       "27661   1406.047524      14.487719  \n",
       "27662  11246.049740     588.843655  \n",
       "27663     42.461956      40.550854  \n",
       "27664  13995.873226    3140.508694  \n",
       "27665  10046.157903     387.257645  \n",
       "27666   2349.632821      80.723503  \n",
       "27667   5176.068320    1270.574105  \n",
       "27668   5420.008904     797.994687  \n",
       "27669      7.744618      76.383578  \n",
       "27670   5701.642723     100.230524  \n",
       "27671   6025.595861     119.674053  \n",
       "27672   2844.461107      87.096359  \n",
       "27673   3872.576449    1954.339456  \n",
       "27674   6998.419960      32.734069  \n",
       "27675     71.285303     108.143395  \n",
       "27676   5610.479760     901.571138  \n",
       "27677  10046.157903    2600.159563  \n",
       "27678   6039.486294   10568.175092  \n",
       "27679    739.605275     105.681751  \n",
       "\n",
       "[27680 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df = pandas.read_csv(\"../data/combined_test_BLIND_dataset_from_kim2013.csv\")\n",
    "validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['HLA-A0201',\n",
       " 'HLA-A0301',\n",
       " 'HLA-A0203',\n",
       " 'HLA-A1101',\n",
       " 'HLA-A0206',\n",
       " 'HLA-A3101',\n",
       " 'HLA-A6802',\n",
       " 'HLA-A0202',\n",
       " 'HLA-A0101',\n",
       " 'HLA-B0702',\n",
       " 'H-2-KB',\n",
       " 'H-2-DB',\n",
       " 'HLA-B1501',\n",
       " 'HLA-A6801',\n",
       " 'HLA-A3301',\n",
       " 'HLA-B2705',\n",
       " 'HLA-A2601',\n",
       " 'HLA-B4001',\n",
       " 'HLA-B5801',\n",
       " 'HLA-A2402',\n",
       " 'HLA-A2902',\n",
       " 'HLA-B3501',\n",
       " 'HLA-B0801',\n",
       " 'Mamu-A01',\n",
       " 'HLA-A6901',\n",
       " 'HLA-B1801',\n",
       " 'HLA-A3001',\n",
       " 'HLA-A2301',\n",
       " 'HLA-B5701',\n",
       " 'HLA-B5101',\n",
       " 'HLA-B4402',\n",
       " 'Mamu-B17',\n",
       " 'HLA-A3002',\n",
       " 'HLA-B4601',\n",
       " 'Mamu-A11',\n",
       " 'HLA-A0219',\n",
       " 'HLA-A2403',\n",
       " 'HLA-A0212',\n",
       " 'Mamu-B03',\n",
       " 'Mamu-B08',\n",
       " 'HLA-A0211',\n",
       " 'HLA-B5401',\n",
       " 'HLA-B5301',\n",
       " 'Mamu-A02',\n",
       " 'HLA-B4403',\n",
       " 'HLA-A0216',\n",
       " 'HLA-B4501',\n",
       " 'HLA-B3901',\n",
       " 'HLA-B4002',\n",
       " 'HLA-B4801',\n",
       " 'HLA-B1517',\n",
       " 'Mamu-B01',\n",
       " 'HLA-A8001',\n",
       " 'Patr-B0101',\n",
       " 'HLA-A3201',\n",
       " 'Patr-A0901',\n",
       " 'Patr-A0701',\n",
       " 'HLA-A2501',\n",
       " 'HLA-B0802',\n",
       " 'Mamu-A2201',\n",
       " 'H-2-KD',\n",
       " 'HLA-B2703',\n",
       " 'HLA-B1503',\n",
       " 'Patr-A0101',\n",
       " 'H-2-KK',\n",
       " 'HLA-B1509',\n",
       " 'Patr-B2401',\n",
       " 'Patr-A0301',\n",
       " 'H-2-DD',\n",
       " 'H-2-LD',\n",
       " 'Patr-A0401',\n",
       " 'HLA-B0803',\n",
       " 'HLA-A2603',\n",
       " 'HLA-A2602',\n",
       " 'Patr-B1301',\n",
       " 'HLA-B1502',\n",
       " 'HLA-B3801',\n",
       " 'HLA-A0250',\n",
       " 'HLA-B7301',\n",
       " 'HLA-A11',\n",
       " 'HLA-A0207',\n",
       " 'HLA-B7',\n",
       " 'HLA-A0205',\n",
       " 'Mamu-A07',\n",
       " 'Mamu-A2601',\n",
       " 'HLA-A2',\n",
       " 'HLA-B5802',\n",
       " 'HLA-A0210',\n",
       " 'Gogo-B0101',\n",
       " 'ELA-A1',\n",
       " 'HLA-A0302']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alleles = sorted((\n",
    "    allele for allele in all_train_data\n",
    "    if len(set(all_train_data[allele].original_peptides)) >= min_peptides_to_consider_allele),\n",
    "                 key=lambda allele: -1 * len(set(all_train_data[allele].original_peptides)))\n",
    "alleles\n",
    "\n",
    "print(len(alleles))\n",
    "alleles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 models\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'activation',\n",
       " 'dropout_probability',\n",
       " 'embedding_output_dim',\n",
       " 'impute',\n",
       " 'layer_sizes'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout_probabilities = [0.0, 0.5]\n",
    "embedding_output_dims_and_layer_sizes_list = [(32, [64]), (8, [4])]\n",
    "activations = [\"tanh\"]\n",
    "\n",
    "models_params_list = []\n",
    "\n",
    "for impute in [False, True]:\n",
    "    for dropout_probability in dropout_probabilities:\n",
    "        for (embedding_output_dim, layer_sizes) in embedding_output_dims_and_layer_sizes_list:\n",
    "            for activation in activations:\n",
    "                models_params_list.append(dict(\n",
    "                    impute=impute,\n",
    "                    dropout_probability=dropout_probability,  \n",
    "                    embedding_output_dim=embedding_output_dim,\n",
    "                    layer_sizes=layer_sizes,\n",
    "                    activation=activation))\n",
    "\n",
    "print(\"%d models\" % len(models_params_list))\n",
    "models_params_explored = set.union(*[set(x) for x in models_params_list])\n",
    "models_params_explored\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_to_ic50(log_value):\n",
    "        \"\"\"\n",
    "        Convert neural network output to IC50 values between 0.0 and\n",
    "        self.max_ic50 (typically 5000, 20000 or 50000)\n",
    "        \"\"\"\n",
    "        return max_ic50 ** (1.0 - log_value)\n",
    "\n",
    "def make_scores(ic50_y, ic50_y_pred, sample_weight=None, threshold_nm=500):     \n",
    "    y_pred = mhcflurry.common.ic50_to_regression_target(ic50_y_pred, max_ic50)\n",
    "    try:\n",
    "        auc = sklearn.metrics.roc_auc_score(ic50_y <= threshold_nm, y_pred, sample_weight=sample_weight)\n",
    "    except ValueError:\n",
    "        auc = numpy.nan\n",
    "    try:\n",
    "        f1 = sklearn.metrics.f1_score(ic50_y <= threshold_nm, ic50_y_pred <= threshold_nm, sample_weight=sample_weight)\n",
    "    except ValueError:\n",
    "        f1 = numpy.nan\n",
    "    try:\n",
    "        tau = scipy.stats.kendalltau(ic50_y_pred, ic50_y)[0]\n",
    "    except ValueError:\n",
    "        tau = numpy.nan\n",
    "    \n",
    "    return dict(\n",
    "        auc=auc,\n",
    "        f1=f1,\n",
    "        tau=tau,\n",
    "    )    \n",
    "\n",
    "def mean_with_std(grouped_column, decimals=3):\n",
    "    pattern = \"%%0.%df\" % decimals\n",
    "    return pandas.Series([\n",
    "        (pattern + \" +/ \" + pattern) % (m, s) if not pandas.isnull(s) else pattern % m\n",
    "        for (m, s) in zip(grouped_column.mean(), grouped_column.std())\n",
    "    ], index = grouped_column.mean().index)\n",
    "\n",
    "def allele_data_to_df(data):\n",
    "    d = data._asdict()\n",
    "    d[\"X_index\"] = [x for x in d[\"X_index\"]]\n",
    "    d[\"X_binary\"] = [x for x in d[\"X_binary\"]]\n",
    "    df = pandas.DataFrame(d).set_index('peptides')\n",
    "    return df\n",
    "\n",
    "def make_2d_array(thing):\n",
    "    return numpy.array([list(x) for x in thing])\n",
    "\n",
    "def df_to_allele_data(df):\n",
    "    d = dict((col, df[col].values) for col in df)\n",
    "    d[\"X_index\"] = make_2d_array(d[\"X_index\"])\n",
    "    (d[\"max_ic50\"],) = list(df.max_ic50.unique())\n",
    "    return mhcflurry.data.AlleleData(peptides = df.index.values, **d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=all_train_data[\"Mamu-A02\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6027"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_and_scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allele 0 model 0\n",
      "Fitting model for allele HLA-A0203 (19879 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 23 sec\n",
      "test set size: 651\n",
      "{'tau': 0.53431495569795218, 'auc': 0.95089201877934271, 'f1': 0.91656590084643286}\n",
      "Allele 0 model 1\n",
      "Fitting model for allele HLA-A0203 (19879 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 21 sec\n",
      "test set size: 651\n",
      "{'tau': 0.57347845157839439, 'auc': 0.97645279081898795, 'f1': 0.95136417556346398}\n",
      "Allele 0 model 2\n",
      "Fitting model for allele HLA-A0203 (19879 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 27 sec\n",
      "test set size: 651\n",
      "{'tau': 0.5928116031092181, 'auc': 0.97432446531038086, 'f1': 0.9537366548042705}\n",
      "Allele 0 model 3\n",
      "Fitting model for allele HLA-A0203 (19879 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 24 sec\n",
      "test set size: 651\n",
      "{'tau': 0.58090765965923019, 'auc': 0.97039123630672919, 'f1': 0.95124851367419738}\n",
      "Allele 0 model 4\n",
      "Fitting model for allele HLA-A0203 (19879 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 25 sec\n",
      "test set size: 651\n",
      "{'tau': 0.51010548998279592, 'auc': 0.9488262910798122, 'f1': 0.92361927144535838}\n",
      "Allele 0 model 5\n",
      "Fitting model for allele HLA-A0203 (19879 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 23 sec\n",
      "test set size: 651\n",
      "{'tau': 0.56630740130731727, 'auc': 0.97348982785602489, 'f1': 0.95113230035756846}\n",
      "Allele 0 model 6\n",
      "Fitting model for allele HLA-A0203 (19879 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 31 sec\n",
      "test set size: 651\n",
      "{'tau': 0.5958521284241548, 'auc': 0.9748148148148148, 'f1': 0.95704057279236276}\n",
      "Allele 0 model 7\n",
      "Fitting model for allele HLA-A0203 (19879 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 27 sec\n",
      "test set size: 651\n",
      "{'tau': 0.58106064206501318, 'auc': 0.97061032863849761, 'f1': 0.95818399044205504}\n",
      "Allele 1 model 0\n",
      "Fitting model for allele HLA-A1101 (19198 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 22 sec\n",
      "test set size: 723\n",
      "{'tau': 0.57990623099628835, 'auc': 0.92036818105049822, 'f1': 0.87207702888583227}\n",
      "Allele 1 model 1\n",
      "Fitting model for allele HLA-A1101 (19198 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 20 sec\n",
      "test set size: 723\n",
      "{'tau': 0.61535057304702234, 'auc': 0.93652024381630872, 'f1': 0.88315217391304357}\n",
      "Allele 1 model 2\n",
      "Fitting model for allele HLA-A1101 (19198 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 26 sec\n",
      "test set size: 723\n",
      "{'tau': 0.61201262581053184, 'auc': 0.93692711611982016, 'f1': 0.88429752066115697}\n",
      "Allele 1 model 3\n",
      "Fitting model for allele HLA-A1101 (19198 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 23 sec\n",
      "test set size: 723\n",
      "{'tau': 0.58619399206967748, 'auc': 0.92545024642643281, 'f1': 0.87637362637362637}\n",
      "Allele 1 model 4\n",
      "Fitting model for allele HLA-A1101 (19198 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 25 sec\n",
      "test set size: 723\n",
      "{'tau': 0.56748596220934699, 'auc': 0.92677833903978146, 'f1': 0.86350974930362112}\n",
      "Allele 1 model 5\n",
      "Fitting model for allele HLA-A1101 (19198 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 22 sec\n",
      "test set size: 723\n",
      "{'tau': 0.61907665368310472, 'auc': 0.93686570143249759, 'f1': 0.88466757123473549}\n",
      "Allele 1 model 6\n",
      "Fitting model for allele HLA-A1101 (19198 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 29 sec\n",
      "test set size: 723\n",
      "{'tau': 0.61191947379462974, 'auc': 0.93728025057192421, 'f1': 0.88429752066115697}\n",
      "Allele 1 model 7\n",
      "Fitting model for allele HLA-A1101 (19198 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 26 sec\n",
      "test set size: 723\n",
      "{'tau': 0.58638805876947342, 'auc': 0.92715450399963151, 'f1': 0.87465181058495822}\n",
      "Allele 2 model 0\n",
      "Fitting model for allele HLA-A0206 (19155 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 22 sec\n",
      "test set size: 682\n",
      "{'tau': 0.46571314955990162, 'auc': 0.8760674065537174, 'f1': 0.84944920440636462}\n",
      "Allele 2 model 1\n",
      "Fitting model for allele HLA-A0206 (19155 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 20 sec\n",
      "test set size: 682\n",
      "{'tau': 0.52887564135823906, 'auc': 0.90908100946486736, 'f1': 0.87223587223587229}\n",
      "Allele 2 model 2\n",
      "Fitting model for allele HLA-A0206 (19155 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 26 sec\n",
      "test set size: 682\n",
      "{'tau': 0.51742063155358575, 'auc': 0.90002450157445291, 'f1': 0.86871165644171766}\n",
      "Allele 2 model 3\n",
      "Fitting model for allele HLA-A0206 (19155 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 23 sec\n",
      "test set size: 682\n",
      "{'tau': 0.50000901665051278, 'auc': 0.8834995508044684, 'f1': 0.86945812807881762}\n",
      "Allele 2 model 4\n",
      "Fitting model for allele HLA-A0206 (19155 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 25 sec\n",
      "test set size: 682\n",
      "{'tau': 0.47354578267915892, 'auc': 0.87119431563472705, 'f1': 0.82397003745318342}\n",
      "Allele 2 model 5\n",
      "Fitting model for allele HLA-A0206 (19155 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 22 sec\n",
      "test set size: 682\n",
      "{'tau': 0.51163693226353812, 'auc': 0.90263800284944229, 'f1': 0.86810551558752991}\n",
      "Allele 2 model 6\n",
      "Fitting model for allele HLA-A0206 (19155 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 29 sec\n",
      "test set size: 682\n",
      "{'tau': 0.51949550125405131, 'auc': 0.89898999065310314, 'f1': 0.88077858880778581}\n",
      "Allele 2 model 7\n",
      "Fitting model for allele HLA-A0206 (19155 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 26 sec\n",
      "test set size: 682\n",
      "{'tau': 0.4927988444413951, 'auc': 0.88308211657304647, 'f1': 0.86386138613861385}\n",
      "Allele 3 model 0\n",
      "Fitting model for allele HLA-A3101 (18153 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 22 sec\n",
      "test set size: 724\n",
      "{'tau': 0.46055427575883406, 'auc': 0.82850967286997579, 'f1': 0.78260869565217406}\n",
      "Allele 3 model 1\n",
      "Fitting model for allele HLA-A3101 (18153 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 19 sec\n",
      "test set size: 724\n",
      "{'tau': 0.5102871916028211, 'auc': 0.85483796793083366, 'f1': 0.82250000000000001}\n",
      "Allele 3 model 2\n",
      "Fitting model for allele HLA-A3101 (18153 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 24 sec\n",
      "test set size: 724\n",
      "{'tau': 0.503989541179374, 'auc': 0.84723457092927001, 'f1': 0.8176100628930818}\n",
      "Allele 3 model 3\n",
      "Fitting model for allele HLA-A3101 (18153 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 22 sec\n",
      "test set size: 724\n",
      "{'tau': 0.48701829941324615, 'auc': 0.84062758684121786, 'f1': 0.78701298701298705}\n",
      "Allele 3 model 4\n",
      "Fitting model for allele HLA-A3101 (18153 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 26 sec\n",
      "test set size: 724\n",
      "{'tau': 0.487072323375212, 'auc': 0.84656774074868935, 'f1': 0.81704260651629068}\n",
      "Allele 3 model 5\n",
      "Fitting model for allele HLA-A3101 (18153 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 21 sec\n",
      "test set size: 724\n",
      "{'tau': 0.51410745748469167, 'auc': 0.85438574976239379, 'f1': 0.82352941176470584}\n",
      "Allele 3 model 6\n",
      "Fitting model for allele HLA-A3101 (18153 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 28 sec\n",
      "test set size: 724\n",
      "{'tau': 0.50511632667180439, 'auc': 0.84739552993837564, 'f1': 0.81658291457286436}\n",
      "Allele 3 model 7\n",
      "Fitting model for allele HLA-A3101 (18153 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 24 sec\n",
      "test set size: 724\n",
      "{'tau': 0.48755082132405236, 'auc': 0.84016003924333937, 'f1': 0.80204342273307794}\n",
      "Allele 4 model 0\n",
      "Fitting model for allele HLA-A6802 (18881 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 21 sec\n",
      "test set size: 669\n",
      "{'tau': 0.60285638757956805, 'auc': 0.9545617654974432, 'f1': 0.87048192771084332}\n",
      "Allele 4 model 1\n",
      "Fitting model for allele HLA-A6802 (18881 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 20 sec\n",
      "test set size: 669\n",
      "{'tau': 0.64020119346378401, 'auc': 0.96835919978469531, 'f1': 0.90000000000000013}\n",
      "Allele 4 model 2\n",
      "Fitting model for allele HLA-A6802 (18881 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 25 sec\n",
      "test set size: 669\n",
      "{'tau': 0.63265989367990116, 'auc': 0.96841302592625822, 'f1': 0.88989441930618407}\n",
      "Allele 4 model 3\n",
      "Fitting model for allele HLA-A6802 (18881 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 23 sec\n",
      "test set size: 669\n",
      "{'tau': 0.59737673809631509, 'auc': 0.95917287162465237, 'f1': 0.76791808873720147}\n",
      "Allele 4 model 4\n",
      "Fitting model for allele HLA-A6802 (18881 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 25 sec\n",
      "test set size: 669\n",
      "{'tau': 0.58739401032484428, 'auc': 0.94603032205974691, 'f1': 0.86274509803921573}\n",
      "Allele 4 model 5\n",
      "Fitting model for allele HLA-A6802 (18881 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 22 sec\n",
      "test set size: 669\n",
      "{'tau': 0.65320405807915982, 'auc': 0.96973176639454561, 'f1': 0.89022556390977448}\n",
      "Allele 4 model 6\n",
      "Fitting model for allele HLA-A6802 (18881 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 28 sec\n",
      "test set size: 669\n",
      "{'tau': 0.63470345932877104, 'auc': 0.96863730151610317, 'f1': 0.88584474885844755}\n",
      "Allele 4 model 7\n",
      "Fitting model for allele HLA-A6802 (18881 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 25 sec\n",
      "test set size: 669\n",
      "{'tau': 0.603082445726567, 'auc': 0.95982775634699924, 'f1': 0.73905429071803863}\n",
      "Allele 5 model 0\n",
      "Fitting model for allele HLA-A0202 (18256 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 21 sec\n",
      "test set size: 126\n",
      "{'tau': 0.56148404980721456, 'auc': 0.83398004434589801, 'f1': 0.72000000000000008}\n",
      "Allele 5 model 1\n",
      "Fitting model for allele HLA-A0202 (18256 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 19 sec\n",
      "test set size: 126\n",
      "{'tau': 0.61470286154560627, 'auc': 0.89329268292682917, 'f1': 0.7640449438202247}\n",
      "Allele 5 model 2\n",
      "Fitting model for allele HLA-A0202 (18256 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 25 sec\n",
      "test set size: 126\n",
      "{'tau': 0.64131226741480218, 'auc': 0.90327050997782699, 'f1': 0.79120879120879128}\n",
      "Allele 5 model 3\n",
      "Fitting model for allele HLA-A0202 (18256 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 22 sec\n",
      "test set size: 126\n",
      "{'tau': 0.66843339262763635, 'auc': 0.9132483370288248, 'f1': 0.77894736842105272}\n",
      "Allele 5 model 4\n",
      "Fitting model for allele HLA-A0202 (18256 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 23 sec\n",
      "test set size: 126\n",
      "{'tau': 0.47653863876324315, 'auc': 0.81845898004434592, 'f1': 0.68627450980392146}\n",
      "Allele 5 model 5\n",
      "Fitting model for allele HLA-A0202 (18256 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 23 sec\n",
      "test set size: 126\n",
      "{'tau': 0.62979858218293849, 'auc': 0.90770509977827052, 'f1': 0.74725274725274726}\n",
      "Allele 5 model 6\n",
      "Fitting model for allele HLA-A0202 (18256 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 28 sec\n",
      "test set size: 126\n",
      "{'tau': 0.64489430282027083, 'auc': 0.90354767184035478, 'f1': 0.78260869565217384}\n",
      "Allele 5 model 7\n",
      "Fitting model for allele HLA-A0202 (18256 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 25 sec\n",
      "test set size: 126\n",
      "{'tau': 0.66664237492490208, 'auc': 0.91712860310421296, 'f1': 0.79545454545454553}\n",
      "Allele 6 model 0\n",
      "Fitting model for allele HLA-A0101 (8694 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 10 sec\n",
      "test set size: 696\n",
      "{'tau': 0.52171951827985108, 'auc': 0.91274554591137502, 'f1': 0.62176165803108807}\n",
      "Allele 6 model 1\n",
      "Fitting model for allele HLA-A0101 (8694 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 9 sec\n",
      "test set size: 696\n",
      "{'tau': 0.53159603702357561, 'auc': 0.92491074903135195, 'f1': 0.65979381443298979}\n",
      "Allele 6 model 2\n",
      "Fitting model for allele HLA-A0101 (8694 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 12 sec\n",
      "test set size: 696\n",
      "{'tau': 0.52174479947117458, 'auc': 0.90926010524000467, 'f1': 0.58620689655172409}\n",
      "Allele 6 model 3\n",
      "Fitting model for allele HLA-A0101 (8694 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 11 sec\n",
      "test set size: 696\n",
      "{'tau': 0.49634562925480452, 'auc': 0.88746764123648547, 'f1': 0.31578947368421051}\n",
      "Allele 6 model 4\n",
      "Fitting model for allele HLA-A0101 (8694 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 13 sec\n",
      "test set size: 696\n",
      "{'tau': 0.51734587218088091, 'auc': 0.91480973893034201, 'f1': 0.61375661375661372}\n",
      "Allele 6 model 5\n",
      "Fitting model for allele HLA-A0101 (8694 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 11 sec\n",
      "test set size: 696\n",
      "{'tau': 0.52730666156235051, 'auc': 0.92360793868331559, 'f1': 0.6424870466321243}\n",
      "Allele 6 model 6\n",
      "Fitting model for allele HLA-A0101 (8694 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 15 sec\n",
      "test set size: 696\n",
      "{'tau': 0.51911555557352773, 'auc': 0.90866791871816999, 'f1': 0.56000000000000005}\n",
      "Allele 6 model 7\n",
      "Fitting model for allele HLA-A0101 (8694 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 13 sec\n",
      "test set size: 696\n",
      "{'tau': 0.50297772844534305, 'auc': 0.89347410452938092, 'f1': 0.34920634920634919}\n",
      "Allele 7 model 0\n",
      "Fitting model for allele HLA-B0702 (7299 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 9 sec\n",
      "test set size: 813\n",
      "{'tau': 0.56450487131791005, 'auc': 0.90498483532802254, 'f1': 0.79271070615034167}\n",
      "Allele 7 model 1\n",
      "Fitting model for allele HLA-B0702 (7299 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 8 sec\n",
      "test set size: 813\n",
      "{'tau': 0.59449736687834731, 'auc': 0.91580146235260063, 'f1': 0.86652542372881358}\n",
      "Allele 7 model 2\n",
      "Fitting model for allele HLA-B0702 (7299 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 10 sec\n",
      "test set size: 813\n",
      "{'tau': 0.58724710228535582, 'auc': 0.90930771070084693, 'f1': 0.83552631578947367}\n",
      "Allele 7 model 3\n",
      "Fitting model for allele HLA-B0702 (7299 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 9 sec\n",
      "test set size: 813\n",
      "{'tau': 0.57684613546151731, 'auc': 0.89804432363046027, 'f1': 0.72277227722772286}\n",
      "Allele 7 model 4\n",
      "Fitting model for allele HLA-B0702 (7299 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 11 sec\n",
      "test set size: 813\n",
      "{'tau': 0.56713856642593474, 'auc': 0.89869873271164469, 'f1': 0.83206933911159264}\n",
      "Allele 7 model 5\n",
      "Fitting model for allele HLA-B0702 (7299 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 10 sec\n",
      "test set size: 813\n",
      "{'tau': 0.59065934929598352, 'auc': 0.9117932067303457, 'f1': 0.86480686695278963}\n",
      "Allele 7 model 6\n",
      "Fitting model for allele HLA-B0702 (7299 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 13 sec\n",
      "test set size: 813\n",
      "{'tau': 0.5873991632038329, 'auc': 0.90969154679654163, 'f1': 0.8351648351648352}\n",
      "Allele 7 model 7\n",
      "Fitting model for allele HLA-B0702 (7299 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 12 sec\n",
      "test set size: 813\n",
      "{'tau': 0.5796562212349754, 'auc': 0.89905110683228262, 'f1': 0.72366790582403961}\n",
      "Allele 8 model 0\n",
      "Fitting model for allele H-2-KB (20345 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 23 sec\n",
      "test set size: 558\n",
      "{'tau': 0.52709641276745123, 'auc': 0.86664935064935067, 'f1': 0.79461279461279466}\n",
      "Allele 8 model 1\n",
      "Fitting model for allele H-2-KB (20345 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 21 sec\n",
      "test set size: 558\n",
      "{'tau': 0.57031655426414829, 'auc': 0.90037662337662328, 'f1': 0.79785330948121647}\n",
      "Allele 8 model 2\n",
      "Fitting model for allele H-2-KB (20345 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 29 sec\n",
      "test set size: 558\n",
      "{'tau': 0.56522955906414962, 'auc': 0.91003896103896098, 'f1': 0.81216457960644006}\n",
      "Allele 8 model 3\n",
      "Fitting model for allele H-2-KB (20345 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 24 sec\n",
      "test set size: 558\n",
      "{'tau': 0.55862290453858177, 'auc': 0.90810389610389608, 'f1': 0.75992438563327036}\n",
      "Allele 8 model 4\n",
      "Fitting model for allele H-2-KB (20345 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 26 sec\n",
      "test set size: 558\n",
      "{'tau': 0.55132081269453315, 'auc': 0.87976623376623375, 'f1': 0.80749574105621802}\n",
      "Allele 8 model 5\n",
      "Fitting model for allele H-2-KB (20345 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 24 sec\n",
      "test set size: 558\n",
      "{'tau': 0.56694239542263014, 'auc': 0.89801298701298693, 'f1': 0.8085106382978724}\n",
      "Allele 8 model 6\n",
      "Fitting model for allele H-2-KB (20345 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 30 sec\n",
      "test set size: 558\n",
      "{'tau': 0.57065139445452795, 'auc': 0.91197402597402599, 'f1': 0.81216457960644006}\n",
      "Allele 8 model 7\n",
      "Fitting model for allele H-2-KB (20345 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 27 sec\n",
      "test set size: 558\n",
      "{'tau': 0.56334930261047922, 'auc': 0.90883116883116877, 'f1': 0.76082862523540484}\n",
      "Allele 9 model 0\n",
      "Fitting model for allele H-2-DB (17953 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 21 sec\n",
      "test set size: 564\n",
      "{'tau': 0.5983700980487523, 'auc': 0.88762432414391557, 'f1': 0.61132075471698111}\n",
      "Allele 9 model 1\n",
      "Fitting model for allele H-2-DB (17953 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 19 sec\n",
      "test set size: 564\n",
      "{'tau': 0.63110952966822775, 'auc': 0.90729924571123421, 'f1': 0.66666666666666663}\n",
      "Allele 9 model 2\n",
      "Fitting model for allele H-2-DB (17953 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 24 sec\n",
      "test set size: 564\n",
      "{'tau': 0.62178815123747855, 'auc': 0.89443294840130838, 'f1': 0.56410256410256421}\n",
      "Allele 9 model 3\n",
      "Fitting model for allele H-2-DB (17953 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 22 sec\n",
      "test set size: 564\n",
      "{'tau': 0.617127462022104, 'auc': 0.88939323142647353, 'f1': 0.38095238095238099}\n",
      "Allele 9 model 4\n",
      "Fitting model for allele H-2-DB (17953 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 23 sec\n",
      "test set size: 564\n",
      "{'tau': 0.59929968906711928, 'auc': 0.89276416794606517, 'f1': 0.60629921259842534}\n",
      "Allele 9 model 5\n",
      "Fitting model for allele H-2-DB (17953 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 21 sec\n",
      "test set size: 564\n",
      "{'tau': 0.61682184305716137, 'auc': 0.89451638742407047, 'f1': 0.5959183673469387}\n",
      "Allele 9 model 6\n",
      "Fitting model for allele H-2-DB (17953 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 27 sec\n",
      "test set size: 564\n",
      "{'tau': 0.61827353314063871, 'auc': 0.89226353380949219, 'f1': 0.56387665198237891}\n",
      "Allele 9 model 7\n",
      "Fitting model for allele H-2-DB (17953 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 24 sec\n",
      "test set size: 564\n",
      "{'tau': 0.61482258566149528, 'auc': 0.89251385087777846, 'f1': 0.42857142857142855}\n",
      "Allele 10 model 0\n",
      "Fitting model for allele HLA-B1501 (3843 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 7 sec\n",
      "test set size: 633\n",
      "{'tau': 0.56938442540766698, 'auc': 0.91498831645710466, 'f1': 0.7935034802784221}\n",
      "Allele 10 model 1\n",
      "Fitting model for allele HLA-B1501 (3843 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 4 sec\n",
      "test set size: 633\n",
      "{'tau': 0.58504553578748686, 'auc': 0.93396016468231891, 'f1': 0.83105022831050235}\n",
      "Allele 10 model 2\n",
      "Fitting model for allele HLA-B1501 (3843 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 6 sec\n",
      "test set size: 633\n",
      "{'tau': 0.59009786824907395, 'auc': 0.93724268387671072, 'f1': 0.82973621103117512}\n",
      "Allele 10 model 3\n",
      "Fitting model for allele HLA-B1501 (3843 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 5 sec\n",
      "test set size: 633\n",
      "{'tau': 0.57890655975877348, 'auc': 0.92918660287081334, 'f1': 0.71508379888268159}\n",
      "Allele 10 model 4\n",
      "Fitting model for allele HLA-B1501 (3843 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 7 sec\n",
      "test set size: 633\n",
      "{'tau': 0.56245007109786116, 'auc': 0.91475464559919872, 'f1': 0.79529411764705882}\n",
      "Allele 10 model 5\n",
      "Fitting model for allele HLA-B1501 (3843 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 7 sec\n",
      "test set size: 633\n",
      "{'tau': 0.58813742882384623, 'auc': 0.93626349171024814, 'f1': 0.8413793103448276}\n",
      "Allele 10 model 6\n",
      "Fitting model for allele HLA-B1501 (3843 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 9 sec\n",
      "test set size: 633\n",
      "{'tau': 0.59085963899716254, 'auc': 0.938188494492044, 'f1': 0.83091787439613529}\n",
      "Allele 10 model 7\n",
      "Fitting model for allele HLA-B1501 (3843 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 8 sec\n",
      "test set size: 633\n",
      "{'tau': 0.57713656302056782, 'auc': 0.92914209413597415, 'f1': 0.71978021978021978}\n",
      "Allele 11 model 0\n",
      "Fitting model for allele HLA-A6801 (16514 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 19 sec\n",
      "test set size: 527\n",
      "{'tau': 0.51960846897677082, 'auc': 0.92669263706899319, 'f1': 0.90449438202247201}\n",
      "Allele 11 model 1\n",
      "Fitting model for allele HLA-A6801 (16514 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 17 sec\n",
      "test set size: 527\n",
      "{'tau': 0.571760167668941, 'auc': 0.94522069476192005, 'f1': 0.91960507757404797}\n",
      "Allele 11 model 2\n",
      "Fitting model for allele HLA-A6801 (16514 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 22 sec\n",
      "test set size: 527\n",
      "{'tau': 0.55670890114868887, 'auc': 0.94174366569009371, 'f1': 0.9050279329608939}\n",
      "Allele 11 model 3\n",
      "Fitting model for allele HLA-A6801 (16514 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 20 sec\n",
      "test set size: 527\n",
      "{'tau': 0.53051015481372565, 'auc': 0.93036283442258794, 'f1': 0.89625360230547557}\n",
      "Allele 11 model 4\n",
      "Fitting model for allele HLA-A6801 (16514 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 22 sec\n",
      "test set size: 527\n",
      "{'tau': 0.50818627824574381, 'auc': 0.91429767232220471, 'f1': 0.89617486338797814}\n",
      "Allele 11 model 5\n",
      "Fitting model for allele HLA-A6801 (16514 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 20 sec\n",
      "test set size: 527\n",
      "{'tau': 0.56194286702796958, 'auc': 0.943723640578217, 'f1': 0.91666666666666663}\n",
      "Allele 11 model 6\n",
      "Fitting model for allele HLA-A6801 (16514 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 25 sec\n",
      "test set size: 527\n",
      "{'tau': 0.55306536689018404, 'auc': 0.94137342648337141, 'f1': 0.90529247910863508}\n",
      "Allele 11 model 7\n",
      "Fitting model for allele HLA-A6801 (16514 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 22 sec\n",
      "test set size: 527\n",
      "{'tau': 0.53601883160932229, 'auc': 0.93079746305656619, 'f1': 0.90196078431372562}\n",
      "Allele 12 model 0\n",
      "Fitting model for allele HLA-A3301 (15830 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 18 sec\n",
      "test set size: 473\n",
      "{'tau': 0.57939392800461176, 'auc': 0.91242492492492488, 'f1': 0.82285714285714284}\n",
      "Allele 12 model 1\n",
      "Fitting model for allele HLA-A3301 (15830 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 17 sec\n",
      "test set size: 473\n",
      "{'tau': 0.60059883390467028, 'auc': 0.92404279279279278, 'f1': 0.86330935251798568}\n",
      "Allele 12 model 2\n",
      "Fitting model for allele HLA-A3301 (15830 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 24 sec\n",
      "test set size: 473\n",
      "{'tau': 0.59327873492135919, 'auc': 0.92263513513513518, 'f1': 0.85027726432532336}\n",
      "Allele 12 model 3\n",
      "Fitting model for allele HLA-A3301 (15830 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 19 sec\n",
      "test set size: 473\n",
      "{'tau': 0.57579682776711238, 'auc': 0.91769894894894899, 'f1': 0.55802469135802468}\n",
      "Allele 12 model 4\n",
      "Fitting model for allele HLA-A3301 (15830 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 21 sec\n",
      "test set size: 473\n",
      "{'tau': 0.57509539322080006, 'auc': 0.90630630630630638, 'f1': 0.83955223880597007}\n",
      "Allele 12 model 5\n",
      "Fitting model for allele HLA-A3301 (15830 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 19 sec\n",
      "test set size: 473\n",
      "{'tau': 0.58802696857461012, 'auc': 0.91099849849849857, 'f1': 0.84036697247706427}\n",
      "Allele 12 model 6\n",
      "Fitting model for allele HLA-A3301 (15830 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 24 sec\n",
      "test set size: 473\n",
      "{'tau': 0.59315283641304661, 'auc': 0.92301051051051053, 'f1': 0.83798882681564246}\n",
      "Allele 12 model 7\n",
      "Fitting model for allele HLA-A3301 (15830 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 21 sec\n",
      "test set size: 473\n",
      "{'tau': 0.57430403116855011, 'auc': 0.91715465465465462, 'f1': 0.51150895140664954}\n",
      "Allele 13 model 0\n",
      "Fitting model for allele HLA-B2705 (9131 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 11 sec\n",
      "test set size: 314\n",
      "{'tau': 0.41636388638881261, 'auc': 0.94567453115547495, 'f1': 0.5714285714285714}\n",
      "Allele 13 model 1\n",
      "Fitting model for allele HLA-B2705 (9131 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 10 sec\n",
      "test set size: 314\n",
      "{'tau': 0.4240047655833582, 'auc': 0.94458560193587415, 'f1': 0.50980392156862742}\n",
      "Allele 13 model 2\n",
      "Fitting model for allele HLA-B2705 (9131 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 13 sec\n",
      "test set size: 314\n",
      "{'tau': 0.43821936207376977, 'auc': 0.94857834240774341, 'f1': 0.48979591836734693}\n",
      "Allele 13 model 3\n",
      "Fitting model for allele HLA-B2705 (9131 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 11 sec\n",
      "test set size: 314\n",
      "{'tau': 0.42511461395798494, 'auc': 0.93744706594071381, 'f1': 0.25641025641025644}\n",
      "Allele 13 model 4\n",
      "Fitting model for allele HLA-B2705 (9131 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 13 sec\n",
      "test set size: 314\n",
      "{'tau': 0.39660004802526738, 'auc': 0.94204476709013907, 'f1': 0.5490196078431373}\n",
      "Allele 13 model 5\n",
      "Fitting model for allele HLA-B2705 (9131 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 12 sec\n",
      "test set size: 314\n",
      "{'tau': 0.4393718969243437, 'auc': 0.9505142165759225, 'f1': 0.38297872340425537}\n",
      "Allele 13 model 6\n",
      "Fitting model for allele HLA-B2705 (9131 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 16 sec\n",
      "test set size: 314\n",
      "{'tau': 0.43839010797755851, 'auc': 0.94797338173018753, 'f1': 0.52000000000000002}\n",
      "Allele 13 model 7\n",
      "Fitting model for allele HLA-B2705 (9131 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 14 sec\n",
      "test set size: 314\n",
      "{'tau': 0.42665132709208348, 'auc': 0.93926194797338169, 'f1': 0.26315789473684215}\n",
      "Allele 14 model 0\n",
      "Fitting model for allele HLA-A2601 (7353 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 9 sec\n",
      "test set size: 1333\n",
      "{'tau': 0.43229919904475306, 'auc': 0.92808363706119323, 'f1': 0.58000000000000007}\n",
      "Allele 14 model 1\n",
      "Fitting model for allele HLA-A2601 (7353 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 8 sec\n",
      "test set size: 1333\n",
      "{'tau': 0.44072227032713596, 'auc': 0.92926018287614298, 'f1': 0.59310344827586203}\n",
      "Allele 14 model 2\n",
      "Fitting model for allele HLA-A2601 (7353 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 10 sec\n",
      "test set size: 1333\n",
      "{'tau': 0.43343720973928773, 'auc': 0.93176034273291142, 'f1': 0.57489878542510109}\n",
      "Allele 14 model 3\n",
      "Fitting model for allele HLA-A2601 (7353 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 9 sec\n",
      "test set size: 1333\n",
      "{'tau': 0.42608643724781814, 'auc': 0.923543704840463, 'f1': 0.375}\n",
      "Allele 14 model 4\n",
      "Fitting model for allele HLA-A2601 (7353 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 11 sec\n",
      "test set size: 1333\n",
      "{'tau': 0.43455132519614192, 'auc': 0.92813479122706055, 'f1': 0.58412698412698416}\n",
      "Allele 14 model 5\n",
      "Fitting model for allele HLA-A2601 (7353 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 10 sec\n",
      "test set size: 1333\n",
      "{'tau': 0.44666322379545498, 'auc': 0.93487435258008822, 'f1': 0.58208955223880599}\n",
      "Allele 14 model 6\n",
      "Fitting model for allele HLA-A2601 (7353 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 16 sec\n",
      "test set size: 1333\n",
      "{'tau': 0.43282190736901438, 'auc': 0.93126798388643772, 'f1': 0.55371900826446285}\n",
      "Allele 14 model 7\n",
      "Fitting model for allele HLA-A2601 (7353 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 12 sec\n",
      "test set size: 1333\n",
      "{'tau': 0.4234835883006573, 'auc': 0.92224566788157814, 'f1': 0.33333333333333337}\n",
      "Allele 15 model 0\n",
      "Fitting model for allele HLA-B4001 (6210 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 7 sec\n",
      "test set size: 407\n",
      "{'tau': 0.58058585477292513, 'auc': 0.89538929969551984, 'f1': 0.7350427350427351}\n",
      "Allele 15 model 1\n",
      "Fitting model for allele HLA-B4001 (6210 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 7 sec\n",
      "test set size: 407\n",
      "{'tau': 0.60243441635143791, 'auc': 0.90665023440143055, 'f1': 0.72303206997084568}\n",
      "Allele 15 model 2\n",
      "Fitting model for allele HLA-B4001 (6210 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 9 sec\n",
      "test set size: 407\n",
      "{'tau': 0.6122479602112838, 'auc': 0.91276400367309463, 'f1': 0.68098159509202449}\n",
      "Allele 15 model 3\n",
      "Fitting model for allele HLA-B4001 (6210 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 8 sec\n",
      "test set size: 407\n",
      "{'tau': 0.62059679603234674, 'auc': 0.92250253733507326, 'f1': 0.42968749999999994}\n",
      "Allele 15 model 4\n",
      "Fitting model for allele HLA-B4001 (6210 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 10 sec\n",
      "test set size: 407\n",
      "{'tau': 0.58566350930737276, 'auc': 0.89495432796868191, 'f1': 0.70414201183431946}\n",
      "Allele 15 model 5\n",
      "Fitting model for allele HLA-B4001 (6210 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 9 sec\n",
      "test set size: 407\n",
      "{'tau': 0.5840523304647115, 'auc': 0.90024648397854135, 'f1': 0.69300911854103342}\n",
      "Allele 15 model 6\n",
      "Fitting model for allele HLA-B4001 (6210 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 12 sec\n",
      "test set size: 407\n",
      "{'tau': 0.61351737384489569, 'auc': 0.91358561693489926, 'f1': 0.65408805031446549}\n",
      "Allele 15 model 7\n",
      "Fitting model for allele HLA-B4001 (6210 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 10 sec\n",
      "test set size: 407\n",
      "{'tau': 0.622842681691814, 'auc': 0.92443574500990766, 'f1': 0.4679245283018868}\n",
      "Allele 16 model 0\n",
      "Fitting model for allele HLA-B5801 (3616 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 4 sec\n",
      "test set size: 445\n",
      "{'tau': 0.55939216750167464, 'auc': 0.89477512425748584, 'f1': 0.83251231527093594}\n",
      "Allele 16 model 1\n",
      "Fitting model for allele HLA-B5801 (3616 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 4 sec\n",
      "test set size: 445\n",
      "{'tau': 0.57168806940826566, 'auc': 0.90431163373338175, 'f1': 0.83582089552238792}\n",
      "Allele 16 model 2\n",
      "Fitting model for allele HLA-B5801 (3616 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 5 sec\n",
      "test set size: 445\n",
      "{'tau': 0.58791947557351676, 'auc': 0.90726148624075642, 'f1': 0.82564102564102571}\n",
      "Allele 16 model 3\n",
      "Fitting model for allele HLA-B5801 (3616 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 5 sec\n",
      "test set size: 445\n",
      "{'tau': 0.58712421823627459, 'auc': 0.90346304602578076, 'f1': 0.7560321715817695}\n",
      "Allele 16 model 4\n",
      "Fitting model for allele HLA-B5801 (3616 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 7 sec\n",
      "test set size: 445\n",
      "{'tau': 0.54935969032415766, 'auc': 0.89299713096536948, 'f1': 0.82706766917293228}\n",
      "Allele 16 model 5\n",
      "Fitting model for allele HLA-B5801 (3616 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 6 sec\n",
      "test set size: 445\n",
      "{'tau': 0.57456323055060288, 'auc': 0.90412979351032441, 'f1': 0.83919597989949735}\n",
      "Allele 16 model 6\n",
      "Fitting model for allele HLA-B5801 (3616 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 8 sec\n",
      "test set size: 445\n",
      "{'tau': 0.58806221406994486, 'auc': 0.90709985048692776, 'f1': 0.82776349614395894}\n",
      "Allele 16 model 7\n",
      "Fitting model for allele HLA-B5801 (3616 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 7 sec\n",
      "test set size: 445\n",
      "{'tau': 0.59061111579187497, 'auc': 0.9077665979714713, 'f1': 0.79365079365079372}\n",
      "Allele 17 model 0\n",
      "Fitting model for allele HLA-A2402 (7418 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 9 sec\n",
      "test set size: 573\n",
      "{'tau': 0.53571426876789507, 'auc': 0.85100377205640365, 'f1': 0.65365853658536577}\n",
      "Allele 17 model 1\n",
      "Fitting model for allele HLA-A2402 (7418 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 8 sec\n",
      "test set size: 573\n",
      "{'tau': 0.55152212996237804, 'auc': 0.85363661679451153, 'f1': 0.6491646778042961}\n",
      "Allele 17 model 2\n",
      "Fitting model for allele HLA-A2402 (7418 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 10 sec\n",
      "test set size: 573\n",
      "{'tau': 0.58567203087746333, 'auc': 0.87071213387002855, 'f1': 0.58536585365853655}\n",
      "Allele 17 model 3\n",
      "Fitting model for allele HLA-A2402 (7418 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 9 sec\n",
      "test set size: 573\n",
      "{'tau': 0.57666708582348547, 'auc': 0.86859826333510537, 'f1': 0.3298245614035088}\n",
      "Allele 17 model 4\n",
      "Fitting model for allele HLA-A2402 (7418 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 11 sec\n",
      "test set size: 573\n",
      "{'tau': 0.53085504295324859, 'auc': 0.84371281739702786, 'f1': 0.63291139240506333}\n",
      "Allele 17 model 5\n",
      "Fitting model for allele HLA-A2402 (7418 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 10 sec\n",
      "test set size: 573\n",
      "{'tau': 0.54029055231991663, 'auc': 0.8407128933444723, 'f1': 0.65714285714285703}\n",
      "Allele 17 model 6\n",
      "Fitting model for allele HLA-A2402 (7418 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 13 sec\n",
      "test set size: 573\n",
      "{'tau': 0.57861077614934409, 'auc': 0.86599073441178698, 'f1': 0.58378378378378382}\n",
      "Allele 17 model 7\n",
      "Fitting model for allele HLA-A2402 (7418 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 15 sec\n",
      "test set size: 573\n",
      "{'tau': 0.57695002808611051, 'auc': 0.86877547403863187, 'f1': 0.38666666666666671}\n",
      "Allele 18 model 0\n",
      "Fitting model for allele HLA-A2902 (7384 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 9 sec\n",
      "test set size: 118\n",
      "{'tau': 0.62045522051562052, 'auc': 0.8909238909238909, 'f1': 0.70000000000000007}\n",
      "Allele 18 model 1\n",
      "Fitting model for allele HLA-A2902 (7384 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 8 sec\n",
      "test set size: 118\n",
      "{'tau': 0.64931360286518425, 'auc': 0.88481888481888482, 'f1': 0.66666666666666663}\n",
      "Allele 18 model 2\n",
      "Fitting model for allele HLA-A2902 (7384 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 10 sec\n",
      "test set size: 118\n",
      "{'tau': 0.62137623271826614, 'auc': 0.87301587301587302, 'f1': 0.64406779661016944}\n",
      "Allele 18 model 3\n",
      "Fitting model for allele HLA-A2902 (7384 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 9 sec\n",
      "test set size: 118\n",
      "{'tau': 0.60387700086799878, 'auc': 0.86202686202686196, 'f1': 0.65454545454545454}\n",
      "Allele 18 model 4\n",
      "Fitting model for allele HLA-A2902 (7384 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 11 sec\n",
      "test set size: 118\n",
      "{'tau': 0.61370113102955237, 'auc': 0.90923890923890927, 'f1': 0.75}\n",
      "Allele 18 model 5\n",
      "Fitting model for allele HLA-A2902 (7384 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 10 sec\n",
      "test set size: 118\n",
      "{'tau': 0.64931360286518425, 'auc': 0.88481888481888482, 'f1': 0.65573770491803285}\n",
      "Allele 18 model 6\n",
      "Fitting model for allele HLA-A2902 (7384 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 13 sec\n",
      "test set size: 118\n",
      "{'tau': 0.62506028152884874, 'auc': 0.87382987382987387, 'f1': 0.64406779661016944}\n",
      "Allele 18 model 7\n",
      "Fitting model for allele HLA-A2902 (7384 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 12 sec\n",
      "test set size: 118\n",
      "{'tau': 0.60418400493554725, 'auc': 0.86161986161986159, 'f1': 0.65454545454545454}\n",
      "Allele 19 model 0\n",
      "Fitting model for allele HLA-B3501 (5983 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 7 sec\n",
      "test set size: 542\n",
      "{'tau': 0.50089626948469679, 'auc': 0.82805010051028305, 'f1': 0.73790322580645162}\n",
      "Allele 19 model 1\n",
      "Fitting model for allele HLA-B3501 (5983 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 6 sec\n",
      "test set size: 542\n",
      "{'tau': 0.52418167291983619, 'auc': 0.8310021507794817, 'f1': 0.72265625}\n",
      "Allele 19 model 2\n",
      "Fitting model for allele HLA-B3501 (5983 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 8 sec\n",
      "test set size: 542\n",
      "{'tau': 0.51993800739079843, 'auc': 0.82934337967583671, 'f1': 0.72210953346855988}\n",
      "Allele 19 model 3\n",
      "Fitting model for allele HLA-B3501 (5983 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 7 sec\n",
      "test set size: 542\n",
      "{'tau': 0.49265534481217826, 'auc': 0.82246931976327364, 'f1': 0.7029478458049887}\n",
      "Allele 19 model 4\n",
      "Fitting model for allele HLA-B3501 (5983 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 10 sec\n",
      "test set size: 542\n",
      "{'tau': 0.47841853142443874, 'auc': 0.81479398906335654, 'f1': 0.70927835051546384}\n",
      "Allele 19 model 5\n",
      "Fitting model for allele HLA-B3501 (5983 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 9 sec\n",
      "test set size: 542\n",
      "{'tau': 0.50490721787181958, 'auc': 0.81957349902301191, 'f1': 0.71314741035856566}\n",
      "Allele 19 model 6\n",
      "Fitting model for allele HLA-B3501 (5983 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 11 sec\n",
      "test set size: 542\n",
      "{'tau': 0.51752870050979638, 'auc': 0.82833124815496861, 'f1': 0.7142857142857143}\n",
      "Allele 19 model 7\n",
      "Fitting model for allele HLA-B3501 (5983 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 10 sec\n",
      "test set size: 542\n",
      "{'tau': 0.50318237317292036, 'auc': 0.82471850092075849, 'f1': 0.70953436807095338}\n",
      "Allele 20 model 0\n",
      "Fitting model for allele HLA-B0801 (3883 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 5 sec\n",
      "test set size: 940\n",
      "{'tau': 0.60081732086857353, 'auc': 0.94269407586932386, 'f1': 0.76293103448275867}\n",
      "Allele 20 model 1\n",
      "Fitting model for allele HLA-B0801 (3883 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 4 sec\n",
      "test set size: 940\n",
      "{'tau': 0.62034199886719654, 'auc': 0.949517316734376, 'f1': 0.77350427350427353}\n",
      "Allele 20 model 2\n",
      "Fitting model for allele HLA-B0801 (3883 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 6 sec\n",
      "test set size: 940\n",
      "{'tau': 0.61649109382293987, 'auc': 0.94827355732626595, 'f1': 0.778523489932886}\n",
      "Allele 20 model 3\n",
      "Fitting model for allele HLA-B0801 (3883 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 5 sec\n",
      "test set size: 940\n",
      "{'tau': 0.60395600495438984, 'auc': 0.94056108660401372, 'f1': 0.58855585831062673}\n",
      "Allele 20 model 4\n",
      "Fitting model for allele HLA-B0801 (3883 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 7 sec\n",
      "test set size: 940\n",
      "{'tau': 0.58823311331323447, 'auc': 0.93942775443307236, 'f1': 0.73799126637554591}\n",
      "Allele 20 model 5\n",
      "Fitting model for allele HLA-B0801 (3883 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 7 sec\n",
      "test set size: 940\n",
      "{'tau': 0.61259598196057308, 'auc': 0.95061577714621148, 'f1': 0.7544642857142857}\n",
      "Allele 20 model 6\n",
      "Fitting model for allele HLA-B0801 (3883 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 9 sec\n",
      "test set size: 940\n",
      "{'tau': 0.61617673422749031, 'auc': 0.94933133401914449, 'f1': 0.77973568281938321}\n",
      "Allele 20 model 7\n",
      "Fitting model for allele HLA-B0801 (3883 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 8 sec\n",
      "test set size: 940\n",
      "{'tau': 0.61117645191237124, 'auc': 0.94459458674059482, 'f1': 0.63324538258575203}\n",
      "Allele 21 model 0\n",
      "Fitting model for allele Mamu-A01 (14764 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 17 sec\n",
      "test set size: 274\n",
      "{'tau': 0.45504873122382083, 'auc': 0.84714110606388338, 'f1': 0.58333333333333337}\n",
      "Allele 21 model 1\n",
      "Fitting model for allele Mamu-A01 (14764 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 16 sec\n",
      "test set size: 274\n",
      "{'tau': 0.50609586453418542, 'auc': 0.88917730189631539, 'f1': 0.64383561643835607}\n",
      "Allele 21 model 2\n",
      "Fitting model for allele Mamu-A01 (14764 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 20 sec\n",
      "test set size: 274\n",
      "{'tau': 0.52759508046807435, 'auc': 0.92277741726151852, 'f1': 0.71641791044776126}\n",
      "Allele 21 model 3\n",
      "Fitting model for allele Mamu-A01 (14764 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 18 sec\n",
      "test set size: 274\n",
      "{'tau': 0.49702081861234282, 'auc': 0.9176580863796957, 'f1': 0.7384615384615385}\n",
      "Allele 21 model 4\n",
      "Fitting model for allele Mamu-A01 (14764 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 20 sec\n",
      "test set size: 274\n",
      "{'tau': 0.44230045242885152, 'auc': 0.84310332395991061, 'f1': 0.6467065868263473}\n",
      "Allele 21 model 5\n",
      "Fitting model for allele Mamu-A01 (14764 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 22 sec\n",
      "test set size: 274\n",
      "{'tau': 0.53051205951438085, 'auc': 0.9079241473790467, 'f1': 0.6802721088435375}\n",
      "Allele 21 model 6\n",
      "Fitting model for allele Mamu-A01 (14764 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 23 sec\n",
      "test set size: 274\n",
      "{'tau': 0.52532631898761373, 'auc': 0.92097483596510199, 'f1': 0.71942446043165464}\n",
      "Allele 21 model 7\n",
      "Fitting model for allele Mamu-A01 (14764 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 21 sec\n",
      "test set size: 274\n",
      "{'tau': 0.50490746566346789, 'auc': 0.91506236931285601, 'f1': 0.69696969696969702}\n",
      "Allele 22 model 0\n",
      "Fitting model for allele HLA-A6901 (2079 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 3 sec\n",
      "test set size: 470\n",
      "{'tau': 0.33818914856959248, 'auc': 0.95885308385308388, 'f1': 0.63888888888888884}\n",
      "Allele 22 model 1\n",
      "Fitting model for allele HLA-A6901 (2079 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 2 sec\n",
      "test set size: 470\n",
      "{'tau': 0.34278882260810867, 'auc': 0.96483021483021492, 'f1': 0.66666666666666652}\n",
      "Allele 22 model 2\n",
      "Fitting model for allele HLA-A6901 (2079 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 3 sec\n",
      "test set size: 470\n",
      "{'tau': 0.32578510956408568, 'auc': 0.95694733194733195, 'f1': 0.69999999999999996}\n",
      "Allele 22 model 3\n",
      "Fitting model for allele HLA-A6901 (2079 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 3 sec\n",
      "test set size: 470\n",
      "{'tau': 0.31183527846366771, 'auc': 0.9424809424809425, 'f1': 0.51282051282051277}\n",
      "Allele 22 model 4\n",
      "Fitting model for allele HLA-A6901 (2079 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 5 sec\n",
      "test set size: 470\n",
      "{'tau': 0.33464513742516194, 'auc': 0.96136521136521136, 'f1': 0.69696969696969702}\n",
      "Allele 22 model 5\n",
      "Fitting model for allele HLA-A6901 (2079 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 5 sec\n",
      "test set size: 470\n",
      "{'tau': 0.34282652485432602, 'auc': 0.96578309078309088, 'f1': 0.67692307692307696}\n",
      "Allele 22 model 6\n",
      "Fitting model for allele HLA-A6901 (2079 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 6 sec\n",
      "test set size: 470\n",
      "{'tau': 0.32265582312804597, 'auc': 0.95356895356895366, 'f1': 0.72131147540983598}\n",
      "Allele 22 model 7\n",
      "Fitting model for allele HLA-A6901 (2079 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 6 sec\n",
      "test set size: 470\n",
      "{'tau': 0.31262844390376437, 'auc': 0.94239431739431745, 'f1': 0.51162790697674421}\n",
      "Allele 23 model 0\n",
      "Fitting model for allele HLA-B1801 (5535 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 7 sec\n",
      "test set size: 503\n",
      "{'tau': 0.20132512250941245, 'auc': 0.77526315789473688, 'f1': 0.32432432432432429}\n",
      "Allele 23 model 1\n",
      "Fitting model for allele HLA-B1801 (5535 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 6 sec\n",
      "test set size: 503\n",
      "{'tau': 0.24180318946897686, 'auc': 0.81150375939849617, 'f1': 0.35897435897435898}\n",
      "Allele 23 model 2\n",
      "Fitting model for allele HLA-B1801 (5535 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 8 sec\n",
      "test set size: 503\n",
      "{'tau': 0.23117736812286638, 'auc': 0.77443609022556392, 'f1': 0.33333333333333331}\n",
      "Allele 23 model 3\n",
      "Fitting model for allele HLA-B1801 (5535 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 7 sec\n",
      "test set size: 503\n",
      "{'tau': 0.23196641426242903, 'auc': 0.76864661654135336, 'f1': 0.068965517241379296}\n",
      "Allele 23 model 4\n",
      "Fitting model for allele HLA-B1801 (5535 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 9 sec\n",
      "test set size: 503\n",
      "{'tau': 0.21679042684484057, 'auc': 0.80511278195488711, 'f1': 0.51063829787234039}\n",
      "Allele 23 model 5\n",
      "Fitting model for allele HLA-B1801 (5535 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 8 sec\n",
      "test set size: 503\n",
      "{'tau': 0.23338669731364184, 'auc': 0.79894736842105263, 'f1': 0.36842105263157893}\n",
      "Allele 23 model 6\n",
      "Fitting model for allele HLA-B1801 (5535 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 11 sec\n",
      "test set size: 503\n",
      "{'tau': 0.23096695581898302, 'auc': 0.77503759398496241, 'f1': 0.33333333333333331}\n",
      "Allele 23 model 7\n",
      "Fitting model for allele HLA-B1801 (5535 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 10 sec\n",
      "test set size: 503\n",
      "{'tau': 0.24448594634348991, 'auc': 0.7784962406015038, 'f1': 0.068965517241379296}\n",
      "Allele 24 model 0\n",
      "Fitting model for allele HLA-A3001 (2857 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 3 sec\n",
      "test set size: 660\n",
      "{'tau': 0.50209043409082954, 'auc': 0.88873996173717784, 'f1': 0.73086419753086429}\n",
      "Allele 24 model 1\n",
      "Fitting model for allele HLA-A3001 (2857 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 3 sec\n",
      "test set size: 660\n",
      "{'tau': 0.4993903931049708, 'auc': 0.88144253130334171, 'f1': 0.73791348600508899}\n",
      "Allele 24 model 2\n",
      "Fitting model for allele HLA-A3001 (2857 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 4 sec\n",
      "test set size: 660\n",
      "{'tau': 0.52154385438824946, 'auc': 0.88959915684320268, 'f1': 0.74559193954659952}\n",
      "Allele 24 model 3\n",
      "Fitting model for allele HLA-A3001 (2857 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 4 sec\n",
      "test set size: 660\n",
      "{'tau': 0.52064384072629655, 'auc': 0.89368892554788015, 'f1': 0.75000000000000011}\n",
      "Allele 24 model 4\n",
      "Fitting model for allele HLA-A3001 (2857 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 6 sec\n",
      "test set size: 660\n",
      "{'tau': 0.49761849120800095, 'auc': 0.88235900608310147, 'f1': 0.71462829736211031}\n",
      "Allele 24 model 5\n",
      "Fitting model for allele HLA-A3001 (2857 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 6 sec\n",
      "test set size: 660\n",
      "{'tau': 0.51036868475233399, 'auc': 0.88816716499982817, 'f1': 0.72864321608040183}\n",
      "Allele 24 model 6\n",
      "Fitting model for allele HLA-A3001 (2857 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 7 sec\n",
      "test set size: 660\n",
      "{'tau': 0.52073759214941662, 'auc': 0.88810988532609325, 'f1': 0.74936708860759493}\n",
      "Allele 24 model 7\n",
      "Fitting model for allele HLA-A3001 (2857 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 6 sec\n",
      "test set size: 660\n",
      "{'tau': 0.52819083028746427, 'auc': 0.89521256486923062, 'f1': 0.7506561679790027}\n",
      "Allele 25 model 0\n",
      "Fitting model for allele HLA-A2301 (6557 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 8 sec\n",
      "test set size: 391\n",
      "{'tau': 0.55641004271320726, 'auc': 0.84058086439418078, 'f1': 0.71060171919770776}\n",
      "Allele 25 model 1\n",
      "Fitting model for allele HLA-A2301 (6557 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 7 sec\n",
      "test set size: 391\n",
      "{'tau': 0.59622641974380863, 'auc': 0.87384517362217273, 'f1': 0.77348066298342533}\n",
      "Allele 25 model 2\n",
      "Fitting model for allele HLA-A2301 (6557 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 9 sec\n",
      "test set size: 391\n",
      "{'tau': 0.58293664496364783, 'auc': 0.87057980248486766, 'f1': 0.74336283185840701}\n",
      "Allele 25 model 3\n",
      "Fitting model for allele HLA-A2301 (6557 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 8 sec\n",
      "test set size: 391\n",
      "{'tau': 0.57467686462219336, 'auc': 0.87140278220239997, 'f1': 0.60869565217391297}\n",
      "Allele 25 model 4\n",
      "Fitting model for allele HLA-A2301 (6557 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 10 sec\n",
      "test set size: 391\n",
      "{'tau': 0.55839556683374925, 'auc': 0.85534140384411173, 'f1': 0.75630252100840334}\n",
      "Allele 25 model 5\n",
      "Fitting model for allele HLA-A2301 (6557 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 9 sec\n",
      "test set size: 391\n",
      "{'tau': 0.56586113752698708, 'auc': 0.85085483699692044, 'f1': 0.7118644067796609}\n",
      "Allele 25 model 6\n",
      "Fitting model for allele HLA-A2301 (6557 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 12 sec\n",
      "test set size: 391\n",
      "{'tau': 0.59275837094659534, 'auc': 0.87426993734735048, 'f1': 0.74635568513119532}\n",
      "Allele 25 model 7\n",
      "Fitting model for allele HLA-A2301 (6557 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 11 sec\n",
      "test set size: 391\n",
      "{'tau': 0.57224128836766186, 'auc': 0.86850907932462573, 'f1': 0.60215053763440851}\n",
      "Allele 26 model 0\n",
      "Fitting model for allele HLA-B5701 (3070 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 4 sec\n",
      "test set size: 815\n",
      "{'tau': 0.52493303553059356, 'auc': 0.91423964114401712, 'f1': 0.75}\n",
      "Allele 26 model 1\n",
      "Fitting model for allele HLA-B5701 (3070 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 9 sec\n",
      "test set size: 815\n",
      "{'tau': 0.53522746470861926, 'auc': 0.91677717204459519, 'f1': 0.74029850746268655}\n",
      "Allele 26 model 2\n",
      "Fitting model for allele HLA-B5701 (3070 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 4 sec\n",
      "test set size: 815\n",
      "{'tau': 0.5453769019263911, 'auc': 0.9228181327046806, 'f1': 0.67088607594936722}\n",
      "Allele 26 model 3\n",
      "Fitting model for allele HLA-B5701 (3070 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 4 sec\n",
      "test set size: 815\n",
      "{'tau': 0.53824467835771195, 'auc': 0.91424782672756733, 'f1': 0.55749128919860624}\n",
      "Allele 26 model 4\n",
      "Fitting model for allele HLA-B5701 (3070 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 6 sec\n",
      "test set size: 815\n",
      "{'tau': 0.52229556272978483, 'auc': 0.91324099995088648, 'f1': 0.75739644970414199}\n",
      "Allele 26 model 5\n",
      "Fitting model for allele HLA-B5701 (3070 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 6 sec\n",
      "test set size: 815\n",
      "{'tau': 0.51802175209182522, 'auc': 0.90824779398523325, 'f1': 0.74927953890489918}\n",
      "Allele 26 model 6\n",
      "Fitting model for allele HLA-B5701 (3070 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 7 sec\n",
      "test set size: 815\n",
      "{'tau': 0.54857362943103627, 'auc': 0.92358757755840415, 'f1': 0.70186335403726707}\n",
      "Allele 26 model 7\n",
      "Fitting model for allele HLA-B5701 (3070 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 7 sec\n",
      "test set size: 815\n",
      "{'tau': 0.54130331828116296, 'auc': 0.91718645122210762, 'f1': 0.53521126760563376}\n",
      "Allele 27 model 0\n",
      "Fitting model for allele HLA-B5101 (5267 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 6 sec\n",
      "test set size: 854\n",
      "{'tau': 0.36919218755237138, 'auc': 0.93304984177215189, 'f1': 0.48979591836734687}\n",
      "Allele 27 model 1\n",
      "Fitting model for allele HLA-B5101 (5267 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 6 sec\n",
      "test set size: 854\n",
      "{'tau': 0.36935861432346495, 'auc': 0.93908227848101267, 'f1': 0.39130434782608697}\n",
      "Allele 27 model 2\n",
      "Fitting model for allele HLA-B5101 (5267 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 7 sec\n",
      "test set size: 854\n",
      "{'tau': 0.38239751173567743, 'auc': 0.94814082278481027, 'f1': 0.40476190476190471}\n",
      "Allele 27 model 3\n",
      "Fitting model for allele HLA-B5101 (5267 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 7 sec\n",
      "test set size: 854\n",
      "{'tau': 0.37372411654984145, 'auc': 0.94566851265822782, 'f1': 0.22222222222222221}\n",
      "Allele 27 model 4\n",
      "Fitting model for allele HLA-B5101 (5267 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 9 sec\n",
      "test set size: 854\n",
      "{'tau': 0.35462984508168732, 'auc': 0.91386471518987333, 'f1': 0.46296296296296297}\n",
      "Allele 27 model 5\n",
      "Fitting model for allele HLA-B5101 (5267 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 8 sec\n",
      "test set size: 854\n",
      "{'tau': 0.36662537465973655, 'auc': 0.93174446202531647, 'f1': 0.45454545454545453}\n",
      "Allele 27 model 6\n",
      "Fitting model for allele HLA-B5101 (5267 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 10 sec\n",
      "test set size: 854\n",
      "{'tau': 0.37976668854646817, 'auc': 0.94612341772151898, 'f1': 0.41860465116279066}\n",
      "Allele 27 model 7\n",
      "Fitting model for allele HLA-B5101 (5267 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 9 sec\n",
      "test set size: 854\n",
      "{'tau': 0.37949784530085551, 'auc': 0.94984177215189869, 'f1': 0.24657534246575341}\n",
      "Allele 28 model 0\n",
      "Fitting model for allele HLA-B4402 (5358 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 6 sec\n",
      "test set size: 411\n",
      "{'tau': 0.5064048392074374, 'auc': 0.86975738650012002, 'f1': 0.68229166666666674}\n",
      "Allele 28 model 1\n",
      "Fitting model for allele HLA-B4402 (5358 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 6 sec\n",
      "test set size: 411\n",
      "{'tau': 0.51306065772147502, 'auc': 0.86819601249099199, 'f1': 0.60115606936416177}\n",
      "Allele 28 model 2\n",
      "Fitting model for allele HLA-B4402 (5358 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 7 sec\n",
      "test set size: 411\n",
      "{'tau': 0.54420893069544263, 'auc': 0.88719673312515002, 'f1': 0.51757188498402562}\n",
      "Allele 28 model 3\n",
      "Fitting model for allele HLA-B4402 (5358 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 7 sec\n",
      "test set size: 411\n",
      "{'tau': 0.54782414147105307, 'auc': 0.88950276243093918, 'f1': 0.12244897959183672}\n",
      "Allele 28 model 4\n",
      "Fitting model for allele HLA-B4402 (5358 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 9 sec\n",
      "test set size: 411\n",
      "{'tau': 0.50822441549185049, 'auc': 0.87105452798462646, 'f1': 0.70129870129870131}\n",
      "Allele 28 model 5\n",
      "Fitting model for allele HLA-B4402 (5358 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 8 sec\n",
      "test set size: 411\n",
      "{'tau': 0.524672427431217, 'auc': 0.87631515733845777, 'f1': 0.61408450704225348}\n",
      "Allele 28 model 6\n",
      "Fitting model for allele HLA-B4402 (5358 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 10 sec\n",
      "test set size: 411\n",
      "{'tau': 0.54887758037255541, 'auc': 0.88798943069901526, 'f1': 0.488599348534202}\n",
      "Allele 28 model 7\n",
      "Fitting model for allele HLA-B4402 (5358 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 9 sec\n",
      "test set size: 411\n",
      "{'tau': 0.532357743053541, 'auc': 0.88109536392024979, 'f1': 0.15936254980079684}\n",
      "Skipping allele Mamu-B17: not in test set\n",
      "Allele 30 model 0\n",
      "Fitting model for allele HLA-A3002 (6055 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 7 sec\n",
      "test set size: 420\n",
      "{'tau': 0.31522651222628539, 'auc': 0.71157447005247454, 'f1': 0.61951219512195121}\n",
      "Allele 30 model 1\n",
      "Fitting model for allele HLA-A3002 (6055 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 6 sec\n",
      "test set size: 420\n",
      "{'tau': 0.3340003903782664, 'auc': 0.72898125245613632, 'f1': 0.62278481012658227}\n",
      "Allele 30 model 2\n",
      "Fitting model for allele HLA-A3002 (6055 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 8 sec\n",
      "test set size: 420\n",
      "{'tau': 0.34358265131997517, 'auc': 0.74204211840310685, 'f1': 0.64516129032258074}\n",
      "Allele 30 model 3\n",
      "Fitting model for allele HLA-A3002 (6055 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 8 sec\n",
      "test set size: 420\n",
      "{'tau': 0.34686865447264748, 'auc': 0.73841281583023188, 'f1': 0.59726027397260273}\n",
      "Allele 30 model 4\n",
      "Fitting model for allele HLA-A3002 (6055 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 10 sec\n",
      "test set size: 420\n",
      "{'tau': 0.32467089890984013, 'auc': 0.71573545389398741, 'f1': 0.64916467780429588}\n",
      "Allele 30 model 5\n",
      "Fitting model for allele HLA-A3002 (6055 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 9 sec\n",
      "test set size: 420\n",
      "{'tau': 0.327773069718307, 'auc': 0.72491273492221264, 'f1': 0.6259541984732826}\n",
      "Allele 30 model 6\n",
      "Fitting model for allele HLA-A3002 (6055 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 11 sec\n",
      "test set size: 420\n",
      "{'tau': 0.34365158844905225, 'auc': 0.74030837513580994, 'f1': 0.63131313131313138}\n",
      "Allele 30 model 7\n",
      "Fitting model for allele HLA-A3002 (6055 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 10 sec\n",
      "test set size: 420\n",
      "{'tau': 0.34868399887167628, 'auc': 0.74333664670935529, 'f1': 0.62140992167101838}\n",
      "Allele 31 model 0\n",
      "Fitting model for allele HLA-B4601 (1582 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 2 sec\n",
      "test set size: 378\n",
      "{'tau': nan, 'auc': nan, 'f1': 0.0}\n",
      "Allele 31 model 1\n",
      "Fitting model for allele HLA-B4601 (1582 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/anaconda2/envs/standard-2.7/lib/python2.7/site-packages/sklearn/metrics/classification.py:1076: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained in 2 sec\n",
      "test set size: 378\n",
      "{'tau': nan, 'auc': nan, 'f1': 0.0}\n",
      "Allele 31 model 2\n",
      "Fitting model for allele HLA-B4601 (1582 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 2 sec\n",
      "test set size: 378\n",
      "{'tau': nan, 'auc': nan, 'f1': 0.0}\n",
      "Allele 31 model 3\n",
      "Fitting model for allele HLA-B4601 (1582 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 2 sec\n",
      "test set size: 378\n",
      "{'tau': nan, 'auc': nan, 'f1': 0.0}\n",
      "Allele 31 model 4\n",
      "Fitting model for allele HLA-B4601 (1582 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 5 sec\n",
      "test set size: 378\n",
      "{'tau': nan, 'auc': nan, 'f1': 0.0}\n",
      "Allele 31 model 5\n",
      "Fitting model for allele HLA-B4601 (1582 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 4 sec\n",
      "test set size: 378\n",
      "{'tau': nan, 'auc': nan, 'f1': 0.0}\n",
      "Allele 31 model 6\n",
      "Fitting model for allele HLA-B4601 (1582 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 6 sec\n",
      "test set size: 378\n",
      "{'tau': nan, 'auc': nan, 'f1': 0.0}\n",
      "Allele 31 model 7\n",
      "Fitting model for allele HLA-B4601 (1582 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 5 sec\n",
      "test set size: 378\n",
      "{'tau': nan, 'auc': nan, 'f1': 0.0}\n",
      "Skipping allele Mamu-A11: not in test set\n",
      "Skipping allele HLA-A0219: not in test set\n",
      "Skipping allele HLA-A2403: not in test set\n",
      "Skipping allele HLA-A0212: not in test set\n",
      "Skipping allele Mamu-B03: not in test set\n",
      "Skipping allele Mamu-B08: not in test set\n",
      "Skipping allele HLA-A0211: not in test set\n",
      "Allele 39 model 0\n",
      "Fitting model for allele HLA-B5401 (4552 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 5 sec\n",
      "test set size: 79\n",
      "{'tau': 0.29578656384800817, 'auc': 0.80000000000000004, 'f1': 0.72727272727272718}\n",
      "Allele 39 model 1\n",
      "Fitting model for allele HLA-B5401 (4552 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 5 sec\n",
      "test set size: 79\n",
      "{'tau': 0.31482233280852351, 'auc': 0.81621621621621632, 'f1': 0.66666666666666663}\n",
      "Allele 39 model 2\n",
      "Fitting model for allele HLA-B5401 (4552 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 6 sec\n",
      "test set size: 79\n",
      "{'tau': 0.31189375296844424, 'auc': 0.80000000000000004, 'f1': 0.72727272727272718}\n",
      "Allele 39 model 3\n",
      "Fitting model for allele HLA-B5401 (4552 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 6 sec\n",
      "test set size: 79\n",
      "{'tau': 0.28992940416784957, 'auc': 0.79729729729729737, 'f1': 0.72727272727272718}\n",
      "Allele 39 model 4\n",
      "Fitting model for allele HLA-B5401 (4552 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 14 sec\n",
      "test set size: 79\n",
      "{'tau': 0.29359012896794867, 'auc': 0.81081081081081086, 'f1': 0.72727272727272718}\n",
      "Allele 39 model 5\n",
      "Fitting model for allele HLA-B5401 (4552 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 7 sec\n",
      "test set size: 79\n",
      "{'tau': 0.26650076544721524, 'auc': 0.82972972972972969, 'f1': 0.72727272727272718}\n",
      "Allele 39 model 6\n",
      "Fitting model for allele HLA-B5401 (4552 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 10 sec\n",
      "test set size: 79\n",
      "{'tau': 0.3038401584082262, 'auc': 0.79729729729729737, 'f1': 0.72727272727272718}\n",
      "Allele 39 model 7\n",
      "Fitting model for allele HLA-B5401 (4552 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 9 sec\n",
      "test set size: 79\n",
      "{'tau': 0.33312595680901907, 'auc': 0.81351351351351353, 'f1': 0.72727272727272718}\n",
      "Allele 40 model 0\n",
      "Fitting model for allele HLA-B5301 (4551 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 5 sec\n",
      "test set size: 485\n",
      "{'tau': 0.51127462707363569, 'auc': 0.83627379336743579, 'f1': 0.71627906976744182}\n",
      "Allele 40 model 1\n",
      "Fitting model for allele HLA-B5301 (4551 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 5 sec\n",
      "test set size: 485\n",
      "{'tau': 0.52818577426104762, 'auc': 0.84954936145039495, 'f1': 0.74056603773584906}\n",
      "Allele 40 model 2\n",
      "Fitting model for allele HLA-B5301 (4551 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 6 sec\n",
      "test set size: 485\n",
      "{'tau': 0.51863998965576741, 'auc': 0.83493405713888014, 'f1': 0.71226415094339623}\n",
      "Allele 40 model 3\n",
      "Fitting model for allele HLA-B5301 (4551 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 6 sec\n",
      "test set size: 485\n",
      "{'tau': 0.54667643976444102, 'auc': 0.85155026620732843, 'f1': 0.7010309278350515}\n",
      "Allele 40 model 4\n",
      "Fitting model for allele HLA-B5301 (4551 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 8 sec\n",
      "test set size: 485\n",
      "{'tau': 0.50238125220756535, 'auc': 0.83258516894595813, 'f1': 0.72160356347438759}\n",
      "Allele 40 model 5\n",
      "Fitting model for allele HLA-B5301 (4551 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 7 sec\n",
      "test set size: 485\n",
      "{'tau': 0.52535294249868925, 'auc': 0.8464001113546995, 'f1': 0.72146118721461183}\n",
      "Allele 40 model 6\n",
      "Fitting model for allele HLA-B5301 (4551 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 9 sec\n",
      "test set size: 485\n",
      "{'tau': 0.52964511183559582, 'auc': 0.84518216932874002, 'f1': 0.7132530120481928}\n",
      "Allele 40 model 7\n",
      "Fitting model for allele HLA-B5301 (4551 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 8 sec\n",
      "test set size: 485\n",
      "{'tau': 0.53586017303543654, 'auc': 0.84807043184744413, 'f1': 0.66844919786096246}\n",
      "Allele 41 model 0\n",
      "Fitting model for allele Mamu-A02 (6027 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 7 sec\n",
      "test set size: 388\n",
      "{'tau': 0.50432868358668881, 'auc': 0.87698859315589361, 'f1': 0.68221574344023317}\n",
      "Allele 41 model 1\n",
      "Fitting model for allele Mamu-A02 (6027 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 6 sec\n",
      "test set size: 388\n",
      "{'tau': 0.50761430019404075, 'auc': 0.87005323193916351, 'f1': 0.66855524079320117}\n",
      "Allele 41 model 2\n",
      "Fitting model for allele Mamu-A02 (6027 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 8 sec\n",
      "test set size: 388\n",
      "{'tau': 0.48539599116891496, 'auc': 0.85399239543726235, 'f1': 0.66666666666666674}\n",
      "Allele 41 model 3\n",
      "Fitting model for allele Mamu-A02 (6027 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 7 sec\n",
      "test set size: 388\n",
      "{'tau': 0.45641792994341757, 'auc': 0.82606844106463884, 'f1': 0.64425770308123254}\n",
      "Allele 41 model 4\n",
      "Fitting model for allele Mamu-A02 (6027 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 10 sec\n",
      "test set size: 388\n",
      "{'tau': 0.52253423134545862, 'auc': 0.8837718631178707, 'f1': 0.68804664723032061}\n",
      "Allele 41 model 5\n",
      "Fitting model for allele Mamu-A02 (6027 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 9 sec\n",
      "test set size: 388\n",
      "{'tau': 0.50201259319134239, 'auc': 0.86795437262357411, 'f1': 0.65745856353591159}\n",
      "Allele 41 model 6\n",
      "Fitting model for allele Mamu-A02 (6027 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 11 sec\n",
      "test set size: 388\n",
      "{'tau': 0.49562987896230626, 'auc': 0.85542205323193921, 'f1': 0.6629213483146067}\n",
      "Allele 41 model 7\n",
      "Fitting model for allele Mamu-A02 (6027 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 10 sec\n",
      "test set size: 388\n",
      "{'tau': 0.45148950503238966, 'auc': 0.81882889733840303, 'f1': 0.64553314121037453}\n",
      "Allele 42 model 0\n",
      "Fitting model for allele HLA-B4403 (4575 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 5 sec\n",
      "test set size: 378\n",
      "{'tau': 0.41999527589156055, 'auc': 0.78588473091715549, 'f1': 0.67733333333333334}\n",
      "Allele 42 model 1\n",
      "Fitting model for allele HLA-B4403 (4575 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 5 sec\n",
      "test set size: 378\n",
      "{'tau': 0.45056036237236702, 'auc': 0.81193677886639903, 'f1': 0.73282442748091603}\n",
      "Allele 42 model 2\n",
      "Fitting model for allele HLA-B4403 (4575 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 6 sec\n",
      "test set size: 378\n",
      "{'tau': 0.46236383816541576, 'auc': 0.81359310519075834, 'f1': 0.61398176291793316}\n",
      "Allele 42 model 3\n",
      "Fitting model for allele HLA-B4403 (4575 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 6 sec\n",
      "test set size: 378\n",
      "{'tau': 0.44901098010836299, 'auc': 0.80564835349934028, 'f1': 0.37209302325581395}\n",
      "Allele 42 model 4\n",
      "Fitting model for allele HLA-B4403 (4575 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 8 sec\n",
      "test set size: 378\n",
      "{'tau': 0.42475610503004563, 'auc': 0.78886050363549587, 'f1': 0.70618556701030932}\n",
      "Allele 42 model 5\n",
      "Fitting model for allele HLA-B4403 (4575 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 7 sec\n",
      "test set size: 378\n",
      "{'tau': 0.43100997525929818, 'auc': 0.79009573004688249, 'f1': 0.71938775510204078}\n",
      "Allele 42 model 6\n",
      "Fitting model for allele HLA-B4403 (4575 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 9 sec\n",
      "test set size: 378\n",
      "{'tau': 0.46030738534228316, 'auc': 0.81395805844866786, 'f1': 0.60790273556231}\n",
      "Allele 42 model 7\n",
      "Fitting model for allele HLA-B4403 (4575 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 8 sec\n",
      "test set size: 378\n",
      "{'tau': 0.46196944995276018, 'auc': 0.81617585132365733, 'f1': 0.35714285714285721}\n",
      "Skipping allele HLA-A0216: not in test set\n",
      "Allele 44 model 0\n",
      "Fitting model for allele HLA-B4501 (4507 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 5 sec\n",
      "test set size: 65\n",
      "{'tau': 0.23004498064532822, 'auc': 0.99666666666666659, 'f1': 0.74999999999999989}\n",
      "Allele 44 model 1\n",
      "Fitting model for allele HLA-B4501 (4507 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 5 sec\n",
      "test set size: 65\n",
      "{'tau': 0.24762145107665667, 'auc': 1.0, 'f1': 0.74999999999999989}\n",
      "Allele 44 model 2\n",
      "Fitting model for allele HLA-B4501 (4507 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 6 sec\n",
      "test set size: 65\n",
      "{'tau': 0.23831626084830632, 'auc': 0.99666666666666659, 'f1': 0.57142857142857151}\n",
      "Allele 44 model 3\n",
      "Fitting model for allele HLA-B4501 (4507 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 6 sec\n",
      "test set size: 65\n",
      "{'tau': 0.27140138166021871, 'auc': 0.99333333333333329, 'f1': 0.57142857142857151}\n",
      "Allele 44 model 4\n",
      "Fitting model for allele HLA-B4501 (4507 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 8 sec\n",
      "test set size: 65\n",
      "{'tau': 0.25175709117814571, 'auc': 1.0, 'f1': 0.74999999999999989}\n",
      "Allele 44 model 5\n",
      "Fitting model for allele HLA-B4501 (4507 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 7 sec\n",
      "test set size: 65\n",
      "{'tau': 0.26106228140649607, 'auc': 0.99666666666666659, 'f1': 0.74999999999999989}\n",
      "Allele 44 model 6\n",
      "Fitting model for allele HLA-B4501 (4507 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 9 sec\n",
      "test set size: 65\n",
      "{'tau': 0.24141799092442309, 'auc': 1.0, 'f1': 0.57142857142857151}\n",
      "Allele 44 model 7\n",
      "Fitting model for allele HLA-B4501 (4507 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 8 sec\n",
      "test set size: 65\n",
      "{'tau': 0.28587612201543033, 'auc': 0.99666666666666659, 'f1': 0.57142857142857151}\n",
      "Allele 45 model 0\n",
      "Fitting model for allele HLA-B3901 (948 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 1 sec\n",
      "test set size: 641\n",
      "{'tau': 0.29743503083401468, 'auc': 0.95350649350649352, 'f1': 0.70588235294117652}\n",
      "Allele 45 model 1\n",
      "Fitting model for allele HLA-B3901 (948 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 1 sec\n",
      "test set size: 641\n",
      "{'tau': 0.29516081432299546, 'auc': 0.95136363636363641, 'f1': 0.73469387755102045}\n",
      "Allele 45 model 2\n",
      "Fitting model for allele HLA-B3901 (948 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 2 sec\n",
      "test set size: 641\n",
      "{'tau': 0.30349960819673266, 'auc': 0.96025974025974026, 'f1': 0.72340425531914909}\n",
      "Allele 45 model 3\n",
      "Fitting model for allele HLA-B3901 (948 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 2 sec\n",
      "test set size: 641\n",
      "{'tau': 0.30247079596555726, 'auc': 0.95194805194805199, 'f1': 0.66666666666666663}\n",
      "Allele 45 model 4\n",
      "Fitting model for allele HLA-B3901 (948 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 4 sec\n",
      "test set size: 641\n",
      "{'tau': 0.29846384306519008, 'auc': 0.95435064935064928, 'f1': 0.74509803921568629}\n",
      "Allele 45 model 5\n",
      "Fitting model for allele HLA-B3901 (948 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 4 sec\n",
      "test set size: 641\n",
      "{'tau': 0.28844646081427205, 'auc': 0.94337662337662342, 'f1': 0.66666666666666663}\n",
      "Allele 45 model 6\n",
      "Fitting model for allele HLA-B3901 (948 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 5 sec\n",
      "test set size: 641\n",
      "{'tau': 0.30469086446440941, 'auc': 0.96051948051948055, 'f1': 0.73469387755102045}\n",
      "Allele 45 model 7\n",
      "Fitting model for allele HLA-B3901 (948 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 4 sec\n",
      "test set size: 641\n",
      "{'tau': 0.3073170430545149, 'auc': 0.95688311688311689, 'f1': 0.73170731707317072}\n",
      "Allele 46 model 0\n",
      "Fitting model for allele HLA-B4002 (4358 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 5 sec\n",
      "test set size: 74\n",
      "{'tau': 0.49430024768381203, 'auc': 0.91666666666666663, 'f1': 0.76470588235294112}\n",
      "Allele 46 model 1\n",
      "Fitting model for allele HLA-B4002 (4358 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 5 sec\n",
      "test set size: 74\n",
      "{'tau': 0.4818454382933538, 'auc': 0.90873015873015872, 'f1': 0.68965517241379315}\n",
      "Allele 46 model 2\n",
      "Fitting model for allele HLA-B4002 (4358 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 6 sec\n",
      "test set size: 74\n",
      "{'tau': 0.49352182209690837, 'auc': 0.93452380952380965, 'f1': 0.75862068965517249}\n",
      "Allele 46 model 3\n",
      "Fitting model for allele HLA-B4002 (4358 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 6 sec\n",
      "test set size: 74\n",
      "{'tau': 0.4343614774922317, 'auc': 0.92460317460317465, 'f1': 0.61538461538461531}\n",
      "Allele 46 model 4\n",
      "Fitting model for allele HLA-B4002 (4358 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 8 sec\n",
      "test set size: 74\n",
      "{'tau': 0.50130607796594484, 'auc': 0.92063492063492069, 'f1': 0.77419354838709675}\n",
      "Allele 46 model 5\n",
      "Fitting model for allele HLA-B4002 (4358 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 7 sec\n",
      "test set size: 74\n",
      "{'tau': 0.48028858711954647, 'auc': 0.90277777777777779, 'f1': 0.70967741935483875}\n",
      "Allele 46 model 6\n",
      "Fitting model for allele HLA-B4002 (4358 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 9 sec\n",
      "test set size: 74\n",
      "{'tau': 0.49118654533619749, 'auc': 0.92757936507936511, 'f1': 0.66666666666666663}\n",
      "Allele 46 model 7\n",
      "Fitting model for allele HLA-B4002 (4358 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 8 sec\n",
      "test set size: 74\n",
      "{'tau': 0.44370258453507538, 'auc': 0.93650793650793651, 'f1': 0.43478260869565222}\n",
      "Skipping allele HLA-B4801: not in test set\n",
      "Allele 48 model 0\n",
      "Fitting model for allele HLA-B1517 (846 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 9 sec\n",
      "test set size: 582\n",
      "{'tau': 0.40731060557782206, 'auc': 0.91048222178243943, 'f1': 0.66108786610878667}\n",
      "Allele 48 model 1\n",
      "Fitting model for allele HLA-B1517 (846 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 1 sec\n",
      "test set size: 582\n",
      "{'tau': 0.41716448479993173, 'auc': 0.92147679046551001, 'f1': 0.66115702479338845}\n",
      "Allele 48 model 2\n",
      "Fitting model for allele HLA-B1517 (846 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 2 sec\n",
      "test set size: 582\n",
      "{'tau': 0.40289159788543272, 'auc': 0.91169162433757722, 'f1': 0.66960352422907488}\n",
      "Allele 48 model 3\n",
      "Fitting model for allele HLA-B1517 (846 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 1 sec\n",
      "test set size: 582\n",
      "{'tau': 0.38150766410961734, 'auc': 0.90181850166017985, 'f1': 0.6696428571428571}\n",
      "Allele 48 model 4\n",
      "Fitting model for allele HLA-B1517 (846 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 4 sec\n",
      "test set size: 582\n",
      "{'tau': 0.41139099199110118, 'auc': 0.91151571123864805, 'f1': 0.65306122448979598}\n",
      "Allele 48 model 5\n",
      "Fitting model for allele HLA-B1517 (846 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 3 sec\n",
      "test set size: 582\n",
      "{'tau': 0.41040899028168132, 'auc': 0.90999846076038438, 'f1': 0.63900414937759342}\n",
      "Allele 48 model 6\n",
      "Fitting model for allele HLA-B1517 (846 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 5 sec\n",
      "test set size: 582\n",
      "{'tau': 0.41877293587570569, 'auc': 0.91925588759152976, 'f1': 0.67241379310344829}\n",
      "Allele 48 model 7\n",
      "Fitting model for allele HLA-B1517 (846 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 4 sec\n",
      "test set size: 582\n",
      "{'tau': 0.38022090324899821, 'auc': 0.89548563009873117, 'f1': 0.66968325791855199}\n",
      "Skipping allele Mamu-B01: not in test set\n",
      "Allele 50 model 0\n",
      "Fitting model for allele HLA-A8001 (782 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 1 sec\n",
      "test set size: 379\n",
      "{'tau': 0.17527980556617601, 'auc': 0.96658986175115202, 'f1': 0.34482758620689652}\n",
      "Allele 50 model 1\n",
      "Fitting model for allele HLA-A8001 (782 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 1 sec\n",
      "test set size: 379\n",
      "{'tau': 0.19423227843869592, 'auc': 0.98847926267281117, 'f1': 0.56000000000000005}\n",
      "Allele 50 model 2\n",
      "Fitting model for allele HLA-A8001 (782 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 1 sec\n",
      "test set size: 379\n",
      "{'tau': 0.18029304032600385, 'auc': 0.98540706605222739, 'f1': 0.46153846153846156}\n",
      "Allele 50 model 3\n",
      "Fitting model for allele HLA-A8001 (782 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 1 sec\n",
      "test set size: 379\n",
      "{'tau': 0.1716115850102044, 'auc': 0.97119815668202769, 'f1': 0.38095238095238093}\n",
      "Allele 50 model 4\n",
      "Fitting model for allele HLA-A8001 (782 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 4 sec\n",
      "test set size: 379\n",
      "{'tau': 0.17442388743644929, 'auc': 0.9623655913978495, 'f1': 0.35714285714285715}\n",
      "Allele 50 model 5\n",
      "Fitting model for allele HLA-A8001 (782 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 3 sec\n",
      "test set size: 379\n",
      "{'tau': 0.16635380221331181, 'auc': 0.96658986175115202, 'f1': 0.33333333333333331}\n",
      "Allele 50 model 6\n",
      "Fitting model for allele HLA-A8001 (782 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 4 sec\n",
      "test set size: 379\n",
      "{'tau': 0.18053758836306863, 'auc': 0.98425499231950853, 'f1': 0.40000000000000002}\n",
      "Allele 50 model 7\n",
      "Fitting model for allele HLA-A8001 (782 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 4 sec\n",
      "test set size: 379\n",
      "{'tau': 0.17491298351057885, 'auc': 0.9731182795698925, 'f1': 0.59999999999999998}\n",
      "Skipping allele Patr-B0101: not in test set\n",
      "Allele 52 model 0\n",
      "Fitting model for allele HLA-A3201 (1225 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 2 sec\n",
      "test set size: 449\n",
      "{'tau': 0.5128688811588975, 'auc': 0.88470401843583468, 'f1': 0.69969040247678016}\n",
      "Allele 52 model 1\n",
      "Fitting model for allele HLA-A3201 (1225 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 1 sec\n",
      "test set size: 449\n",
      "{'tau': 0.51465305183230581, 'auc': 0.89368188583225305, 'f1': 0.70886075949367078}\n",
      "Allele 52 model 2\n",
      "Fitting model for allele HLA-A3201 (1225 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 2 sec\n",
      "test set size: 449\n",
      "{'tau': 0.48406154118487765, 'auc': 0.9006433338134332, 'f1': 0.77777777777777779}\n",
      "Allele 52 model 3\n",
      "Fitting model for allele HLA-A3201 (1225 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 2 sec\n",
      "test set size: 449\n",
      "{'tau': 0.48394126001588383, 'auc': 0.90045129386912481, 'f1': 0.775244299674267}\n",
      "Allele 52 model 4\n",
      "Fitting model for allele HLA-A3201 (1225 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 4 sec\n",
      "test set size: 449\n",
      "{'tau': 0.51772022164164822, 'auc': 0.89027317682077878, 'f1': 0.72077922077922085}\n",
      "Allele 52 model 5\n",
      "Fitting model for allele HLA-A3201 (1225 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 4 sec\n",
      "test set size: 449\n",
      "{'tau': 0.5402128002434925, 'auc': 0.91065341591050941, 'f1': 0.75641025641025639}\n",
      "Allele 52 model 6\n",
      "Fitting model for allele HLA-A3201 (1225 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 5 sec\n",
      "test set size: 449\n",
      "{'tau': 0.48257807343395387, 'auc': 0.89699457487157319, 'f1': 0.78032786885245897}\n",
      "Allele 52 model 7\n",
      "Fitting model for allele HLA-A3201 (1225 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 4 sec\n",
      "test set size: 449\n",
      "{'tau': 0.44795714362523281, 'auc': 0.89077728167458825, 'f1': 0.75816993464052285}\n",
      "Skipping allele Patr-A0901: not in test set\n",
      "Skipping allele Patr-A0701: not in test set\n",
      "Allele 55 model 0\n",
      "Fitting model for allele HLA-A2501 (519 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 1 sec\n",
      "test set size: 416\n",
      "{'tau': 0.14643032217137397, 'auc': 0.97566909975669103, 'f1': 0.36363636363636359}\n",
      "Allele 55 model 1\n",
      "Fitting model for allele HLA-A2501 (519 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 1 sec\n",
      "test set size: 416\n",
      "{'tau': 0.14568131796589379, 'auc': 0.97274939172749397, 'f1': 0.38095238095238093}\n",
      "Allele 55 model 2\n",
      "Fitting model for allele HLA-A2501 (519 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 1 sec\n",
      "test set size: 416\n",
      "{'tau': 0.1470295255357581, 'auc': 0.9771289537712895, 'f1': 0.59999999999999998}\n",
      "Allele 55 model 3\n",
      "Fitting model for allele HLA-A2501 (519 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 1 sec\n",
      "test set size: 416\n",
      "{'tau': 0.14538171628370172, 'auc': 0.97177615571776155, 'f1': 0.28571428571428575}\n",
      "Allele 55 model 4\n",
      "Fitting model for allele HLA-A2501 (519 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 3 sec\n",
      "test set size: 416\n",
      "{'tau': 0.15152355076863913, 'auc': 0.99221411192214115, 'f1': 0.47058823529411764}\n",
      "Allele 55 model 5\n",
      "Fitting model for allele HLA-A2501 (519 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 3 sec\n",
      "test set size: 416\n",
      "{'tau': 0.15122394908644707, 'auc': 0.99124087591240884, 'f1': 0.50000000000000011}\n",
      "Allele 55 model 6\n",
      "Fitting model for allele HLA-A2501 (519 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 4 sec\n",
      "test set size: 416\n",
      "{'tau': 0.1513737499275431, 'auc': 0.99124087591240884, 'f1': 0.54545454545454541}\n",
      "Allele 55 model 7\n",
      "Fitting model for allele HLA-A2501 (519 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 4 sec\n",
      "test set size: 416\n",
      "{'tau': 0.14942633899329466, 'auc': 0.98491484184914846, 'f1': 0.44444444444444448}\n",
      "Allele 56 model 0\n",
      "Fitting model for allele HLA-B0802 (496 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 1 sec\n",
      "test set size: 509\n",
      "{'tau': 0.36189421270302241, 'auc': 0.97872821905408458, 'f1': 0.2857142857142857}\n",
      "Allele 56 model 1\n",
      "Fitting model for allele HLA-B0802 (496 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 1 sec\n",
      "test set size: 509\n",
      "{'tau': 0.34163325061861227, 'auc': 0.96492419099343751, 'f1': 0.16}\n",
      "Allele 56 model 2\n",
      "Fitting model for allele HLA-B0802 (496 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 1 sec\n",
      "test set size: 509\n",
      "{'tau': 0.35848790164269606, 'auc': 0.98087802670287394, 'f1': 0.0}\n",
      "Allele 56 model 3\n",
      "Fitting model for allele HLA-B0802 (496 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/anaconda2/envs/standard-2.7/lib/python2.7/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained in 1 sec\n",
      "test set size: 509\n",
      "{'tau': 0.35834743520721868, 'auc': 0.96605566870332649, 'f1': 0.0}\n",
      "Allele 56 model 4\n",
      "Fitting model for allele HLA-B0802 (496 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 3 sec\n",
      "test set size: 509\n",
      "{'tau': 0.36294771096910272, 'auc': 0.98110432224485178, 'f1': 0.36363636363636359}\n",
      "Allele 56 model 5\n",
      "Fitting model for allele HLA-B0802 (496 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 3 sec\n",
      "test set size: 509\n",
      "{'tau': 0.36477377463030858, 'auc': 0.97408916044353933, 'f1': 0.0}\n",
      "Allele 56 model 6\n",
      "Fitting model for allele HLA-B0802 (496 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 4 sec\n",
      "test set size: 509\n",
      "{'tau': 0.37608132268623728, 'auc': 0.98087802670287394, 'f1': 0.2857142857142857}\n",
      "Allele 56 model 7\n",
      "Fitting model for allele HLA-B0802 (496 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 4 sec\n",
      "test set size: 509\n",
      "{'tau': 0.34019214842176793, 'auc': 0.97341027381760581, 'f1': 0.0}\n",
      "Skipping allele Mamu-A2201: not in test set\n",
      "Allele 58 model 0\n",
      "Fitting model for allele H-2-KD (1494 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 2 sec\n",
      "test set size: 229\n",
      "{'tau': 0.33672340400344547, 'auc': 0.76522316243570598, 'f1': 0.6133333333333334}\n",
      "Allele 58 model 1\n",
      "Fitting model for allele H-2-KD (1494 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 2 sec\n",
      "test set size: 229\n",
      "{'tau': 0.32369988751950401, 'auc': 0.76480836236933802, 'f1': 0.56944444444444453}\n",
      "Allele 58 model 2\n",
      "Fitting model for allele H-2-KD (1494 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 2 sec\n",
      "test set size: 229\n",
      "{'tau': 0.34812861370535875, 'auc': 0.76688236270117804, 'f1': 0.54545454545454553}\n",
      "Allele 58 model 3\n",
      "Fitting model for allele H-2-KD (1494 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 2 sec\n",
      "test set size: 229\n",
      "{'tau': 0.32608881657868855, 'auc': 0.75410652065704342, 'f1': 0.52800000000000014}\n",
      "Allele 58 model 4\n",
      "Fitting model for allele H-2-KD (1494 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 4 sec\n",
      "test set size: 229\n",
      "{'tau': 0.35028635608139636, 'auc': 0.7768375642940103, 'f1': 0.58992805755395683}\n",
      "Allele 58 model 5\n",
      "Fitting model for allele H-2-KD (1494 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 4 sec\n",
      "test set size: 229\n",
      "{'tau': 0.32185039405432891, 'auc': 0.75062220009955194, 'f1': 0.59420289855072461}\n",
      "Allele 58 model 6\n",
      "Fitting model for allele H-2-KD (1494 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 5 sec\n",
      "test set size: 229\n",
      "{'tau': 0.34966985825967134, 'auc': 0.76265140202422432, 'f1': 0.54545454545454553}\n",
      "Allele 58 model 7\n",
      "Fitting model for allele H-2-KD (1494 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 5 sec\n",
      "test set size: 229\n",
      "{'tau': 0.34704974251733989, 'auc': 0.77061556329849001, 'f1': 0.56060606060606066}\n",
      "Allele 59 model 0\n",
      "Fitting model for allele HLA-B2703 (433 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 1 sec\n",
      "test set size: 441\n",
      "{'tau': 0.018978623564314262, 'auc': nan, 'f1': 0.0}\n",
      "Allele 59 model 1\n",
      "Fitting model for allele HLA-B2703 (433 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 1 sec\n",
      "test set size: 441\n",
      "{'tau': 0.060609152673132646, 'auc': nan, 'f1': 0.0}\n",
      "Allele 59 model 2\n",
      "Fitting model for allele HLA-B2703 (433 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 1 sec\n",
      "test set size: 441\n",
      "{'tau': -0.027855721683106419, 'auc': nan, 'f1': 0.0}\n",
      "Allele 59 model 3\n",
      "Fitting model for allele HLA-B2703 (433 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 1 sec\n",
      "test set size: 441\n",
      "{'tau': 0.0070404571286972265, 'auc': nan, 'f1': 0.0}\n",
      "Allele 59 model 4\n",
      "Fitting model for allele HLA-B2703 (433 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 3 sec\n",
      "test set size: 441\n",
      "{'tau': -0.0085709912871096677, 'auc': nan, 'f1': 0.0}\n",
      "Allele 59 model 5\n",
      "Fitting model for allele HLA-B2703 (433 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 3 sec\n",
      "test set size: 441\n",
      "{'tau': 0.029998469504883836, 'auc': nan, 'f1': 0.0}\n",
      "Allele 59 model 6\n",
      "Fitting model for allele HLA-B2703 (433 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 4 sec\n",
      "test set size: 441\n",
      "{'tau': 0.041324422277135897, 'auc': nan, 'f1': 0.0}\n",
      "Allele 59 model 7\n",
      "Fitting model for allele HLA-B2703 (433 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 4 sec\n",
      "test set size: 441\n",
      "{'tau': 0.0012244273267299525, 'auc': nan, 'f1': 0.0}\n",
      "Allele 60 model 0\n",
      "Fitting model for allele HLA-B1503 (541 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 1 sec\n",
      "test set size: 165\n",
      "{'tau': 0.36137097593536355, 'auc': 0.78833792470156105, 'f1': 0.47706422018348627}\n",
      "Allele 60 model 1\n",
      "Fitting model for allele HLA-B1503 (541 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 1 sec\n",
      "test set size: 165\n",
      "{'tau': 0.3777219250333107, 'auc': 0.82047750229568406, 'f1': 0.47619047619047622}\n",
      "Allele 60 model 2\n",
      "Fitting model for allele HLA-B1503 (541 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 1 sec\n",
      "test set size: 165\n",
      "{'tau': 0.41312397996097605, 'auc': 0.83999081726354452, 'f1': 0.53703703703703698}\n",
      "Allele 60 model 3\n",
      "Fitting model for allele HLA-B1503 (541 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 1 sec\n",
      "test set size: 165\n",
      "{'tau': 0.36512119361837897, 'auc': 0.80211202938475668, 'f1': 0.45378151260504201}\n",
      "Allele 60 model 4\n",
      "Fitting model for allele HLA-B1503 (541 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 3 sec\n",
      "test set size: 165\n",
      "{'tau': 0.39527294378982275, 'auc': 0.80968778696051413, 'f1': 0.50980392156862742}\n",
      "Allele 60 model 5\n",
      "Fitting model for allele HLA-B1503 (541 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 3 sec\n",
      "test set size: 165\n",
      "{'tau': 0.4261747374978696, 'auc': 0.82093663911845727, 'f1': 0.55102040816326536}\n",
      "Allele 60 model 6\n",
      "Fitting model for allele HLA-B1503 (541 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 4 sec\n",
      "test set size: 165\n",
      "{'tau': 0.40787367520475448, 'auc': 0.81841138659320478, 'f1': 0.51428571428571423}\n",
      "Allele 60 model 7\n",
      "Fitting model for allele HLA-B1503 (541 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 13 sec\n",
      "test set size: 165\n",
      "{'tau': 0.34892025322775244, 'auc': 0.81244260789715328, 'f1': 0.4576271186440678}\n",
      "Skipping allele Patr-A0101: not in test set\n",
      "Skipping allele H-2-KK: not in test set\n",
      "Allele 63 model 0\n",
      "Fitting model for allele HLA-B1509 (346 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 1 sec\n",
      "test set size: 466\n",
      "{'tau': 0.29886931000858158, 'auc': 0.85244219995265524, 'f1': 0.48780487804878059}\n",
      "Allele 63 model 1\n",
      "Fitting model for allele HLA-B1509 (346 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 1 sec\n",
      "test set size: 466\n",
      "{'tau': 0.33116141294055246, 'auc': 0.87185354691075512, 'f1': 0.31578947368421056}\n",
      "Allele 63 model 2\n",
      "Fitting model for allele HLA-B1509 (346 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 1 sec\n",
      "test set size: 466\n",
      "{'tau': 0.33030071523160554, 'auc': 0.88069123333070309, 'f1': 0.12903225806451613}\n",
      "Allele 63 model 3\n",
      "Fitting model for allele HLA-B1509 (346 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 1 sec\n",
      "test set size: 466\n",
      "{'tau': 0.27652304726888599, 'auc': 0.81780162550303803, 'f1': 0.0}\n",
      "Allele 63 model 4\n",
      "Fitting model for allele HLA-B1509 (346 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 3 sec\n",
      "test set size: 466\n",
      "{'tau': 0.32411644280435742, 'auc': 0.85204766038033619, 'f1': 0.45000000000000007}\n",
      "Allele 63 model 5\n",
      "Fitting model for allele HLA-B1509 (346 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 3 sec\n",
      "test set size: 466\n",
      "{'tau': 0.29402390068413975, 'auc': 0.86199005760277758, 'f1': 0.2162162162162162}\n",
      "Allele 63 model 6\n",
      "Fitting model for allele HLA-B1509 (346 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 4 sec\n",
      "test set size: 466\n",
      "{'tau': 0.34123476390452362, 'auc': 0.8606486230568926, 'f1': 0.12903225806451613}\n",
      "Allele 63 model 7\n",
      "Fitting model for allele HLA-B1509 (346 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 3 sec\n",
      "test set size: 466\n",
      "{'tau': 0.28793526133566349, 'auc': 0.86112207054367551, 'f1': 0.0}\n",
      "Skipping allele Patr-B2401: not in test set\n",
      "Skipping allele Patr-A0301: not in test set\n",
      "Skipping allele H-2-DD: not in test set\n",
      "Skipping allele H-2-LD: not in test set\n",
      "Skipping allele Patr-A0401: not in test set\n",
      "Allele 69 model 0\n",
      "Fitting model for allele HLA-B0803 (217 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 0 sec\n",
      "test set size: 234\n",
      "{'tau': 0.30276759200323755, 'auc': 0.95259259259259255, 'f1': 0.0}\n",
      "Allele 69 model 1\n",
      "Fitting model for allele HLA-B0803 (217 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 0 sec\n",
      "test set size: 234\n",
      "{'tau': 0.27394282658339114, 'auc': 0.88691358024691347, 'f1': 0.0}\n",
      "Allele 69 model 2\n",
      "Fitting model for allele HLA-B0803 (217 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 1 sec\n",
      "test set size: 234\n",
      "{'tau': 0.30965111807364859, 'auc': 0.95703703703703702, 'f1': 0.0}\n",
      "Allele 69 model 3\n",
      "Fitting model for allele HLA-B0803 (217 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 1 sec\n",
      "test set size: 234\n",
      "{'tau': -0.019252361978181001, 'auc': 0.40790123456790123, 'f1': 0.0}\n",
      "Allele 69 model 4\n",
      "Fitting model for allele HLA-B0803 (217 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 3 sec\n",
      "test set size: 234\n",
      "{'tau': 0.29825027801953025, 'auc': 0.9520987654320987, 'f1': 0.19999999999999998}\n",
      "Allele 69 model 5\n",
      "Fitting model for allele HLA-B0803 (217 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 3 sec\n",
      "test set size: 234\n",
      "{'tau': 0.28943076024181608, 'auc': 0.91012345679012352, 'f1': 0.0}\n",
      "Allele 69 model 6\n",
      "Fitting model for allele HLA-B0803 (217 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 4 sec\n",
      "test set size: 234\n",
      "{'tau': 0.30276759200323755, 'auc': 0.94074074074074077, 'f1': 0.19999999999999998}\n",
      "Allele 69 model 7\n",
      "Fitting model for allele HLA-B0803 (217 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 3 sec\n",
      "test set size: 234\n",
      "{'tau': 0.29975604934743266, 'auc': 0.95160493827160497, 'f1': 0.0}\n",
      "Allele 70 model 0\n",
      "Fitting model for allele HLA-A2603 (205 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 0 sec\n",
      "test set size: 312\n",
      "{'tau': 0.32088462083696517, 'auc': 0.8608353808353808, 'f1': 0.52941176470588236}\n",
      "Allele 70 model 1\n",
      "Fitting model for allele HLA-A2603 (205 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 0 sec\n",
      "test set size: 312\n",
      "{'tau': 0.3156341041002188, 'auc': 0.86358722358722362, 'f1': 0.53731343283582089}\n",
      "Allele 70 model 2\n",
      "Fitting model for allele HLA-A2603 (205 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 1 sec\n",
      "test set size: 312\n",
      "{'tau': 0.30484925891122627, 'auc': 0.8626044226044226, 'f1': 0.5185185185185186}\n",
      "Allele 70 model 3\n",
      "Fitting model for allele HLA-A2603 (205 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 1 sec\n",
      "test set size: 312\n",
      "{'tau': 0.27880953401411929, 'auc': 0.83400491400491406, 'f1': 0.31111111111111112}\n",
      "Allele 70 model 4\n",
      "Fitting model for allele HLA-A2603 (205 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 3 sec\n",
      "test set size: 312\n",
      "{'tau': 0.31400218673609492, 'auc': 0.86417690417690418, 'f1': 0.52459016393442626}\n",
      "Allele 70 model 5\n",
      "Fitting model for allele HLA-A2603 (205 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 3 sec\n",
      "test set size: 312\n",
      "{'tau': 0.31932365640171623, 'auc': 0.87714987714987724, 'f1': 0.57627118644067798}\n",
      "Allele 70 model 6\n",
      "Fitting model for allele HLA-A2603 (205 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 4 sec\n",
      "test set size: 312\n",
      "{'tau': 0.32996659573295883, 'auc': 0.88697788697788704, 'f1': 0.54545454545454553}\n",
      "Allele 70 model 7\n",
      "Fitting model for allele HLA-A2603 (205 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 3 sec\n",
      "test set size: 312\n",
      "{'tau': 0.31045454029234737, 'auc': 0.85130221130221129, 'f1': 0.34042553191489361}\n",
      "Allele 71 model 0\n",
      "Fitting model for allele HLA-A2602 (202 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 0 sec\n",
      "test set size: 413\n",
      "{'tau': 0.46962671355564872, 'auc': 0.90272148233931671, 'f1': 0.71361502347417827}\n",
      "Allele 71 model 1\n",
      "Fitting model for allele HLA-A2602 (202 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 0 sec\n",
      "test set size: 413\n",
      "{'tau': 0.49337623707746675, 'auc': 0.92626906002702181, 'f1': 0.74641148325358853}\n",
      "Allele 71 model 2\n",
      "Fitting model for allele HLA-A2602 (202 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 1 sec\n",
      "test set size: 413\n",
      "{'tau': 0.51849649846879997, 'auc': 0.93090137039181631, 'f1': 0.78606965174129362}\n",
      "Allele 71 model 3\n",
      "Fitting model for allele HLA-A2602 (202 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 1 sec\n",
      "test set size: 413\n",
      "{'tau': 0.49543234388173957, 'auc': 0.9206716850028952, 'f1': 0.73000000000000009}\n",
      "Allele 71 model 4\n",
      "Fitting model for allele HLA-A2602 (202 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 3 sec\n",
      "test set size: 413\n",
      "{'tau': 0.49492576684300565, 'auc': 0.91819468571060925, 'f1': 0.71171171171171177}\n",
      "Allele 71 model 5\n",
      "Fitting model for allele HLA-A2602 (202 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 3 sec\n",
      "test set size: 413\n",
      "{'tau': 0.4790728853955688, 'auc': 0.91301550537219323, 'f1': 0.7562189054726367}\n",
      "Allele 71 model 6\n",
      "Fitting model for allele HLA-A2602 (202 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 4 sec\n",
      "test set size: 413\n",
      "{'tau': 0.51277515779604077, 'auc': 0.92675159235668791, 'f1': 0.77777777777777779}\n",
      "Allele 71 model 7\n",
      "Fitting model for allele HLA-A2602 (202 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 3 sec\n",
      "test set size: 413\n",
      "{'tau': 0.51953945119560507, 'auc': 0.92755581290613143, 'f1': 0.77358490566037741}\n",
      "Skipping allele Patr-B1301: not in test set\n",
      "Skipping allele HLA-B1502: not in test set\n",
      "Allele 74 model 0\n",
      "Fitting model for allele HLA-B3801 (136 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 0 sec\n",
      "test set size: 351\n",
      "{'tau': 0.50035869922381693, 'auc': 0.90121580547112468, 'f1': 0.0}\n",
      "Allele 74 model 1\n",
      "Fitting model for allele HLA-B3801 (136 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 0 sec\n",
      "test set size: 351\n",
      "{'tau': 0.51573216133172728, 'auc': 0.9091185410334347, 'f1': 0.0}\n",
      "Allele 74 model 2\n",
      "Fitting model for allele HLA-B3801 (136 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 1 sec\n",
      "test set size: 351\n",
      "{'tau': 0.52452704812114959, 'auc': 0.921952043228639, 'f1': 0.0}\n",
      "Allele 74 model 3\n",
      "Fitting model for allele HLA-B3801 (136 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 1 sec\n",
      "test set size: 351\n",
      "{'tau': 0.27418939054703151, 'auc': 0.7257683215130023, 'f1': 0.0}\n",
      "Allele 74 model 4\n",
      "Fitting model for allele HLA-B3801 (136 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 3 sec\n",
      "test set size: 351\n",
      "{'tau': 0.52508992087567263, 'auc': 0.93468422830124953, 'f1': 0.0}\n",
      "Allele 74 model 5\n",
      "Fitting model for allele HLA-B3801 (136 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 3 sec\n",
      "test set size: 351\n",
      "{'tau': 0.52375309808368042, 'auc': 0.93590003377237418, 'f1': 0.0}\n",
      "Allele 74 model 6\n",
      "Fitting model for allele HLA-B3801 (136 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 4 sec\n",
      "test set size: 351\n",
      "{'tau': 0.50707799273093568, 'auc': 0.91962174940898334, 'f1': 0.0}\n",
      "Allele 74 model 7\n",
      "Fitting model for allele HLA-B3801 (136 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 3 sec\n",
      "test set size: 351\n",
      "{'tau': 0.51773739551971554, 'auc': 0.92272880783519085, 'f1': 0.0}\n",
      "Skipping allele HLA-A0250: not in test set\n",
      "Skipping allele HLA-B7301: not in test set\n",
      "Skipping allele HLA-A11: not in test set\n",
      "Skipping allele HLA-A0207: not in test set\n",
      "Skipping allele HLA-B7: not in test set\n",
      "Skipping allele HLA-A0205: not in test set\n",
      "Skipping allele Mamu-A07: not in test set\n",
      "Skipping allele Mamu-A2601: not in test set\n",
      "Skipping allele HLA-A2: not in test set\n",
      "Skipping allele HLA-B5802: not in test set\n",
      "Skipping allele HLA-A0210: not in test set\n",
      "Skipping allele Gogo-B0101: not in test set\n",
      "Skipping allele ELA-A1: not in test set\n",
      "Skipping allele HLA-A0302: not in test set\n"
     ]
    }
   ],
   "source": [
    "# train and test models, adding columns to validation_df_with_mhcflurry\n",
    "validation_df_with_mhcflurry = validation_df.copy()\n",
    "def make_and_fit_model(allele, original_params):\n",
    "    params = dict(original_params)\n",
    "    impute = params[\"impute\"]\n",
    "    del params[\"impute\"]\n",
    "    model = mhcflurry.Class1BindingPredictor.from_hyperparameters(max_ic50=max_ic50, **params)\n",
    "    print(\"Fitting model for allele %s (%d + %d): %s\" % (\n",
    "            allele, len(all_train_data[allele].Y), len(imputed_train_data[allele].Y), str(original_params)))\n",
    "    t = -time.time()\n",
    "    model.fit(all_train_data[allele].X_index,\n",
    "              all_train_data[allele].Y,\n",
    "              sample_weights=all_train_data[allele].weights,\n",
    "              X_pretrain=imputed_train_data[allele].X_index if impute else None,\n",
    "              Y_pretrain=imputed_train_data[allele].Y if impute else None,\n",
    "              sample_weights_pretrain=imputed_train_data[allele].weights if impute else None,\n",
    "              verbose=False,\n",
    "              batch_size=128,\n",
    "              n_training_epochs=250)\n",
    "    t += time.time()\n",
    "    print(\"Trained in %d sec\" % t)\n",
    "    return model\n",
    "\n",
    "models_and_scores = {}\n",
    "for (i, allele) in enumerate(alleles[2:]):\n",
    "    if allele not in validation_df_with_mhcflurry.allele.unique():\n",
    "        print(\"Skipping allele %s: not in test set\" % allele)\n",
    "        continue\n",
    "    values_for_allele = []\n",
    "    for (j, params) in enumerate(models_params_list):\n",
    "        print(\"Allele %d model %d\" % (i, j))\n",
    "        model = make_and_fit_model(allele, params)\n",
    "        predictions = model.predict_peptides_ic50(\n",
    "            list(validation_df_with_mhcflurry.ix[validation_df_with_mhcflurry.allele == allele].peptide))\n",
    "        print(\"test set size: %d\" % len(predictions))\n",
    "        validation_df_with_mhcflurry.loc[(validation_df_with_mhcflurry.allele == allele),\n",
    "                                         (\"mhcflurry %d\" % j)] = predictions\n",
    "        scores = make_scores(validation_df_with_mhcflurry.ix[validation_df.allele == allele].meas,\n",
    "                            predictions)\n",
    "        print(scores)\n",
    "        values_for_allele.append((params, model, scores))\n",
    "        \n",
    "    models_and_scores[allele] = values_for_allele\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>allele</th>\n",
       "      <th>peptide</th>\n",
       "      <th>length</th>\n",
       "      <th>meas</th>\n",
       "      <th>netmhc</th>\n",
       "      <th>netmhcpan</th>\n",
       "      <th>smmpmbec_cpp</th>\n",
       "      <th>mhcflurry 0</th>\n",
       "      <th>mhcflurry 1</th>\n",
       "      <th>mhcflurry 2</th>\n",
       "      <th>mhcflurry 3</th>\n",
       "      <th>mhcflurry 4</th>\n",
       "      <th>mhcflurry 5</th>\n",
       "      <th>mhcflurry 6</th>\n",
       "      <th>mhcflurry 7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAACNVATA</td>\n",
       "      <td>9</td>\n",
       "      <td>657.657837</td>\n",
       "      <td>154.881662</td>\n",
       "      <td>711.213514</td>\n",
       "      <td>438.530698</td>\n",
       "      <td>112.340509</td>\n",
       "      <td>192.754410</td>\n",
       "      <td>379.761317</td>\n",
       "      <td>1212.385318</td>\n",
       "      <td>843.942591</td>\n",
       "      <td>341.705000</td>\n",
       "      <td>315.451272</td>\n",
       "      <td>769.001144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAFEFVYV</td>\n",
       "      <td>8</td>\n",
       "      <td>30831.879502</td>\n",
       "      <td>6456.542290</td>\n",
       "      <td>785.235635</td>\n",
       "      <td>10351.421667</td>\n",
       "      <td>10256.714551</td>\n",
       "      <td>7634.586072</td>\n",
       "      <td>11346.142026</td>\n",
       "      <td>15914.774702</td>\n",
       "      <td>12353.946556</td>\n",
       "      <td>11654.561717</td>\n",
       "      <td>10944.064962</td>\n",
       "      <td>13952.278587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAFVNDYSL</td>\n",
       "      <td>9</td>\n",
       "      <td>77.446180</td>\n",
       "      <td>17.458222</td>\n",
       "      <td>7.516229</td>\n",
       "      <td>28.054336</td>\n",
       "      <td>18.079150</td>\n",
       "      <td>14.860045</td>\n",
       "      <td>34.326764</td>\n",
       "      <td>193.255404</td>\n",
       "      <td>183.575076</td>\n",
       "      <td>10.568342</td>\n",
       "      <td>41.402359</td>\n",
       "      <td>138.784560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAIANQAAV</td>\n",
       "      <td>9</td>\n",
       "      <td>1.999862</td>\n",
       "      <td>9.638290</td>\n",
       "      <td>9.749896</td>\n",
       "      <td>25.703958</td>\n",
       "      <td>11.153717</td>\n",
       "      <td>8.599590</td>\n",
       "      <td>20.528593</td>\n",
       "      <td>79.818190</td>\n",
       "      <td>5.110946</td>\n",
       "      <td>9.293367</td>\n",
       "      <td>24.251661</td>\n",
       "      <td>61.158162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAIANQAVV</td>\n",
       "      <td>9</td>\n",
       "      <td>1.517050</td>\n",
       "      <td>8.550667</td>\n",
       "      <td>8.336812</td>\n",
       "      <td>28.773984</td>\n",
       "      <td>3.084469</td>\n",
       "      <td>5.068488</td>\n",
       "      <td>16.218493</td>\n",
       "      <td>67.657268</td>\n",
       "      <td>4.023290</td>\n",
       "      <td>5.418416</td>\n",
       "      <td>19.621984</td>\n",
       "      <td>61.784682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAIENYVRF</td>\n",
       "      <td>9</td>\n",
       "      <td>37.844258</td>\n",
       "      <td>252.348077</td>\n",
       "      <td>114.815362</td>\n",
       "      <td>187.068214</td>\n",
       "      <td>179.024829</td>\n",
       "      <td>119.080622</td>\n",
       "      <td>249.575544</td>\n",
       "      <td>416.183183</td>\n",
       "      <td>111.430051</td>\n",
       "      <td>220.841440</td>\n",
       "      <td>271.067287</td>\n",
       "      <td>300.608053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAINFITTM</td>\n",
       "      <td>9</td>\n",
       "      <td>3.155005</td>\n",
       "      <td>199.986187</td>\n",
       "      <td>389.045145</td>\n",
       "      <td>200.909281</td>\n",
       "      <td>6.328530</td>\n",
       "      <td>44.094703</td>\n",
       "      <td>45.987591</td>\n",
       "      <td>266.413599</td>\n",
       "      <td>26.621323</td>\n",
       "      <td>61.821550</td>\n",
       "      <td>46.339128</td>\n",
       "      <td>158.900606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAIPAPPPI</td>\n",
       "      <td>9</td>\n",
       "      <td>3243.396173</td>\n",
       "      <td>1059.253725</td>\n",
       "      <td>493.173804</td>\n",
       "      <td>295.120923</td>\n",
       "      <td>110.038647</td>\n",
       "      <td>144.313801</td>\n",
       "      <td>213.078557</td>\n",
       "      <td>194.911970</td>\n",
       "      <td>534.798653</td>\n",
       "      <td>838.954495</td>\n",
       "      <td>242.103225</td>\n",
       "      <td>215.492933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAKLNRPPL</td>\n",
       "      <td>9</td>\n",
       "      <td>654.636174</td>\n",
       "      <td>66.374307</td>\n",
       "      <td>77.268059</td>\n",
       "      <td>38.459178</td>\n",
       "      <td>99.976029</td>\n",
       "      <td>216.748867</td>\n",
       "      <td>94.699289</td>\n",
       "      <td>231.042685</td>\n",
       "      <td>470.524253</td>\n",
       "      <td>62.406672</td>\n",
       "      <td>93.215675</td>\n",
       "      <td>159.333748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AALDMVDAL</td>\n",
       "      <td>9</td>\n",
       "      <td>229.614865</td>\n",
       "      <td>547.015963</td>\n",
       "      <td>597.035287</td>\n",
       "      <td>225.423921</td>\n",
       "      <td>5587.566650</td>\n",
       "      <td>1967.343743</td>\n",
       "      <td>588.703181</td>\n",
       "      <td>747.603811</td>\n",
       "      <td>3545.913989</td>\n",
       "      <td>1120.155821</td>\n",
       "      <td>599.119879</td>\n",
       "      <td>333.132501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AALQHLRSI</td>\n",
       "      <td>9</td>\n",
       "      <td>905.732601</td>\n",
       "      <td>1686.553025</td>\n",
       "      <td>2032.357011</td>\n",
       "      <td>698.232404</td>\n",
       "      <td>786.309282</td>\n",
       "      <td>1237.196137</td>\n",
       "      <td>1716.506042</td>\n",
       "      <td>2989.325384</td>\n",
       "      <td>5393.792863</td>\n",
       "      <td>2917.331150</td>\n",
       "      <td>1869.227525</td>\n",
       "      <td>1617.893972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AALVRLTAL</td>\n",
       "      <td>9</td>\n",
       "      <td>1106.623784</td>\n",
       "      <td>435.511874</td>\n",
       "      <td>214.783047</td>\n",
       "      <td>378.442585</td>\n",
       "      <td>2232.905078</td>\n",
       "      <td>1918.012270</td>\n",
       "      <td>1258.549640</td>\n",
       "      <td>3932.638061</td>\n",
       "      <td>4785.061547</td>\n",
       "      <td>1945.389506</td>\n",
       "      <td>1521.067404</td>\n",
       "      <td>2110.680997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAMVPTGSL</td>\n",
       "      <td>9</td>\n",
       "      <td>1836.538343</td>\n",
       "      <td>4055.085354</td>\n",
       "      <td>5176.068320</td>\n",
       "      <td>1545.254440</td>\n",
       "      <td>4440.827219</td>\n",
       "      <td>2844.312485</td>\n",
       "      <td>5569.762393</td>\n",
       "      <td>4860.163064</td>\n",
       "      <td>13034.467166</td>\n",
       "      <td>6236.581508</td>\n",
       "      <td>4797.353134</td>\n",
       "      <td>5295.100788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AANSPWAPV</td>\n",
       "      <td>9</td>\n",
       "      <td>4325.138310</td>\n",
       "      <td>903.649474</td>\n",
       "      <td>1023.292992</td>\n",
       "      <td>557.185749</td>\n",
       "      <td>249.515757</td>\n",
       "      <td>805.309236</td>\n",
       "      <td>1258.031102</td>\n",
       "      <td>2064.657653</td>\n",
       "      <td>1251.380944</td>\n",
       "      <td>1261.157328</td>\n",
       "      <td>1094.852842</td>\n",
       "      <td>3271.749992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAPSGAAPL</td>\n",
       "      <td>9</td>\n",
       "      <td>2.844461</td>\n",
       "      <td>97.948999</td>\n",
       "      <td>501.187234</td>\n",
       "      <td>822.242650</td>\n",
       "      <td>212.468342</td>\n",
       "      <td>99.927233</td>\n",
       "      <td>879.959202</td>\n",
       "      <td>1459.675643</td>\n",
       "      <td>38.257331</td>\n",
       "      <td>260.914640</td>\n",
       "      <td>1059.657399</td>\n",
       "      <td>1045.890658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAPVSEPTV</td>\n",
       "      <td>9</td>\n",
       "      <td>106.905488</td>\n",
       "      <td>2654.605562</td>\n",
       "      <td>1918.668741</td>\n",
       "      <td>1870.682140</td>\n",
       "      <td>1959.447026</td>\n",
       "      <td>2284.064516</td>\n",
       "      <td>2303.540785</td>\n",
       "      <td>1854.533502</td>\n",
       "      <td>1144.076316</td>\n",
       "      <td>3285.933345</td>\n",
       "      <td>2461.964928</td>\n",
       "      <td>1770.746601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAVANGFAA</td>\n",
       "      <td>9</td>\n",
       "      <td>83.368118</td>\n",
       "      <td>205.116218</td>\n",
       "      <td>228.559880</td>\n",
       "      <td>200.447203</td>\n",
       "      <td>274.574166</td>\n",
       "      <td>432.904800</td>\n",
       "      <td>299.986004</td>\n",
       "      <td>729.847405</td>\n",
       "      <td>19.308634</td>\n",
       "      <td>1013.697894</td>\n",
       "      <td>276.058692</td>\n",
       "      <td>339.793381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAVLLGAPV</td>\n",
       "      <td>9</td>\n",
       "      <td>373.250158</td>\n",
       "      <td>320.626932</td>\n",
       "      <td>623.734835</td>\n",
       "      <td>286.417797</td>\n",
       "      <td>1681.570081</td>\n",
       "      <td>397.424315</td>\n",
       "      <td>666.733362</td>\n",
       "      <td>1515.685156</td>\n",
       "      <td>22.470471</td>\n",
       "      <td>250.590079</td>\n",
       "      <td>882.174075</td>\n",
       "      <td>912.799024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAVQLLFPA</td>\n",
       "      <td>9</td>\n",
       "      <td>347.536161</td>\n",
       "      <td>2494.594727</td>\n",
       "      <td>5152.286446</td>\n",
       "      <td>679.203633</td>\n",
       "      <td>2200.627852</td>\n",
       "      <td>1134.968527</td>\n",
       "      <td>1457.964316</td>\n",
       "      <td>3828.918429</td>\n",
       "      <td>2296.786145</td>\n",
       "      <td>2195.843245</td>\n",
       "      <td>1554.468224</td>\n",
       "      <td>1831.128909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAVTAGVAL</td>\n",
       "      <td>9</td>\n",
       "      <td>229.614865</td>\n",
       "      <td>1552.387010</td>\n",
       "      <td>2172.701179</td>\n",
       "      <td>1927.524913</td>\n",
       "      <td>361.893576</td>\n",
       "      <td>2808.835091</td>\n",
       "      <td>2527.631535</td>\n",
       "      <td>2888.031397</td>\n",
       "      <td>352.205107</td>\n",
       "      <td>2417.652937</td>\n",
       "      <td>2944.234479</td>\n",
       "      <td>1956.858856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAVVNRSLV</td>\n",
       "      <td>9</td>\n",
       "      <td>8.609938</td>\n",
       "      <td>26.302680</td>\n",
       "      <td>29.580125</td>\n",
       "      <td>63.826349</td>\n",
       "      <td>6.415570</td>\n",
       "      <td>18.256138</td>\n",
       "      <td>71.040804</td>\n",
       "      <td>278.989425</td>\n",
       "      <td>26.672586</td>\n",
       "      <td>19.675699</td>\n",
       "      <td>80.588456</td>\n",
       "      <td>192.768209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAYVPADAV</td>\n",
       "      <td>9</td>\n",
       "      <td>331.894458</td>\n",
       "      <td>1757.923614</td>\n",
       "      <td>3206.269325</td>\n",
       "      <td>595.662144</td>\n",
       "      <td>1064.770463</td>\n",
       "      <td>2364.955905</td>\n",
       "      <td>1892.732704</td>\n",
       "      <td>3685.673402</td>\n",
       "      <td>8491.497336</td>\n",
       "      <td>1014.081387</td>\n",
       "      <td>1719.493084</td>\n",
       "      <td>4271.351313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>ACMVNLERL</td>\n",
       "      <td>9</td>\n",
       "      <td>737.904230</td>\n",
       "      <td>239.331576</td>\n",
       "      <td>679.203633</td>\n",
       "      <td>444.631267</td>\n",
       "      <td>3458.283446</td>\n",
       "      <td>335.970204</td>\n",
       "      <td>586.719201</td>\n",
       "      <td>834.805741</td>\n",
       "      <td>1703.538177</td>\n",
       "      <td>1592.685251</td>\n",
       "      <td>674.838162</td>\n",
       "      <td>755.982211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>ADLVNHPPV</td>\n",
       "      <td>9</td>\n",
       "      <td>558.470195</td>\n",
       "      <td>1694.337800</td>\n",
       "      <td>1857.804455</td>\n",
       "      <td>325.087297</td>\n",
       "      <td>6789.133692</td>\n",
       "      <td>951.892616</td>\n",
       "      <td>1206.634603</td>\n",
       "      <td>2467.089933</td>\n",
       "      <td>2362.779711</td>\n",
       "      <td>2703.888338</td>\n",
       "      <td>1131.856153</td>\n",
       "      <td>4096.967360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AEMEEALKG</td>\n",
       "      <td>9</td>\n",
       "      <td>78523.563461</td>\n",
       "      <td>41304.750199</td>\n",
       "      <td>46989.410861</td>\n",
       "      <td>440554.863507</td>\n",
       "      <td>48444.974950</td>\n",
       "      <td>45911.476692</td>\n",
       "      <td>41310.640019</td>\n",
       "      <td>34020.082131</td>\n",
       "      <td>48370.188031</td>\n",
       "      <td>42991.571329</td>\n",
       "      <td>40211.041193</td>\n",
       "      <td>34802.068684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AFFAFRYV</td>\n",
       "      <td>8</td>\n",
       "      <td>87902.251683</td>\n",
       "      <td>18967.059212</td>\n",
       "      <td>12882.495517</td>\n",
       "      <td>7816.278046</td>\n",
       "      <td>22510.621519</td>\n",
       "      <td>14131.408515</td>\n",
       "      <td>24028.326064</td>\n",
       "      <td>23476.493271</td>\n",
       "      <td>17522.598466</td>\n",
       "      <td>11914.024459</td>\n",
       "      <td>23053.294332</td>\n",
       "      <td>20401.753074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AGLDNKFYL</td>\n",
       "      <td>9</td>\n",
       "      <td>659.173895</td>\n",
       "      <td>357.272838</td>\n",
       "      <td>202.768272</td>\n",
       "      <td>260.015956</td>\n",
       "      <td>273.754382</td>\n",
       "      <td>457.337748</td>\n",
       "      <td>230.741005</td>\n",
       "      <td>413.527284</td>\n",
       "      <td>421.345136</td>\n",
       "      <td>779.788064</td>\n",
       "      <td>243.028525</td>\n",
       "      <td>299.565612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AGLLFVLL</td>\n",
       "      <td>8</td>\n",
       "      <td>87902.251683</td>\n",
       "      <td>14996.848355</td>\n",
       "      <td>9885.530947</td>\n",
       "      <td>26242.185434</td>\n",
       "      <td>22081.121649</td>\n",
       "      <td>18089.342700</td>\n",
       "      <td>17013.935532</td>\n",
       "      <td>18772.599101</td>\n",
       "      <td>23074.003835</td>\n",
       "      <td>12846.305291</td>\n",
       "      <td>16831.450351</td>\n",
       "      <td>16307.715541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AGLQVINL</td>\n",
       "      <td>8</td>\n",
       "      <td>59429.215862</td>\n",
       "      <td>25061.092530</td>\n",
       "      <td>12560.299637</td>\n",
       "      <td>35399.734108</td>\n",
       "      <td>37796.957034</td>\n",
       "      <td>29464.968194</td>\n",
       "      <td>26854.865987</td>\n",
       "      <td>23782.866417</td>\n",
       "      <td>35764.525074</td>\n",
       "      <td>26667.236430</td>\n",
       "      <td>26938.288598</td>\n",
       "      <td>23404.234219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AGLVSFNFL</td>\n",
       "      <td>9</td>\n",
       "      <td>210.862815</td>\n",
       "      <td>620.869034</td>\n",
       "      <td>2285.598803</td>\n",
       "      <td>1811.340093</td>\n",
       "      <td>1351.673792</td>\n",
       "      <td>1005.323581</td>\n",
       "      <td>1924.966573</td>\n",
       "      <td>3488.180577</td>\n",
       "      <td>3976.828493</td>\n",
       "      <td>1208.701244</td>\n",
       "      <td>2324.415009</td>\n",
       "      <td>3350.396781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27650</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YNTVCVIW</td>\n",
       "      <td>8</td>\n",
       "      <td>909.913273</td>\n",
       "      <td>734.513868</td>\n",
       "      <td>10023.052381</td>\n",
       "      <td>48.865236</td>\n",
       "      <td>787.330338</td>\n",
       "      <td>2958.936745</td>\n",
       "      <td>1835.522241</td>\n",
       "      <td>863.192009</td>\n",
       "      <td>1110.536882</td>\n",
       "      <td>1172.812917</td>\n",
       "      <td>1596.437344</td>\n",
       "      <td>1260.557007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27651</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YRHDGGNVL</td>\n",
       "      <td>9</td>\n",
       "      <td>78342.964277</td>\n",
       "      <td>1355.189412</td>\n",
       "      <td>22233.098907</td>\n",
       "      <td>1409.288798</td>\n",
       "      <td>43.089568</td>\n",
       "      <td>339.667620</td>\n",
       "      <td>192.155799</td>\n",
       "      <td>772.560006</td>\n",
       "      <td>128.266535</td>\n",
       "      <td>874.426580</td>\n",
       "      <td>321.426370</td>\n",
       "      <td>389.192968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27652</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YSEGQYMNTP</td>\n",
       "      <td>10</td>\n",
       "      <td>7362.070975</td>\n",
       "      <td>2958.012467</td>\n",
       "      <td>8709.635900</td>\n",
       "      <td>4477.133042</td>\n",
       "      <td>2406.176442</td>\n",
       "      <td>1591.222509</td>\n",
       "      <td>1466.886453</td>\n",
       "      <td>1205.418451</td>\n",
       "      <td>2075.214415</td>\n",
       "      <td>1276.099359</td>\n",
       "      <td>581.970672</td>\n",
       "      <td>1350.074906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27653</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YSEVALNVTES</td>\n",
       "      <td>11</td>\n",
       "      <td>7709.034691</td>\n",
       "      <td>1682.674061</td>\n",
       "      <td>5105.050000</td>\n",
       "      <td>51.522864</td>\n",
       "      <td>331.873207</td>\n",
       "      <td>74.191562</td>\n",
       "      <td>155.536028</td>\n",
       "      <td>66.558974</td>\n",
       "      <td>220.157526</td>\n",
       "      <td>103.419004</td>\n",
       "      <td>99.271320</td>\n",
       "      <td>94.265753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27654</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YSLVFVILM</td>\n",
       "      <td>9</td>\n",
       "      <td>1256.029964</td>\n",
       "      <td>17.498467</td>\n",
       "      <td>49.431069</td>\n",
       "      <td>6.324119</td>\n",
       "      <td>16.941694</td>\n",
       "      <td>6.452229</td>\n",
       "      <td>17.484177</td>\n",
       "      <td>27.971011</td>\n",
       "      <td>8.318621</td>\n",
       "      <td>7.130104</td>\n",
       "      <td>13.458614</td>\n",
       "      <td>33.100315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27655</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YSQIGAGVY</td>\n",
       "      <td>9</td>\n",
       "      <td>1.999862</td>\n",
       "      <td>17.864876</td>\n",
       "      <td>15.346170</td>\n",
       "      <td>16.292960</td>\n",
       "      <td>31.859938</td>\n",
       "      <td>32.414771</td>\n",
       "      <td>70.732408</td>\n",
       "      <td>78.354937</td>\n",
       "      <td>21.518184</td>\n",
       "      <td>87.288525</td>\n",
       "      <td>47.211193</td>\n",
       "      <td>81.384785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27656</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YSRDLICEQS</td>\n",
       "      <td>10</td>\n",
       "      <td>1059.253725</td>\n",
       "      <td>1297.179271</td>\n",
       "      <td>3863.669771</td>\n",
       "      <td>220.292646</td>\n",
       "      <td>146.469163</td>\n",
       "      <td>24.823854</td>\n",
       "      <td>95.090068</td>\n",
       "      <td>152.750013</td>\n",
       "      <td>167.003616</td>\n",
       "      <td>30.827187</td>\n",
       "      <td>66.831597</td>\n",
       "      <td>114.730647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27657</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YSYKAFIKY</td>\n",
       "      <td>9</td>\n",
       "      <td>2.317395</td>\n",
       "      <td>4.285485</td>\n",
       "      <td>3.090295</td>\n",
       "      <td>1.778279</td>\n",
       "      <td>8.399109</td>\n",
       "      <td>6.322224</td>\n",
       "      <td>10.207831</td>\n",
       "      <td>29.189567</td>\n",
       "      <td>13.441404</td>\n",
       "      <td>4.085901</td>\n",
       "      <td>9.953399</td>\n",
       "      <td>27.190004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27658</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YSYKAFIKYP</td>\n",
       "      <td>10</td>\n",
       "      <td>4830.588020</td>\n",
       "      <td>1199.499303</td>\n",
       "      <td>1753.880502</td>\n",
       "      <td>3111.716337</td>\n",
       "      <td>658.359300</td>\n",
       "      <td>680.468133</td>\n",
       "      <td>751.426515</td>\n",
       "      <td>876.350086</td>\n",
       "      <td>1288.018623</td>\n",
       "      <td>376.015131</td>\n",
       "      <td>334.145311</td>\n",
       "      <td>1087.782959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27659</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YSYKAFIKYPE</td>\n",
       "      <td>11</td>\n",
       "      <td>4897.788194</td>\n",
       "      <td>671.428853</td>\n",
       "      <td>2666.858665</td>\n",
       "      <td>42.559841</td>\n",
       "      <td>911.273799</td>\n",
       "      <td>501.668843</td>\n",
       "      <td>472.900430</td>\n",
       "      <td>581.714770</td>\n",
       "      <td>899.006392</td>\n",
       "      <td>1086.389895</td>\n",
       "      <td>577.809352</td>\n",
       "      <td>613.660403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27660</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTAFTLPSV</td>\n",
       "      <td>9</td>\n",
       "      <td>101.624869</td>\n",
       "      <td>32.210688</td>\n",
       "      <td>52.844525</td>\n",
       "      <td>91.833260</td>\n",
       "      <td>6.136728</td>\n",
       "      <td>7.438836</td>\n",
       "      <td>17.473784</td>\n",
       "      <td>48.313531</td>\n",
       "      <td>5.619066</td>\n",
       "      <td>6.024876</td>\n",
       "      <td>14.479473</td>\n",
       "      <td>58.326735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27661</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTAFTLPSVN</td>\n",
       "      <td>10</td>\n",
       "      <td>2529.297996</td>\n",
       "      <td>236.591970</td>\n",
       "      <td>1406.047524</td>\n",
       "      <td>14.487719</td>\n",
       "      <td>11.266901</td>\n",
       "      <td>21.995981</td>\n",
       "      <td>26.502780</td>\n",
       "      <td>152.782343</td>\n",
       "      <td>17.133504</td>\n",
       "      <td>33.079500</td>\n",
       "      <td>23.752858</td>\n",
       "      <td>74.715754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27662</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTDGSCNKQS</td>\n",
       "      <td>10</td>\n",
       "      <td>11091.748153</td>\n",
       "      <td>4345.102242</td>\n",
       "      <td>11246.049740</td>\n",
       "      <td>588.843655</td>\n",
       "      <td>258.952779</td>\n",
       "      <td>137.857837</td>\n",
       "      <td>456.796814</td>\n",
       "      <td>329.864147</td>\n",
       "      <td>561.113888</td>\n",
       "      <td>242.290124</td>\n",
       "      <td>252.656751</td>\n",
       "      <td>340.538442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27663</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTGDFDSVI</td>\n",
       "      <td>9</td>\n",
       "      <td>215.278173</td>\n",
       "      <td>354.813389</td>\n",
       "      <td>42.461956</td>\n",
       "      <td>40.550854</td>\n",
       "      <td>237.098327</td>\n",
       "      <td>183.208082</td>\n",
       "      <td>220.892433</td>\n",
       "      <td>512.009163</td>\n",
       "      <td>38.372849</td>\n",
       "      <td>609.332718</td>\n",
       "      <td>236.980465</td>\n",
       "      <td>439.462170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27664</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTPKVVGG</td>\n",
       "      <td>8</td>\n",
       "      <td>70145.529842</td>\n",
       "      <td>17947.336268</td>\n",
       "      <td>13995.873226</td>\n",
       "      <td>3140.508694</td>\n",
       "      <td>17250.549088</td>\n",
       "      <td>13199.088223</td>\n",
       "      <td>7488.080326</td>\n",
       "      <td>3503.806165</td>\n",
       "      <td>18239.227402</td>\n",
       "      <td>12760.786019</td>\n",
       "      <td>7315.381080</td>\n",
       "      <td>2653.227223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27665</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTPKVVGGIGG</td>\n",
       "      <td>11</td>\n",
       "      <td>70145.529842</td>\n",
       "      <td>9418.895965</td>\n",
       "      <td>10046.157903</td>\n",
       "      <td>387.257645</td>\n",
       "      <td>7022.992659</td>\n",
       "      <td>5341.508901</td>\n",
       "      <td>3244.550478</td>\n",
       "      <td>2835.346490</td>\n",
       "      <td>5878.557994</td>\n",
       "      <td>4794.104390</td>\n",
       "      <td>3429.932093</td>\n",
       "      <td>1987.332832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27666</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTSGPGIR</td>\n",
       "      <td>8</td>\n",
       "      <td>16865.530254</td>\n",
       "      <td>619.441075</td>\n",
       "      <td>2349.632821</td>\n",
       "      <td>80.723503</td>\n",
       "      <td>941.543101</td>\n",
       "      <td>745.499093</td>\n",
       "      <td>151.600975</td>\n",
       "      <td>140.994677</td>\n",
       "      <td>1831.689173</td>\n",
       "      <td>401.690884</td>\n",
       "      <td>147.960793</td>\n",
       "      <td>117.923387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27667</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTSGPGIRYP</td>\n",
       "      <td>10</td>\n",
       "      <td>7128.530301</td>\n",
       "      <td>783.429643</td>\n",
       "      <td>5176.068320</td>\n",
       "      <td>1270.574105</td>\n",
       "      <td>725.775535</td>\n",
       "      <td>640.683922</td>\n",
       "      <td>615.384157</td>\n",
       "      <td>693.441744</td>\n",
       "      <td>686.622736</td>\n",
       "      <td>360.663616</td>\n",
       "      <td>255.911727</td>\n",
       "      <td>802.599947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27668</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTSGPGTRYP</td>\n",
       "      <td>10</td>\n",
       "      <td>3097.419299</td>\n",
       "      <td>483.058802</td>\n",
       "      <td>5420.008904</td>\n",
       "      <td>797.994687</td>\n",
       "      <td>874.851439</td>\n",
       "      <td>492.031339</td>\n",
       "      <td>424.265164</td>\n",
       "      <td>450.086126</td>\n",
       "      <td>613.714342</td>\n",
       "      <td>276.632693</td>\n",
       "      <td>171.122399</td>\n",
       "      <td>580.782868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27669</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTSGPGTRYPM</td>\n",
       "      <td>11</td>\n",
       "      <td>25.941794</td>\n",
       "      <td>39.174188</td>\n",
       "      <td>7.744618</td>\n",
       "      <td>76.383578</td>\n",
       "      <td>57.880756</td>\n",
       "      <td>53.473778</td>\n",
       "      <td>92.619044</td>\n",
       "      <td>169.760972</td>\n",
       "      <td>67.420406</td>\n",
       "      <td>73.748992</td>\n",
       "      <td>72.638754</td>\n",
       "      <td>169.702809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27670</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTTGGTSR</td>\n",
       "      <td>8</td>\n",
       "      <td>70145.529842</td>\n",
       "      <td>357.272838</td>\n",
       "      <td>5701.642723</td>\n",
       "      <td>100.230524</td>\n",
       "      <td>461.321351</td>\n",
       "      <td>627.342762</td>\n",
       "      <td>50.256392</td>\n",
       "      <td>33.741332</td>\n",
       "      <td>359.617060</td>\n",
       "      <td>280.240597</td>\n",
       "      <td>55.424336</td>\n",
       "      <td>27.590184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27671</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTTGGTSRN</td>\n",
       "      <td>9</td>\n",
       "      <td>36897.759857</td>\n",
       "      <td>186.637969</td>\n",
       "      <td>6025.595861</td>\n",
       "      <td>119.674053</td>\n",
       "      <td>43.395717</td>\n",
       "      <td>37.559594</td>\n",
       "      <td>12.526246</td>\n",
       "      <td>34.190160</td>\n",
       "      <td>32.472393</td>\n",
       "      <td>20.927878</td>\n",
       "      <td>11.394560</td>\n",
       "      <td>17.767197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27672</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTTTGASRN</td>\n",
       "      <td>9</td>\n",
       "      <td>587.489353</td>\n",
       "      <td>135.518941</td>\n",
       "      <td>2844.461107</td>\n",
       "      <td>87.096359</td>\n",
       "      <td>20.387685</td>\n",
       "      <td>28.239154</td>\n",
       "      <td>10.550078</td>\n",
       "      <td>32.552511</td>\n",
       "      <td>16.739460</td>\n",
       "      <td>25.594238</td>\n",
       "      <td>10.402953</td>\n",
       "      <td>16.482407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27673</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTYEAYVRYP</td>\n",
       "      <td>10</td>\n",
       "      <td>2624.218543</td>\n",
       "      <td>1552.387010</td>\n",
       "      <td>3872.576449</td>\n",
       "      <td>1954.339456</td>\n",
       "      <td>2567.219461</td>\n",
       "      <td>1593.237741</td>\n",
       "      <td>1104.326370</td>\n",
       "      <td>760.166185</td>\n",
       "      <td>1214.150226</td>\n",
       "      <td>1043.119643</td>\n",
       "      <td>431.744442</td>\n",
       "      <td>956.725414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27674</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTYEAYVRYPE</td>\n",
       "      <td>11</td>\n",
       "      <td>1905.460718</td>\n",
       "      <td>864.967919</td>\n",
       "      <td>6998.419960</td>\n",
       "      <td>32.734069</td>\n",
       "      <td>1171.145117</td>\n",
       "      <td>1013.032063</td>\n",
       "      <td>824.132388</td>\n",
       "      <td>673.094577</td>\n",
       "      <td>668.038474</td>\n",
       "      <td>1850.646766</td>\n",
       "      <td>835.730209</td>\n",
       "      <td>722.625199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27675</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YVADALAAF</td>\n",
       "      <td>9</td>\n",
       "      <td>15.381546</td>\n",
       "      <td>453.941617</td>\n",
       "      <td>71.285303</td>\n",
       "      <td>108.143395</td>\n",
       "      <td>68.784648</td>\n",
       "      <td>140.960896</td>\n",
       "      <td>324.069355</td>\n",
       "      <td>415.835482</td>\n",
       "      <td>64.685070</td>\n",
       "      <td>178.037996</td>\n",
       "      <td>331.132138</td>\n",
       "      <td>414.120689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27676</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YVFPVIFSK</td>\n",
       "      <td>9</td>\n",
       "      <td>39264.493540</td>\n",
       "      <td>2098.939884</td>\n",
       "      <td>5610.479760</td>\n",
       "      <td>901.571138</td>\n",
       "      <td>6765.292717</td>\n",
       "      <td>2638.342248</td>\n",
       "      <td>1256.731235</td>\n",
       "      <td>1315.245564</td>\n",
       "      <td>9834.825208</td>\n",
       "      <td>4178.991391</td>\n",
       "      <td>1345.230723</td>\n",
       "      <td>410.734671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27677</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YVFPVIFSR</td>\n",
       "      <td>9</td>\n",
       "      <td>36728.230050</td>\n",
       "      <td>2333.458062</td>\n",
       "      <td>10046.157903</td>\n",
       "      <td>2600.159563</td>\n",
       "      <td>4617.224003</td>\n",
       "      <td>6644.263700</td>\n",
       "      <td>493.353716</td>\n",
       "      <td>284.256376</td>\n",
       "      <td>9740.930600</td>\n",
       "      <td>6475.449199</td>\n",
       "      <td>627.929271</td>\n",
       "      <td>183.415439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27678</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YVPCHIRQI</td>\n",
       "      <td>9</td>\n",
       "      <td>10764.652136</td>\n",
       "      <td>21134.890398</td>\n",
       "      <td>6039.486294</td>\n",
       "      <td>10568.175092</td>\n",
       "      <td>10641.192621</td>\n",
       "      <td>14329.102804</td>\n",
       "      <td>16479.337083</td>\n",
       "      <td>9548.516013</td>\n",
       "      <td>16166.182763</td>\n",
       "      <td>23410.741934</td>\n",
       "      <td>13180.588086</td>\n",
       "      <td>10068.797539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27679</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YVVQMLARL</td>\n",
       "      <td>9</td>\n",
       "      <td>152.405275</td>\n",
       "      <td>232.273680</td>\n",
       "      <td>739.605275</td>\n",
       "      <td>105.681751</td>\n",
       "      <td>617.707822</td>\n",
       "      <td>821.465706</td>\n",
       "      <td>587.540294</td>\n",
       "      <td>622.287192</td>\n",
       "      <td>571.351420</td>\n",
       "      <td>256.068212</td>\n",
       "      <td>562.595643</td>\n",
       "      <td>653.777183</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24743 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         allele      peptide  length          meas        netmhc  \\\n",
       "0        H-2-DB    AAACNVATA       9    657.657837    154.881662   \n",
       "1        H-2-DB     AAFEFVYV       8  30831.879502   6456.542290   \n",
       "2        H-2-DB    AAFVNDYSL       9     77.446180     17.458222   \n",
       "3        H-2-DB    AAIANQAAV       9      1.999862      9.638290   \n",
       "4        H-2-DB    AAIANQAVV       9      1.517050      8.550667   \n",
       "5        H-2-DB    AAIENYVRF       9     37.844258    252.348077   \n",
       "6        H-2-DB    AAINFITTM       9      3.155005    199.986187   \n",
       "7        H-2-DB    AAIPAPPPI       9   3243.396173   1059.253725   \n",
       "8        H-2-DB    AAKLNRPPL       9    654.636174     66.374307   \n",
       "9        H-2-DB    AALDMVDAL       9    229.614865    547.015963   \n",
       "10       H-2-DB    AALQHLRSI       9    905.732601   1686.553025   \n",
       "11       H-2-DB    AALVRLTAL       9   1106.623784    435.511874   \n",
       "12       H-2-DB    AAMVPTGSL       9   1836.538343   4055.085354   \n",
       "13       H-2-DB    AANSPWAPV       9   4325.138310    903.649474   \n",
       "14       H-2-DB    AAPSGAAPL       9      2.844461     97.948999   \n",
       "15       H-2-DB    AAPVSEPTV       9    106.905488   2654.605562   \n",
       "16       H-2-DB    AAVANGFAA       9     83.368118    205.116218   \n",
       "17       H-2-DB    AAVLLGAPV       9    373.250158    320.626932   \n",
       "18       H-2-DB    AAVQLLFPA       9    347.536161   2494.594727   \n",
       "19       H-2-DB    AAVTAGVAL       9    229.614865   1552.387010   \n",
       "20       H-2-DB    AAVVNRSLV       9      8.609938     26.302680   \n",
       "21       H-2-DB    AAYVPADAV       9    331.894458   1757.923614   \n",
       "22       H-2-DB    ACMVNLERL       9    737.904230    239.331576   \n",
       "23       H-2-DB    ADLVNHPPV       9    558.470195   1694.337800   \n",
       "24       H-2-DB    AEMEEALKG       9  78523.563461  41304.750199   \n",
       "25       H-2-DB     AFFAFRYV       8  87902.251683  18967.059212   \n",
       "26       H-2-DB    AGLDNKFYL       9    659.173895    357.272838   \n",
       "27       H-2-DB     AGLLFVLL       8  87902.251683  14996.848355   \n",
       "28       H-2-DB     AGLQVINL       8  59429.215862  25061.092530   \n",
       "29       H-2-DB    AGLVSFNFL       9    210.862815    620.869034   \n",
       "...         ...          ...     ...           ...           ...   \n",
       "27650  Mamu-A02     YNTVCVIW       8    909.913273    734.513868   \n",
       "27651  Mamu-A02    YRHDGGNVL       9  78342.964277   1355.189412   \n",
       "27652  Mamu-A02   YSEGQYMNTP      10   7362.070975   2958.012467   \n",
       "27653  Mamu-A02  YSEVALNVTES      11   7709.034691   1682.674061   \n",
       "27654  Mamu-A02    YSLVFVILM       9   1256.029964     17.498467   \n",
       "27655  Mamu-A02    YSQIGAGVY       9      1.999862     17.864876   \n",
       "27656  Mamu-A02   YSRDLICEQS      10   1059.253725   1297.179271   \n",
       "27657  Mamu-A02    YSYKAFIKY       9      2.317395      4.285485   \n",
       "27658  Mamu-A02   YSYKAFIKYP      10   4830.588020   1199.499303   \n",
       "27659  Mamu-A02  YSYKAFIKYPE      11   4897.788194    671.428853   \n",
       "27660  Mamu-A02    YTAFTLPSV       9    101.624869     32.210688   \n",
       "27661  Mamu-A02   YTAFTLPSVN      10   2529.297996    236.591970   \n",
       "27662  Mamu-A02   YTDGSCNKQS      10  11091.748153   4345.102242   \n",
       "27663  Mamu-A02    YTGDFDSVI       9    215.278173    354.813389   \n",
       "27664  Mamu-A02     YTPKVVGG       8  70145.529842  17947.336268   \n",
       "27665  Mamu-A02  YTPKVVGGIGG      11  70145.529842   9418.895965   \n",
       "27666  Mamu-A02     YTSGPGIR       8  16865.530254    619.441075   \n",
       "27667  Mamu-A02   YTSGPGIRYP      10   7128.530301    783.429643   \n",
       "27668  Mamu-A02   YTSGPGTRYP      10   3097.419299    483.058802   \n",
       "27669  Mamu-A02  YTSGPGTRYPM      11     25.941794     39.174188   \n",
       "27670  Mamu-A02     YTTGGTSR       8  70145.529842    357.272838   \n",
       "27671  Mamu-A02    YTTGGTSRN       9  36897.759857    186.637969   \n",
       "27672  Mamu-A02    YTTTGASRN       9    587.489353    135.518941   \n",
       "27673  Mamu-A02   YTYEAYVRYP      10   2624.218543   1552.387010   \n",
       "27674  Mamu-A02  YTYEAYVRYPE      11   1905.460718    864.967919   \n",
       "27675  Mamu-A02    YVADALAAF       9     15.381546    453.941617   \n",
       "27676  Mamu-A02    YVFPVIFSK       9  39264.493540   2098.939884   \n",
       "27677  Mamu-A02    YVFPVIFSR       9  36728.230050   2333.458062   \n",
       "27678  Mamu-A02    YVPCHIRQI       9  10764.652136  21134.890398   \n",
       "27679  Mamu-A02    YVVQMLARL       9    152.405275    232.273680   \n",
       "\n",
       "          netmhcpan   smmpmbec_cpp   mhcflurry 0   mhcflurry 1   mhcflurry 2  \\\n",
       "0        711.213514     438.530698    112.340509    192.754410    379.761317   \n",
       "1        785.235635   10351.421667  10256.714551   7634.586072  11346.142026   \n",
       "2          7.516229      28.054336     18.079150     14.860045     34.326764   \n",
       "3          9.749896      25.703958     11.153717      8.599590     20.528593   \n",
       "4          8.336812      28.773984      3.084469      5.068488     16.218493   \n",
       "5        114.815362     187.068214    179.024829    119.080622    249.575544   \n",
       "6        389.045145     200.909281      6.328530     44.094703     45.987591   \n",
       "7        493.173804     295.120923    110.038647    144.313801    213.078557   \n",
       "8         77.268059      38.459178     99.976029    216.748867     94.699289   \n",
       "9        597.035287     225.423921   5587.566650   1967.343743    588.703181   \n",
       "10      2032.357011     698.232404    786.309282   1237.196137   1716.506042   \n",
       "11       214.783047     378.442585   2232.905078   1918.012270   1258.549640   \n",
       "12      5176.068320    1545.254440   4440.827219   2844.312485   5569.762393   \n",
       "13      1023.292992     557.185749    249.515757    805.309236   1258.031102   \n",
       "14       501.187234     822.242650    212.468342     99.927233    879.959202   \n",
       "15      1918.668741    1870.682140   1959.447026   2284.064516   2303.540785   \n",
       "16       228.559880     200.447203    274.574166    432.904800    299.986004   \n",
       "17       623.734835     286.417797   1681.570081    397.424315    666.733362   \n",
       "18      5152.286446     679.203633   2200.627852   1134.968527   1457.964316   \n",
       "19      2172.701179    1927.524913    361.893576   2808.835091   2527.631535   \n",
       "20        29.580125      63.826349      6.415570     18.256138     71.040804   \n",
       "21      3206.269325     595.662144   1064.770463   2364.955905   1892.732704   \n",
       "22       679.203633     444.631267   3458.283446    335.970204    586.719201   \n",
       "23      1857.804455     325.087297   6789.133692    951.892616   1206.634603   \n",
       "24     46989.410861  440554.863507  48444.974950  45911.476692  41310.640019   \n",
       "25     12882.495517    7816.278046  22510.621519  14131.408515  24028.326064   \n",
       "26       202.768272     260.015956    273.754382    457.337748    230.741005   \n",
       "27      9885.530947   26242.185434  22081.121649  18089.342700  17013.935532   \n",
       "28     12560.299637   35399.734108  37796.957034  29464.968194  26854.865987   \n",
       "29      2285.598803    1811.340093   1351.673792   1005.323581   1924.966573   \n",
       "...             ...            ...           ...           ...           ...   \n",
       "27650  10023.052381      48.865236    787.330338   2958.936745   1835.522241   \n",
       "27651  22233.098907    1409.288798     43.089568    339.667620    192.155799   \n",
       "27652   8709.635900    4477.133042   2406.176442   1591.222509   1466.886453   \n",
       "27653   5105.050000      51.522864    331.873207     74.191562    155.536028   \n",
       "27654     49.431069       6.324119     16.941694      6.452229     17.484177   \n",
       "27655     15.346170      16.292960     31.859938     32.414771     70.732408   \n",
       "27656   3863.669771     220.292646    146.469163     24.823854     95.090068   \n",
       "27657      3.090295       1.778279      8.399109      6.322224     10.207831   \n",
       "27658   1753.880502    3111.716337    658.359300    680.468133    751.426515   \n",
       "27659   2666.858665      42.559841    911.273799    501.668843    472.900430   \n",
       "27660     52.844525      91.833260      6.136728      7.438836     17.473784   \n",
       "27661   1406.047524      14.487719     11.266901     21.995981     26.502780   \n",
       "27662  11246.049740     588.843655    258.952779    137.857837    456.796814   \n",
       "27663     42.461956      40.550854    237.098327    183.208082    220.892433   \n",
       "27664  13995.873226    3140.508694  17250.549088  13199.088223   7488.080326   \n",
       "27665  10046.157903     387.257645   7022.992659   5341.508901   3244.550478   \n",
       "27666   2349.632821      80.723503    941.543101    745.499093    151.600975   \n",
       "27667   5176.068320    1270.574105    725.775535    640.683922    615.384157   \n",
       "27668   5420.008904     797.994687    874.851439    492.031339    424.265164   \n",
       "27669      7.744618      76.383578     57.880756     53.473778     92.619044   \n",
       "27670   5701.642723     100.230524    461.321351    627.342762     50.256392   \n",
       "27671   6025.595861     119.674053     43.395717     37.559594     12.526246   \n",
       "27672   2844.461107      87.096359     20.387685     28.239154     10.550078   \n",
       "27673   3872.576449    1954.339456   2567.219461   1593.237741   1104.326370   \n",
       "27674   6998.419960      32.734069   1171.145117   1013.032063    824.132388   \n",
       "27675     71.285303     108.143395     68.784648    140.960896    324.069355   \n",
       "27676   5610.479760     901.571138   6765.292717   2638.342248   1256.731235   \n",
       "27677  10046.157903    2600.159563   4617.224003   6644.263700    493.353716   \n",
       "27678   6039.486294   10568.175092  10641.192621  14329.102804  16479.337083   \n",
       "27679    739.605275     105.681751    617.707822    821.465706    587.540294   \n",
       "\n",
       "        mhcflurry 3   mhcflurry 4   mhcflurry 5   mhcflurry 6   mhcflurry 7  \n",
       "0       1212.385318    843.942591    341.705000    315.451272    769.001144  \n",
       "1      15914.774702  12353.946556  11654.561717  10944.064962  13952.278587  \n",
       "2        193.255404    183.575076     10.568342     41.402359    138.784560  \n",
       "3         79.818190      5.110946      9.293367     24.251661     61.158162  \n",
       "4         67.657268      4.023290      5.418416     19.621984     61.784682  \n",
       "5        416.183183    111.430051    220.841440    271.067287    300.608053  \n",
       "6        266.413599     26.621323     61.821550     46.339128    158.900606  \n",
       "7        194.911970    534.798653    838.954495    242.103225    215.492933  \n",
       "8        231.042685    470.524253     62.406672     93.215675    159.333748  \n",
       "9        747.603811   3545.913989   1120.155821    599.119879    333.132501  \n",
       "10      2989.325384   5393.792863   2917.331150   1869.227525   1617.893972  \n",
       "11      3932.638061   4785.061547   1945.389506   1521.067404   2110.680997  \n",
       "12      4860.163064  13034.467166   6236.581508   4797.353134   5295.100788  \n",
       "13      2064.657653   1251.380944   1261.157328   1094.852842   3271.749992  \n",
       "14      1459.675643     38.257331    260.914640   1059.657399   1045.890658  \n",
       "15      1854.533502   1144.076316   3285.933345   2461.964928   1770.746601  \n",
       "16       729.847405     19.308634   1013.697894    276.058692    339.793381  \n",
       "17      1515.685156     22.470471    250.590079    882.174075    912.799024  \n",
       "18      3828.918429   2296.786145   2195.843245   1554.468224   1831.128909  \n",
       "19      2888.031397    352.205107   2417.652937   2944.234479   1956.858856  \n",
       "20       278.989425     26.672586     19.675699     80.588456    192.768209  \n",
       "21      3685.673402   8491.497336   1014.081387   1719.493084   4271.351313  \n",
       "22       834.805741   1703.538177   1592.685251    674.838162    755.982211  \n",
       "23      2467.089933   2362.779711   2703.888338   1131.856153   4096.967360  \n",
       "24     34020.082131  48370.188031  42991.571329  40211.041193  34802.068684  \n",
       "25     23476.493271  17522.598466  11914.024459  23053.294332  20401.753074  \n",
       "26       413.527284    421.345136    779.788064    243.028525    299.565612  \n",
       "27     18772.599101  23074.003835  12846.305291  16831.450351  16307.715541  \n",
       "28     23782.866417  35764.525074  26667.236430  26938.288598  23404.234219  \n",
       "29      3488.180577   3976.828493   1208.701244   2324.415009   3350.396781  \n",
       "...             ...           ...           ...           ...           ...  \n",
       "27650    863.192009   1110.536882   1172.812917   1596.437344   1260.557007  \n",
       "27651    772.560006    128.266535    874.426580    321.426370    389.192968  \n",
       "27652   1205.418451   2075.214415   1276.099359    581.970672   1350.074906  \n",
       "27653     66.558974    220.157526    103.419004     99.271320     94.265753  \n",
       "27654     27.971011      8.318621      7.130104     13.458614     33.100315  \n",
       "27655     78.354937     21.518184     87.288525     47.211193     81.384785  \n",
       "27656    152.750013    167.003616     30.827187     66.831597    114.730647  \n",
       "27657     29.189567     13.441404      4.085901      9.953399     27.190004  \n",
       "27658    876.350086   1288.018623    376.015131    334.145311   1087.782959  \n",
       "27659    581.714770    899.006392   1086.389895    577.809352    613.660403  \n",
       "27660     48.313531      5.619066      6.024876     14.479473     58.326735  \n",
       "27661    152.782343     17.133504     33.079500     23.752858     74.715754  \n",
       "27662    329.864147    561.113888    242.290124    252.656751    340.538442  \n",
       "27663    512.009163     38.372849    609.332718    236.980465    439.462170  \n",
       "27664   3503.806165  18239.227402  12760.786019   7315.381080   2653.227223  \n",
       "27665   2835.346490   5878.557994   4794.104390   3429.932093   1987.332832  \n",
       "27666    140.994677   1831.689173    401.690884    147.960793    117.923387  \n",
       "27667    693.441744    686.622736    360.663616    255.911727    802.599947  \n",
       "27668    450.086126    613.714342    276.632693    171.122399    580.782868  \n",
       "27669    169.760972     67.420406     73.748992     72.638754    169.702809  \n",
       "27670     33.741332    359.617060    280.240597     55.424336     27.590184  \n",
       "27671     34.190160     32.472393     20.927878     11.394560     17.767197  \n",
       "27672     32.552511     16.739460     25.594238     10.402953     16.482407  \n",
       "27673    760.166185   1214.150226   1043.119643    431.744442    956.725414  \n",
       "27674    673.094577    668.038474   1850.646766    835.730209    722.625199  \n",
       "27675    415.835482     64.685070    178.037996    331.132138    414.120689  \n",
       "27676   1315.245564   9834.825208   4178.991391   1345.230723    410.734671  \n",
       "27677    284.256376   9740.930600   6475.449199    627.929271    183.415439  \n",
       "27678   9548.516013  16166.182763  23410.741934  13180.588086  10068.797539  \n",
       "27679    622.287192    571.351420    256.068212    562.595643    653.777183  \n",
       "\n",
       "[24743 rows x 15 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df_with_mhcflurry_results = validation_df_with_mhcflurry.ix[validation_df_with_mhcflurry.allele.isin(models_and_scores)]\n",
    "validation_df_with_mhcflurry_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/anaconda2/envs/standard-2.7/lib/python2.7/site-packages/ipykernel/__main__.py:13: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>allele</th>\n",
       "      <th>mhcflurry 0_auc</th>\n",
       "      <th>mhcflurry 0_f1</th>\n",
       "      <th>mhcflurry 0_tau</th>\n",
       "      <th>mhcflurry 1_auc</th>\n",
       "      <th>mhcflurry 1_f1</th>\n",
       "      <th>mhcflurry 1_tau</th>\n",
       "      <th>mhcflurry 2_auc</th>\n",
       "      <th>mhcflurry 2_f1</th>\n",
       "      <th>mhcflurry 2_tau</th>\n",
       "      <th>...</th>\n",
       "      <th>netmhc_auc</th>\n",
       "      <th>netmhc_f1</th>\n",
       "      <th>netmhc_tau</th>\n",
       "      <th>netmhcpan_auc</th>\n",
       "      <th>netmhcpan_f1</th>\n",
       "      <th>netmhcpan_tau</th>\n",
       "      <th>smmpmbec_cpp_auc</th>\n",
       "      <th>smmpmbec_cpp_f1</th>\n",
       "      <th>smmpmbec_cpp_tau</th>\n",
       "      <th>test_size</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>allele</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>HLA-A2601</th>\n",
       "      <td>HLA-A2601</td>\n",
       "      <td>0.928084</td>\n",
       "      <td>0.580000</td>\n",
       "      <td>0.432299</td>\n",
       "      <td>0.929260</td>\n",
       "      <td>0.593103</td>\n",
       "      <td>0.440722</td>\n",
       "      <td>0.931760</td>\n",
       "      <td>0.574899</td>\n",
       "      <td>0.433437</td>\n",
       "      <td>...</td>\n",
       "      <td>0.930840</td>\n",
       "      <td>0.541176</td>\n",
       "      <td>0.436646</td>\n",
       "      <td>0.942279</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.451666</td>\n",
       "      <td>0.927339</td>\n",
       "      <td>0.603053</td>\n",
       "      <td>0.422900</td>\n",
       "      <td>1333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HLA-B0801</th>\n",
       "      <td>HLA-B0801</td>\n",
       "      <td>0.942694</td>\n",
       "      <td>0.762931</td>\n",
       "      <td>0.600817</td>\n",
       "      <td>0.949517</td>\n",
       "      <td>0.773504</td>\n",
       "      <td>0.620342</td>\n",
       "      <td>0.948274</td>\n",
       "      <td>0.778523</td>\n",
       "      <td>0.616491</td>\n",
       "      <td>...</td>\n",
       "      <td>0.946187</td>\n",
       "      <td>0.776053</td>\n",
       "      <td>0.629519</td>\n",
       "      <td>0.942822</td>\n",
       "      <td>0.701671</td>\n",
       "      <td>0.606762</td>\n",
       "      <td>0.940555</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.618953</td>\n",
       "      <td>940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HLA-B5101</th>\n",
       "      <td>HLA-B5101</td>\n",
       "      <td>0.933050</td>\n",
       "      <td>0.489796</td>\n",
       "      <td>0.369192</td>\n",
       "      <td>0.939082</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.369359</td>\n",
       "      <td>0.948141</td>\n",
       "      <td>0.404762</td>\n",
       "      <td>0.382398</td>\n",
       "      <td>...</td>\n",
       "      <td>0.919175</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.365307</td>\n",
       "      <td>0.948695</td>\n",
       "      <td>0.610169</td>\n",
       "      <td>0.406973</td>\n",
       "      <td>0.927621</td>\n",
       "      <td>0.395604</td>\n",
       "      <td>0.344534</td>\n",
       "      <td>854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HLA-B5701</th>\n",
       "      <td>HLA-B5701</td>\n",
       "      <td>0.914240</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.524933</td>\n",
       "      <td>0.916777</td>\n",
       "      <td>0.740299</td>\n",
       "      <td>0.535227</td>\n",
       "      <td>0.922818</td>\n",
       "      <td>0.670886</td>\n",
       "      <td>0.545377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.899092</td>\n",
       "      <td>0.741840</td>\n",
       "      <td>0.484975</td>\n",
       "      <td>0.883077</td>\n",
       "      <td>0.642623</td>\n",
       "      <td>0.460133</td>\n",
       "      <td>0.929923</td>\n",
       "      <td>0.709091</td>\n",
       "      <td>0.569615</td>\n",
       "      <td>815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HLA-B0702</th>\n",
       "      <td>HLA-B0702</td>\n",
       "      <td>0.904985</td>\n",
       "      <td>0.792711</td>\n",
       "      <td>0.564505</td>\n",
       "      <td>0.915801</td>\n",
       "      <td>0.866525</td>\n",
       "      <td>0.594497</td>\n",
       "      <td>0.909308</td>\n",
       "      <td>0.835526</td>\n",
       "      <td>0.587247</td>\n",
       "      <td>...</td>\n",
       "      <td>0.916126</td>\n",
       "      <td>0.869383</td>\n",
       "      <td>0.606014</td>\n",
       "      <td>0.913527</td>\n",
       "      <td>0.855011</td>\n",
       "      <td>0.604720</td>\n",
       "      <td>0.897928</td>\n",
       "      <td>0.857719</td>\n",
       "      <td>0.563960</td>\n",
       "      <td>813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HLA-A3101</th>\n",
       "      <td>HLA-A3101</td>\n",
       "      <td>0.828510</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.460554</td>\n",
       "      <td>0.854838</td>\n",
       "      <td>0.822500</td>\n",
       "      <td>0.510287</td>\n",
       "      <td>0.847235</td>\n",
       "      <td>0.817610</td>\n",
       "      <td>0.503990</td>\n",
       "      <td>...</td>\n",
       "      <td>0.862606</td>\n",
       "      <td>0.833958</td>\n",
       "      <td>0.525026</td>\n",
       "      <td>0.859261</td>\n",
       "      <td>0.822335</td>\n",
       "      <td>0.528941</td>\n",
       "      <td>0.851270</td>\n",
       "      <td>0.837037</td>\n",
       "      <td>0.502586</td>\n",
       "      <td>724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HLA-A1101</th>\n",
       "      <td>HLA-A1101</td>\n",
       "      <td>0.920368</td>\n",
       "      <td>0.872077</td>\n",
       "      <td>0.579906</td>\n",
       "      <td>0.936520</td>\n",
       "      <td>0.883152</td>\n",
       "      <td>0.615351</td>\n",
       "      <td>0.936927</td>\n",
       "      <td>0.884298</td>\n",
       "      <td>0.612013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.949252</td>\n",
       "      <td>0.883402</td>\n",
       "      <td>0.632199</td>\n",
       "      <td>0.945065</td>\n",
       "      <td>0.887671</td>\n",
       "      <td>0.625794</td>\n",
       "      <td>0.944205</td>\n",
       "      <td>0.882514</td>\n",
       "      <td>0.618388</td>\n",
       "      <td>723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HLA-A0101</th>\n",
       "      <td>HLA-A0101</td>\n",
       "      <td>0.912746</td>\n",
       "      <td>0.621762</td>\n",
       "      <td>0.521720</td>\n",
       "      <td>0.924911</td>\n",
       "      <td>0.659794</td>\n",
       "      <td>0.531596</td>\n",
       "      <td>0.909260</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.521745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.915334</td>\n",
       "      <td>0.619565</td>\n",
       "      <td>0.524866</td>\n",
       "      <td>0.894895</td>\n",
       "      <td>0.594286</td>\n",
       "      <td>0.498767</td>\n",
       "      <td>0.832665</td>\n",
       "      <td>0.437811</td>\n",
       "      <td>0.428064</td>\n",
       "      <td>696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HLA-A0206</th>\n",
       "      <td>HLA-A0206</td>\n",
       "      <td>0.876067</td>\n",
       "      <td>0.849449</td>\n",
       "      <td>0.465713</td>\n",
       "      <td>0.909081</td>\n",
       "      <td>0.872236</td>\n",
       "      <td>0.528876</td>\n",
       "      <td>0.900025</td>\n",
       "      <td>0.868712</td>\n",
       "      <td>0.517421</td>\n",
       "      <td>...</td>\n",
       "      <td>0.913636</td>\n",
       "      <td>0.872902</td>\n",
       "      <td>0.543184</td>\n",
       "      <td>0.910796</td>\n",
       "      <td>0.866258</td>\n",
       "      <td>0.535067</td>\n",
       "      <td>0.904317</td>\n",
       "      <td>0.878282</td>\n",
       "      <td>0.527571</td>\n",
       "      <td>682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HLA-A6802</th>\n",
       "      <td>HLA-A6802</td>\n",
       "      <td>0.954562</td>\n",
       "      <td>0.870482</td>\n",
       "      <td>0.602856</td>\n",
       "      <td>0.968359</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.640201</td>\n",
       "      <td>0.968413</td>\n",
       "      <td>0.889894</td>\n",
       "      <td>0.632660</td>\n",
       "      <td>...</td>\n",
       "      <td>0.971961</td>\n",
       "      <td>0.922636</td>\n",
       "      <td>0.652332</td>\n",
       "      <td>0.964197</td>\n",
       "      <td>0.900293</td>\n",
       "      <td>0.653216</td>\n",
       "      <td>0.965793</td>\n",
       "      <td>0.928775</td>\n",
       "      <td>0.629530</td>\n",
       "      <td>669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HLA-A3001</th>\n",
       "      <td>HLA-A3001</td>\n",
       "      <td>0.888740</td>\n",
       "      <td>0.730864</td>\n",
       "      <td>0.502090</td>\n",
       "      <td>0.881443</td>\n",
       "      <td>0.737913</td>\n",
       "      <td>0.499390</td>\n",
       "      <td>0.889599</td>\n",
       "      <td>0.745592</td>\n",
       "      <td>0.521544</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866899</td>\n",
       "      <td>0.731959</td>\n",
       "      <td>0.456482</td>\n",
       "      <td>0.871974</td>\n",
       "      <td>0.746114</td>\n",
       "      <td>0.459857</td>\n",
       "      <td>0.869935</td>\n",
       "      <td>0.692875</td>\n",
       "      <td>0.505244</td>\n",
       "      <td>660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HLA-A0203</th>\n",
       "      <td>HLA-A0203</td>\n",
       "      <td>0.950892</td>\n",
       "      <td>0.916566</td>\n",
       "      <td>0.534315</td>\n",
       "      <td>0.976453</td>\n",
       "      <td>0.951364</td>\n",
       "      <td>0.573478</td>\n",
       "      <td>0.974324</td>\n",
       "      <td>0.953737</td>\n",
       "      <td>0.592812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.975879</td>\n",
       "      <td>0.948626</td>\n",
       "      <td>0.586911</td>\n",
       "      <td>0.974158</td>\n",
       "      <td>0.944578</td>\n",
       "      <td>0.591463</td>\n",
       "      <td>0.972885</td>\n",
       "      <td>0.946746</td>\n",
       "      <td>0.583908</td>\n",
       "      <td>651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HLA-B3901</th>\n",
       "      <td>HLA-B3901</td>\n",
       "      <td>0.953506</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.297435</td>\n",
       "      <td>0.951364</td>\n",
       "      <td>0.734694</td>\n",
       "      <td>0.295161</td>\n",
       "      <td>0.960260</td>\n",
       "      <td>0.723404</td>\n",
       "      <td>0.303500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.965195</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.305138</td>\n",
       "      <td>0.980065</td>\n",
       "      <td>0.744186</td>\n",
       "      <td>0.316739</td>\n",
       "      <td>0.949708</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.296639</td>\n",
       "      <td>641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HLA-B1501</th>\n",
       "      <td>HLA-B1501</td>\n",
       "      <td>0.914988</td>\n",
       "      <td>0.793503</td>\n",
       "      <td>0.569384</td>\n",
       "      <td>0.933960</td>\n",
       "      <td>0.831050</td>\n",
       "      <td>0.585046</td>\n",
       "      <td>0.937243</td>\n",
       "      <td>0.829736</td>\n",
       "      <td>0.590098</td>\n",
       "      <td>...</td>\n",
       "      <td>0.934416</td>\n",
       "      <td>0.846512</td>\n",
       "      <td>0.591735</td>\n",
       "      <td>0.935596</td>\n",
       "      <td>0.823245</td>\n",
       "      <td>0.589097</td>\n",
       "      <td>0.936063</td>\n",
       "      <td>0.819048</td>\n",
       "      <td>0.590100</td>\n",
       "      <td>633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HLA-B1517</th>\n",
       "      <td>HLA-B1517</td>\n",
       "      <td>0.910482</td>\n",
       "      <td>0.661088</td>\n",
       "      <td>0.407311</td>\n",
       "      <td>0.921477</td>\n",
       "      <td>0.661157</td>\n",
       "      <td>0.417164</td>\n",
       "      <td>0.911692</td>\n",
       "      <td>0.669604</td>\n",
       "      <td>0.402892</td>\n",
       "      <td>...</td>\n",
       "      <td>0.902687</td>\n",
       "      <td>0.637931</td>\n",
       "      <td>0.422497</td>\n",
       "      <td>0.934516</td>\n",
       "      <td>0.710280</td>\n",
       "      <td>0.448575</td>\n",
       "      <td>0.899620</td>\n",
       "      <td>0.652542</td>\n",
       "      <td>0.406582</td>\n",
       "      <td>582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HLA-A2402</th>\n",
       "      <td>HLA-A2402</td>\n",
       "      <td>0.851004</td>\n",
       "      <td>0.653659</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>0.853637</td>\n",
       "      <td>0.649165</td>\n",
       "      <td>0.551522</td>\n",
       "      <td>0.870712</td>\n",
       "      <td>0.585366</td>\n",
       "      <td>0.585672</td>\n",
       "      <td>...</td>\n",
       "      <td>0.864200</td>\n",
       "      <td>0.632911</td>\n",
       "      <td>0.575710</td>\n",
       "      <td>0.892876</td>\n",
       "      <td>0.693069</td>\n",
       "      <td>0.600686</td>\n",
       "      <td>0.848915</td>\n",
       "      <td>0.633416</td>\n",
       "      <td>0.537589</td>\n",
       "      <td>573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H-2-DB</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>0.887624</td>\n",
       "      <td>0.611321</td>\n",
       "      <td>0.598370</td>\n",
       "      <td>0.907299</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.631110</td>\n",
       "      <td>0.894433</td>\n",
       "      <td>0.564103</td>\n",
       "      <td>0.621788</td>\n",
       "      <td>...</td>\n",
       "      <td>0.896152</td>\n",
       "      <td>0.621212</td>\n",
       "      <td>0.600337</td>\n",
       "      <td>0.874574</td>\n",
       "      <td>0.577236</td>\n",
       "      <td>0.574262</td>\n",
       "      <td>0.884187</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.571252</td>\n",
       "      <td>564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H-2-KB</th>\n",
       "      <td>H-2-KB</td>\n",
       "      <td>0.866649</td>\n",
       "      <td>0.794613</td>\n",
       "      <td>0.527096</td>\n",
       "      <td>0.900377</td>\n",
       "      <td>0.797853</td>\n",
       "      <td>0.570317</td>\n",
       "      <td>0.910039</td>\n",
       "      <td>0.812165</td>\n",
       "      <td>0.565230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.891675</td>\n",
       "      <td>0.813675</td>\n",
       "      <td>0.573199</td>\n",
       "      <td>0.825565</td>\n",
       "      <td>0.665354</td>\n",
       "      <td>0.486836</td>\n",
       "      <td>0.915994</td>\n",
       "      <td>0.859967</td>\n",
       "      <td>0.589218</td>\n",
       "      <td>558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HLA-B3501</th>\n",
       "      <td>HLA-B3501</td>\n",
       "      <td>0.828050</td>\n",
       "      <td>0.737903</td>\n",
       "      <td>0.500896</td>\n",
       "      <td>0.831002</td>\n",
       "      <td>0.722656</td>\n",
       "      <td>0.524182</td>\n",
       "      <td>0.829343</td>\n",
       "      <td>0.722110</td>\n",
       "      <td>0.519938</td>\n",
       "      <td>...</td>\n",
       "      <td>0.828690</td>\n",
       "      <td>0.712000</td>\n",
       "      <td>0.515205</td>\n",
       "      <td>0.836281</td>\n",
       "      <td>0.711027</td>\n",
       "      <td>0.514413</td>\n",
       "      <td>0.823734</td>\n",
       "      <td>0.705411</td>\n",
       "      <td>0.501630</td>\n",
       "      <td>542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HLA-A6801</th>\n",
       "      <td>HLA-A6801</td>\n",
       "      <td>0.926693</td>\n",
       "      <td>0.904494</td>\n",
       "      <td>0.519608</td>\n",
       "      <td>0.945221</td>\n",
       "      <td>0.919605</td>\n",
       "      <td>0.571760</td>\n",
       "      <td>0.941744</td>\n",
       "      <td>0.905028</td>\n",
       "      <td>0.556709</td>\n",
       "      <td>...</td>\n",
       "      <td>0.948038</td>\n",
       "      <td>0.924791</td>\n",
       "      <td>0.571403</td>\n",
       "      <td>0.948907</td>\n",
       "      <td>0.923944</td>\n",
       "      <td>0.599562</td>\n",
       "      <td>0.944553</td>\n",
       "      <td>0.924347</td>\n",
       "      <td>0.556418</td>\n",
       "      <td>527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HLA-B0802</th>\n",
       "      <td>HLA-B0802</td>\n",
       "      <td>0.978728</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.361894</td>\n",
       "      <td>0.964924</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.341633</td>\n",
       "      <td>0.980878</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.358488</td>\n",
       "      <td>...</td>\n",
       "      <td>0.989930</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.393833</td>\n",
       "      <td>0.989590</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.410579</td>\n",
       "      <td>0.987214</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.385838</td>\n",
       "      <td>509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HLA-B1801</th>\n",
       "      <td>HLA-B1801</td>\n",
       "      <td>0.775263</td>\n",
       "      <td>0.324324</td>\n",
       "      <td>0.201325</td>\n",
       "      <td>0.811504</td>\n",
       "      <td>0.358974</td>\n",
       "      <td>0.241803</td>\n",
       "      <td>0.774436</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.231177</td>\n",
       "      <td>...</td>\n",
       "      <td>0.794549</td>\n",
       "      <td>0.358974</td>\n",
       "      <td>0.237118</td>\n",
       "      <td>0.789549</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.234201</td>\n",
       "      <td>0.787782</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.215534</td>\n",
       "      <td>503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HLA-B5301</th>\n",
       "      <td>HLA-B5301</td>\n",
       "      <td>0.836274</td>\n",
       "      <td>0.716279</td>\n",
       "      <td>0.511275</td>\n",
       "      <td>0.849549</td>\n",
       "      <td>0.740566</td>\n",
       "      <td>0.528186</td>\n",
       "      <td>0.834934</td>\n",
       "      <td>0.712264</td>\n",
       "      <td>0.518640</td>\n",
       "      <td>...</td>\n",
       "      <td>0.841415</td>\n",
       "      <td>0.733813</td>\n",
       "      <td>0.515585</td>\n",
       "      <td>0.885722</td>\n",
       "      <td>0.753623</td>\n",
       "      <td>0.559543</td>\n",
       "      <td>0.821328</td>\n",
       "      <td>0.734177</td>\n",
       "      <td>0.495308</td>\n",
       "      <td>485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HLA-A3301</th>\n",
       "      <td>HLA-A3301</td>\n",
       "      <td>0.912425</td>\n",
       "      <td>0.822857</td>\n",
       "      <td>0.579394</td>\n",
       "      <td>0.924043</td>\n",
       "      <td>0.863309</td>\n",
       "      <td>0.600599</td>\n",
       "      <td>0.922635</td>\n",
       "      <td>0.850277</td>\n",
       "      <td>0.593279</td>\n",
       "      <td>...</td>\n",
       "      <td>0.914593</td>\n",
       "      <td>0.868327</td>\n",
       "      <td>0.587112</td>\n",
       "      <td>0.931156</td>\n",
       "      <td>0.829175</td>\n",
       "      <td>0.612147</td>\n",
       "      <td>0.907414</td>\n",
       "      <td>0.855098</td>\n",
       "      <td>0.572158</td>\n",
       "      <td>473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HLA-A6901</th>\n",
       "      <td>HLA-A6901</td>\n",
       "      <td>0.958853</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>0.338189</td>\n",
       "      <td>0.964830</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.342789</td>\n",
       "      <td>0.956947</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.325785</td>\n",
       "      <td>...</td>\n",
       "      <td>0.960066</td>\n",
       "      <td>0.698413</td>\n",
       "      <td>0.335339</td>\n",
       "      <td>0.944257</td>\n",
       "      <td>0.680851</td>\n",
       "      <td>0.327432</td>\n",
       "      <td>0.954868</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.322498</td>\n",
       "      <td>470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HLA-B1509</th>\n",
       "      <td>HLA-B1509</td>\n",
       "      <td>0.852442</td>\n",
       "      <td>0.487805</td>\n",
       "      <td>0.298869</td>\n",
       "      <td>0.871854</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.331161</td>\n",
       "      <td>0.880691</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.330301</td>\n",
       "      <td>...</td>\n",
       "      <td>0.901247</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.354311</td>\n",
       "      <td>0.922907</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.448099</td>\n",
       "      <td>0.894618</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.364673</td>\n",
       "      <td>466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HLA-A3201</th>\n",
       "      <td>HLA-A3201</td>\n",
       "      <td>0.884704</td>\n",
       "      <td>0.699690</td>\n",
       "      <td>0.512869</td>\n",
       "      <td>0.893682</td>\n",
       "      <td>0.708861</td>\n",
       "      <td>0.514653</td>\n",
       "      <td>0.900643</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.484062</td>\n",
       "      <td>...</td>\n",
       "      <td>0.893070</td>\n",
       "      <td>0.772881</td>\n",
       "      <td>0.448284</td>\n",
       "      <td>0.907161</td>\n",
       "      <td>0.766423</td>\n",
       "      <td>0.480669</td>\n",
       "      <td>0.832013</td>\n",
       "      <td>0.580796</td>\n",
       "      <td>0.493693</td>\n",
       "      <td>449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HLA-B5801</th>\n",
       "      <td>HLA-B5801</td>\n",
       "      <td>0.894775</td>\n",
       "      <td>0.832512</td>\n",
       "      <td>0.559392</td>\n",
       "      <td>0.904312</td>\n",
       "      <td>0.835821</td>\n",
       "      <td>0.571688</td>\n",
       "      <td>0.907261</td>\n",
       "      <td>0.825641</td>\n",
       "      <td>0.587919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.891654</td>\n",
       "      <td>0.827411</td>\n",
       "      <td>0.545132</td>\n",
       "      <td>0.882016</td>\n",
       "      <td>0.805195</td>\n",
       "      <td>0.531508</td>\n",
       "      <td>0.905645</td>\n",
       "      <td>0.817757</td>\n",
       "      <td>0.582026</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HLA-B2703</th>\n",
       "      <td>HLA-B2703</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018979</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060609</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.027856</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053658</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.067451</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HLA-A3002</th>\n",
       "      <td>HLA-A3002</td>\n",
       "      <td>0.711574</td>\n",
       "      <td>0.619512</td>\n",
       "      <td>0.315227</td>\n",
       "      <td>0.728981</td>\n",
       "      <td>0.622785</td>\n",
       "      <td>0.334000</td>\n",
       "      <td>0.742042</td>\n",
       "      <td>0.645161</td>\n",
       "      <td>0.343583</td>\n",
       "      <td>...</td>\n",
       "      <td>0.749659</td>\n",
       "      <td>0.663438</td>\n",
       "      <td>0.342161</td>\n",
       "      <td>0.732749</td>\n",
       "      <td>0.611260</td>\n",
       "      <td>0.320577</td>\n",
       "      <td>0.754271</td>\n",
       "      <td>0.704104</td>\n",
       "      <td>0.354956</td>\n",
       "      <td>420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HLA-A2501</th>\n",
       "      <td>HLA-A2501</td>\n",
       "      <td>0.975669</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.146430</td>\n",
       "      <td>0.972749</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.145681</td>\n",
       "      <td>0.977129</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.147030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.991727</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.151836</td>\n",
       "      <td>0.998540</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.153968</td>\n",
       "      <td>0.992701</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.151997</td>\n",
       "      <td>416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HLA-A2602</th>\n",
       "      <td>HLA-A2602</td>\n",
       "      <td>0.902721</td>\n",
       "      <td>0.713615</td>\n",
       "      <td>0.469627</td>\n",
       "      <td>0.926269</td>\n",
       "      <td>0.746411</td>\n",
       "      <td>0.493376</td>\n",
       "      <td>0.930901</td>\n",
       "      <td>0.786070</td>\n",
       "      <td>0.518496</td>\n",
       "      <td>...</td>\n",
       "      <td>0.931577</td>\n",
       "      <td>0.766839</td>\n",
       "      <td>0.525365</td>\n",
       "      <td>0.957811</td>\n",
       "      <td>0.839378</td>\n",
       "      <td>0.561093</td>\n",
       "      <td>0.942981</td>\n",
       "      <td>0.790244</td>\n",
       "      <td>0.531805</td>\n",
       "      <td>413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HLA-B4402</th>\n",
       "      <td>HLA-B4402</td>\n",
       "      <td>0.869757</td>\n",
       "      <td>0.682292</td>\n",
       "      <td>0.506405</td>\n",
       "      <td>0.868196</td>\n",
       "      <td>0.601156</td>\n",
       "      <td>0.513061</td>\n",
       "      <td>0.887197</td>\n",
       "      <td>0.517572</td>\n",
       "      <td>0.544209</td>\n",
       "      <td>...</td>\n",
       "      <td>0.912335</td>\n",
       "      <td>0.568807</td>\n",
       "      <td>0.574916</td>\n",
       "      <td>0.934554</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.598960</td>\n",
       "      <td>0.802018</td>\n",
       "      <td>0.570652</td>\n",
       "      <td>0.377900</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HLA-B4001</th>\n",
       "      <td>HLA-B4001</td>\n",
       "      <td>0.895389</td>\n",
       "      <td>0.735043</td>\n",
       "      <td>0.580586</td>\n",
       "      <td>0.906650</td>\n",
       "      <td>0.723032</td>\n",
       "      <td>0.602434</td>\n",
       "      <td>0.912764</td>\n",
       "      <td>0.680982</td>\n",
       "      <td>0.612248</td>\n",
       "      <td>...</td>\n",
       "      <td>0.928725</td>\n",
       "      <td>0.829787</td>\n",
       "      <td>0.635308</td>\n",
       "      <td>0.920678</td>\n",
       "      <td>0.845570</td>\n",
       "      <td>0.619418</td>\n",
       "      <td>0.862331</td>\n",
       "      <td>0.674487</td>\n",
       "      <td>0.520725</td>\n",
       "      <td>407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HLA-A2301</th>\n",
       "      <td>HLA-A2301</td>\n",
       "      <td>0.840581</td>\n",
       "      <td>0.710602</td>\n",
       "      <td>0.556410</td>\n",
       "      <td>0.873845</td>\n",
       "      <td>0.773481</td>\n",
       "      <td>0.596226</td>\n",
       "      <td>0.870580</td>\n",
       "      <td>0.743363</td>\n",
       "      <td>0.582937</td>\n",
       "      <td>...</td>\n",
       "      <td>0.881451</td>\n",
       "      <td>0.773842</td>\n",
       "      <td>0.605598</td>\n",
       "      <td>0.895229</td>\n",
       "      <td>0.788406</td>\n",
       "      <td>0.636944</td>\n",
       "      <td>0.849381</td>\n",
       "      <td>0.745000</td>\n",
       "      <td>0.561259</td>\n",
       "      <td>391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mamu-A02</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>0.876989</td>\n",
       "      <td>0.682216</td>\n",
       "      <td>0.504329</td>\n",
       "      <td>0.870053</td>\n",
       "      <td>0.668555</td>\n",
       "      <td>0.507614</td>\n",
       "      <td>0.853992</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.485396</td>\n",
       "      <td>...</td>\n",
       "      <td>0.929688</td>\n",
       "      <td>0.767123</td>\n",
       "      <td>0.598964</td>\n",
       "      <td>0.883087</td>\n",
       "      <td>0.748092</td>\n",
       "      <td>0.524622</td>\n",
       "      <td>0.810692</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.478589</td>\n",
       "      <td>388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HLA-A8001</th>\n",
       "      <td>HLA-A8001</td>\n",
       "      <td>0.966590</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.175280</td>\n",
       "      <td>0.988479</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.194232</td>\n",
       "      <td>0.985407</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.180293</td>\n",
       "      <td>...</td>\n",
       "      <td>0.983487</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.182374</td>\n",
       "      <td>0.980799</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.186524</td>\n",
       "      <td>0.990015</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.183987</td>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HLA-B4403</th>\n",
       "      <td>HLA-B4403</td>\n",
       "      <td>0.785885</td>\n",
       "      <td>0.677333</td>\n",
       "      <td>0.419995</td>\n",
       "      <td>0.811937</td>\n",
       "      <td>0.732824</td>\n",
       "      <td>0.450560</td>\n",
       "      <td>0.813593</td>\n",
       "      <td>0.613982</td>\n",
       "      <td>0.462364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.869501</td>\n",
       "      <td>0.766304</td>\n",
       "      <td>0.541066</td>\n",
       "      <td>0.891047</td>\n",
       "      <td>0.775956</td>\n",
       "      <td>0.587824</td>\n",
       "      <td>0.796454</td>\n",
       "      <td>0.754902</td>\n",
       "      <td>0.432021</td>\n",
       "      <td>378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HLA-B4601</th>\n",
       "      <td>HLA-B4601</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HLA-B3801</th>\n",
       "      <td>HLA-B3801</td>\n",
       "      <td>0.901216</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500359</td>\n",
       "      <td>0.909119</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.515732</td>\n",
       "      <td>0.921952</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.524527</td>\n",
       "      <td>...</td>\n",
       "      <td>0.925684</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.513269</td>\n",
       "      <td>0.980074</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.652173</td>\n",
       "      <td>0.949206</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.547438</td>\n",
       "      <td>351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HLA-B2705</th>\n",
       "      <td>HLA-B2705</td>\n",
       "      <td>0.945675</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.416364</td>\n",
       "      <td>0.944586</td>\n",
       "      <td>0.509804</td>\n",
       "      <td>0.424005</td>\n",
       "      <td>0.948578</td>\n",
       "      <td>0.489796</td>\n",
       "      <td>0.438219</td>\n",
       "      <td>...</td>\n",
       "      <td>0.948457</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.430561</td>\n",
       "      <td>0.943860</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.377208</td>\n",
       "      <td>0.941682</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.416884</td>\n",
       "      <td>314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HLA-A2603</th>\n",
       "      <td>HLA-A2603</td>\n",
       "      <td>0.860835</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.320885</td>\n",
       "      <td>0.863587</td>\n",
       "      <td>0.537313</td>\n",
       "      <td>0.315634</td>\n",
       "      <td>0.862604</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.304849</td>\n",
       "      <td>...</td>\n",
       "      <td>0.890172</td>\n",
       "      <td>0.542373</td>\n",
       "      <td>0.366317</td>\n",
       "      <td>0.934300</td>\n",
       "      <td>0.581818</td>\n",
       "      <td>0.426438</td>\n",
       "      <td>0.843194</td>\n",
       "      <td>0.509091</td>\n",
       "      <td>0.306741</td>\n",
       "      <td>312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mamu-A01</th>\n",
       "      <td>Mamu-A01</td>\n",
       "      <td>0.847141</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.455049</td>\n",
       "      <td>0.889177</td>\n",
       "      <td>0.643836</td>\n",
       "      <td>0.506096</td>\n",
       "      <td>0.922777</td>\n",
       "      <td>0.716418</td>\n",
       "      <td>0.527595</td>\n",
       "      <td>...</td>\n",
       "      <td>0.916721</td>\n",
       "      <td>0.694915</td>\n",
       "      <td>0.569423</td>\n",
       "      <td>0.912755</td>\n",
       "      <td>0.706897</td>\n",
       "      <td>0.539013</td>\n",
       "      <td>0.839390</td>\n",
       "      <td>0.609272</td>\n",
       "      <td>0.469975</td>\n",
       "      <td>274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HLA-B0803</th>\n",
       "      <td>HLA-B0803</td>\n",
       "      <td>0.952593</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.302768</td>\n",
       "      <td>0.886914</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.273943</td>\n",
       "      <td>0.957037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.309651</td>\n",
       "      <td>...</td>\n",
       "      <td>0.968395</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.318249</td>\n",
       "      <td>0.952346</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.315978</td>\n",
       "      <td>0.973333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.314266</td>\n",
       "      <td>234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H-2-KD</th>\n",
       "      <td>H-2-KD</td>\n",
       "      <td>0.765223</td>\n",
       "      <td>0.613333</td>\n",
       "      <td>0.336723</td>\n",
       "      <td>0.764808</td>\n",
       "      <td>0.569444</td>\n",
       "      <td>0.323700</td>\n",
       "      <td>0.766882</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.348129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.815331</td>\n",
       "      <td>0.657718</td>\n",
       "      <td>0.403275</td>\n",
       "      <td>0.819189</td>\n",
       "      <td>0.645570</td>\n",
       "      <td>0.390333</td>\n",
       "      <td>0.753692</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.365247</td>\n",
       "      <td>229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HLA-B1503</th>\n",
       "      <td>HLA-B1503</td>\n",
       "      <td>0.788338</td>\n",
       "      <td>0.477064</td>\n",
       "      <td>0.361371</td>\n",
       "      <td>0.820478</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.377722</td>\n",
       "      <td>0.839991</td>\n",
       "      <td>0.537037</td>\n",
       "      <td>0.413124</td>\n",
       "      <td>...</td>\n",
       "      <td>0.864784</td>\n",
       "      <td>0.592593</td>\n",
       "      <td>0.468922</td>\n",
       "      <td>0.870064</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.522577</td>\n",
       "      <td>0.839417</td>\n",
       "      <td>0.523364</td>\n",
       "      <td>0.444917</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HLA-A0202</th>\n",
       "      <td>HLA-A0202</td>\n",
       "      <td>0.833980</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.561484</td>\n",
       "      <td>0.893293</td>\n",
       "      <td>0.764045</td>\n",
       "      <td>0.614703</td>\n",
       "      <td>0.903271</td>\n",
       "      <td>0.791209</td>\n",
       "      <td>0.641312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.890382</td>\n",
       "      <td>0.755556</td>\n",
       "      <td>0.627143</td>\n",
       "      <td>0.898697</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.624280</td>\n",
       "      <td>0.882206</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.606938</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HLA-A2902</th>\n",
       "      <td>HLA-A2902</td>\n",
       "      <td>0.890924</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.620455</td>\n",
       "      <td>0.884819</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.649314</td>\n",
       "      <td>0.873016</td>\n",
       "      <td>0.644068</td>\n",
       "      <td>0.621376</td>\n",
       "      <td>...</td>\n",
       "      <td>0.868946</td>\n",
       "      <td>0.644068</td>\n",
       "      <td>0.619830</td>\n",
       "      <td>0.882377</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.641257</td>\n",
       "      <td>0.871795</td>\n",
       "      <td>0.655172</td>\n",
       "      <td>0.612364</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HLA-B5401</th>\n",
       "      <td>HLA-B5401</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.295787</td>\n",
       "      <td>0.816216</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.314822</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.311894</td>\n",
       "      <td>...</td>\n",
       "      <td>0.816216</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.300252</td>\n",
       "      <td>0.845946</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.349826</td>\n",
       "      <td>0.839189</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.333600</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HLA-B4002</th>\n",
       "      <td>HLA-B4002</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.494300</td>\n",
       "      <td>0.908730</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.481845</td>\n",
       "      <td>0.934524</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>0.493522</td>\n",
       "      <td>...</td>\n",
       "      <td>0.920635</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>0.456242</td>\n",
       "      <td>0.918651</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.518720</td>\n",
       "      <td>0.934524</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>0.455548</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HLA-B4501</th>\n",
       "      <td>HLA-B4501</td>\n",
       "      <td>0.996667</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.230045</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.247621</td>\n",
       "      <td>0.996667</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.238316</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.263574</td>\n",
       "      <td>0.993333</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.246426</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.251301</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              allele  mhcflurry 0_auc  mhcflurry 0_f1  mhcflurry 0_tau  \\\n",
       "allele                                                                   \n",
       "HLA-A2601  HLA-A2601         0.928084        0.580000         0.432299   \n",
       "HLA-B0801  HLA-B0801         0.942694        0.762931         0.600817   \n",
       "HLA-B5101  HLA-B5101         0.933050        0.489796         0.369192   \n",
       "HLA-B5701  HLA-B5701         0.914240        0.750000         0.524933   \n",
       "HLA-B0702  HLA-B0702         0.904985        0.792711         0.564505   \n",
       "HLA-A3101  HLA-A3101         0.828510        0.782609         0.460554   \n",
       "HLA-A1101  HLA-A1101         0.920368        0.872077         0.579906   \n",
       "HLA-A0101  HLA-A0101         0.912746        0.621762         0.521720   \n",
       "HLA-A0206  HLA-A0206         0.876067        0.849449         0.465713   \n",
       "HLA-A6802  HLA-A6802         0.954562        0.870482         0.602856   \n",
       "HLA-A3001  HLA-A3001         0.888740        0.730864         0.502090   \n",
       "HLA-A0203  HLA-A0203         0.950892        0.916566         0.534315   \n",
       "HLA-B3901  HLA-B3901         0.953506        0.705882         0.297435   \n",
       "HLA-B1501  HLA-B1501         0.914988        0.793503         0.569384   \n",
       "HLA-B1517  HLA-B1517         0.910482        0.661088         0.407311   \n",
       "HLA-A2402  HLA-A2402         0.851004        0.653659         0.535714   \n",
       "H-2-DB        H-2-DB         0.887624        0.611321         0.598370   \n",
       "H-2-KB        H-2-KB         0.866649        0.794613         0.527096   \n",
       "HLA-B3501  HLA-B3501         0.828050        0.737903         0.500896   \n",
       "HLA-A6801  HLA-A6801         0.926693        0.904494         0.519608   \n",
       "HLA-B0802  HLA-B0802         0.978728        0.285714         0.361894   \n",
       "HLA-B1801  HLA-B1801         0.775263        0.324324         0.201325   \n",
       "HLA-B5301  HLA-B5301         0.836274        0.716279         0.511275   \n",
       "HLA-A3301  HLA-A3301         0.912425        0.822857         0.579394   \n",
       "HLA-A6901  HLA-A6901         0.958853        0.638889         0.338189   \n",
       "HLA-B1509  HLA-B1509         0.852442        0.487805         0.298869   \n",
       "HLA-A3201  HLA-A3201         0.884704        0.699690         0.512869   \n",
       "HLA-B5801  HLA-B5801         0.894775        0.832512         0.559392   \n",
       "HLA-B2703  HLA-B2703              NaN        0.000000         0.018979   \n",
       "HLA-A3002  HLA-A3002         0.711574        0.619512         0.315227   \n",
       "HLA-A2501  HLA-A2501         0.975669        0.363636         0.146430   \n",
       "HLA-A2602  HLA-A2602         0.902721        0.713615         0.469627   \n",
       "HLA-B4402  HLA-B4402         0.869757        0.682292         0.506405   \n",
       "HLA-B4001  HLA-B4001         0.895389        0.735043         0.580586   \n",
       "HLA-A2301  HLA-A2301         0.840581        0.710602         0.556410   \n",
       "Mamu-A02    Mamu-A02         0.876989        0.682216         0.504329   \n",
       "HLA-A8001  HLA-A8001         0.966590        0.344828         0.175280   \n",
       "HLA-B4403  HLA-B4403         0.785885        0.677333         0.419995   \n",
       "HLA-B4601  HLA-B4601              NaN        0.000000              NaN   \n",
       "HLA-B3801  HLA-B3801         0.901216        0.000000         0.500359   \n",
       "HLA-B2705  HLA-B2705         0.945675        0.571429         0.416364   \n",
       "HLA-A2603  HLA-A2603         0.860835        0.529412         0.320885   \n",
       "Mamu-A01    Mamu-A01         0.847141        0.583333         0.455049   \n",
       "HLA-B0803  HLA-B0803         0.952593        0.000000         0.302768   \n",
       "H-2-KD        H-2-KD         0.765223        0.613333         0.336723   \n",
       "HLA-B1503  HLA-B1503         0.788338        0.477064         0.361371   \n",
       "HLA-A0202  HLA-A0202         0.833980        0.720000         0.561484   \n",
       "HLA-A2902  HLA-A2902         0.890924        0.700000         0.620455   \n",
       "HLA-B5401  HLA-B5401         0.800000        0.727273         0.295787   \n",
       "HLA-B4002  HLA-B4002         0.916667        0.764706         0.494300   \n",
       "HLA-B4501  HLA-B4501         0.996667        0.750000         0.230045   \n",
       "\n",
       "           mhcflurry 1_auc  mhcflurry 1_f1  mhcflurry 1_tau  mhcflurry 2_auc  \\\n",
       "allele                                                                         \n",
       "HLA-A2601         0.929260        0.593103         0.440722         0.931760   \n",
       "HLA-B0801         0.949517        0.773504         0.620342         0.948274   \n",
       "HLA-B5101         0.939082        0.391304         0.369359         0.948141   \n",
       "HLA-B5701         0.916777        0.740299         0.535227         0.922818   \n",
       "HLA-B0702         0.915801        0.866525         0.594497         0.909308   \n",
       "HLA-A3101         0.854838        0.822500         0.510287         0.847235   \n",
       "HLA-A1101         0.936520        0.883152         0.615351         0.936927   \n",
       "HLA-A0101         0.924911        0.659794         0.531596         0.909260   \n",
       "HLA-A0206         0.909081        0.872236         0.528876         0.900025   \n",
       "HLA-A6802         0.968359        0.900000         0.640201         0.968413   \n",
       "HLA-A3001         0.881443        0.737913         0.499390         0.889599   \n",
       "HLA-A0203         0.976453        0.951364         0.573478         0.974324   \n",
       "HLA-B3901         0.951364        0.734694         0.295161         0.960260   \n",
       "HLA-B1501         0.933960        0.831050         0.585046         0.937243   \n",
       "HLA-B1517         0.921477        0.661157         0.417164         0.911692   \n",
       "HLA-A2402         0.853637        0.649165         0.551522         0.870712   \n",
       "H-2-DB            0.907299        0.666667         0.631110         0.894433   \n",
       "H-2-KB            0.900377        0.797853         0.570317         0.910039   \n",
       "HLA-B3501         0.831002        0.722656         0.524182         0.829343   \n",
       "HLA-A6801         0.945221        0.919605         0.571760         0.941744   \n",
       "HLA-B0802         0.964924        0.160000         0.341633         0.980878   \n",
       "HLA-B1801         0.811504        0.358974         0.241803         0.774436   \n",
       "HLA-B5301         0.849549        0.740566         0.528186         0.834934   \n",
       "HLA-A3301         0.924043        0.863309         0.600599         0.922635   \n",
       "HLA-A6901         0.964830        0.666667         0.342789         0.956947   \n",
       "HLA-B1509         0.871854        0.315789         0.331161         0.880691   \n",
       "HLA-A3201         0.893682        0.708861         0.514653         0.900643   \n",
       "HLA-B5801         0.904312        0.835821         0.571688         0.907261   \n",
       "HLA-B2703              NaN        0.000000         0.060609              NaN   \n",
       "HLA-A3002         0.728981        0.622785         0.334000         0.742042   \n",
       "HLA-A2501         0.972749        0.380952         0.145681         0.977129   \n",
       "HLA-A2602         0.926269        0.746411         0.493376         0.930901   \n",
       "HLA-B4402         0.868196        0.601156         0.513061         0.887197   \n",
       "HLA-B4001         0.906650        0.723032         0.602434         0.912764   \n",
       "HLA-A2301         0.873845        0.773481         0.596226         0.870580   \n",
       "Mamu-A02          0.870053        0.668555         0.507614         0.853992   \n",
       "HLA-A8001         0.988479        0.560000         0.194232         0.985407   \n",
       "HLA-B4403         0.811937        0.732824         0.450560         0.813593   \n",
       "HLA-B4601              NaN        0.000000              NaN              NaN   \n",
       "HLA-B3801         0.909119        0.000000         0.515732         0.921952   \n",
       "HLA-B2705         0.944586        0.509804         0.424005         0.948578   \n",
       "HLA-A2603         0.863587        0.537313         0.315634         0.862604   \n",
       "Mamu-A01          0.889177        0.643836         0.506096         0.922777   \n",
       "HLA-B0803         0.886914        0.000000         0.273943         0.957037   \n",
       "H-2-KD            0.764808        0.569444         0.323700         0.766882   \n",
       "HLA-B1503         0.820478        0.476190         0.377722         0.839991   \n",
       "HLA-A0202         0.893293        0.764045         0.614703         0.903271   \n",
       "HLA-A2902         0.884819        0.666667         0.649314         0.873016   \n",
       "HLA-B5401         0.816216        0.666667         0.314822         0.800000   \n",
       "HLA-B4002         0.908730        0.689655         0.481845         0.934524   \n",
       "HLA-B4501         1.000000        0.750000         0.247621         0.996667   \n",
       "\n",
       "           mhcflurry 2_f1  mhcflurry 2_tau    ...      netmhc_auc  netmhc_f1  \\\n",
       "allele                                        ...                              \n",
       "HLA-A2601        0.574899         0.433437    ...        0.930840   0.541176   \n",
       "HLA-B0801        0.778523         0.616491    ...        0.946187   0.776053   \n",
       "HLA-B5101        0.404762         0.382398    ...        0.919175   0.428571   \n",
       "HLA-B5701        0.670886         0.545377    ...        0.899092   0.741840   \n",
       "HLA-B0702        0.835526         0.587247    ...        0.916126   0.869383   \n",
       "HLA-A3101        0.817610         0.503990    ...        0.862606   0.833958   \n",
       "HLA-A1101        0.884298         0.612013    ...        0.949252   0.883402   \n",
       "HLA-A0101        0.586207         0.521745    ...        0.915334   0.619565   \n",
       "HLA-A0206        0.868712         0.517421    ...        0.913636   0.872902   \n",
       "HLA-A6802        0.889894         0.632660    ...        0.971961   0.922636   \n",
       "HLA-A3001        0.745592         0.521544    ...        0.866899   0.731959   \n",
       "HLA-A0203        0.953737         0.592812    ...        0.975879   0.948626   \n",
       "HLA-B3901        0.723404         0.303500    ...        0.965195   0.750000   \n",
       "HLA-B1501        0.829736         0.590098    ...        0.934416   0.846512   \n",
       "HLA-B1517        0.669604         0.402892    ...        0.902687   0.637931   \n",
       "HLA-A2402        0.585366         0.585672    ...        0.864200   0.632911   \n",
       "H-2-DB           0.564103         0.621788    ...        0.896152   0.621212   \n",
       "H-2-KB           0.812165         0.565230    ...        0.891675   0.813675   \n",
       "HLA-B3501        0.722110         0.519938    ...        0.828690   0.712000   \n",
       "HLA-A6801        0.905028         0.556709    ...        0.948038   0.924791   \n",
       "HLA-B0802        0.000000         0.358488    ...        0.989930   0.190476   \n",
       "HLA-B1801        0.333333         0.231177    ...        0.794549   0.358974   \n",
       "HLA-B5301        0.712264         0.518640    ...        0.841415   0.733813   \n",
       "HLA-A3301        0.850277         0.593279    ...        0.914593   0.868327   \n",
       "HLA-A6901        0.700000         0.325785    ...        0.960066   0.698413   \n",
       "HLA-B1509        0.129032         0.330301    ...        0.901247   0.187500   \n",
       "HLA-A3201        0.777778         0.484062    ...        0.893070   0.772881   \n",
       "HLA-B5801        0.825641         0.587919    ...        0.891654   0.827411   \n",
       "HLA-B2703        0.000000        -0.027856    ...             NaN   0.000000   \n",
       "HLA-A3002        0.645161         0.343583    ...        0.749659   0.663438   \n",
       "HLA-A2501        0.600000         0.147030    ...        0.991727   0.666667   \n",
       "HLA-A2602        0.786070         0.518496    ...        0.931577   0.766839   \n",
       "HLA-B4402        0.517572         0.544209    ...        0.912335   0.568807   \n",
       "HLA-B4001        0.680982         0.612248    ...        0.928725   0.829787   \n",
       "HLA-A2301        0.743363         0.582937    ...        0.881451   0.773842   \n",
       "Mamu-A02         0.666667         0.485396    ...        0.929688   0.767123   \n",
       "HLA-A8001        0.461538         0.180293    ...        0.983487   0.434783   \n",
       "HLA-B4403        0.613982         0.462364    ...        0.869501   0.766304   \n",
       "HLA-B4601        0.000000              NaN    ...             NaN   0.000000   \n",
       "HLA-B3801        0.000000         0.524527    ...        0.925684   0.000000   \n",
       "HLA-B2705        0.489796         0.438219    ...        0.948457   0.285714   \n",
       "HLA-A2603        0.518519         0.304849    ...        0.890172   0.542373   \n",
       "Mamu-A01         0.716418         0.527595    ...        0.916721   0.694915   \n",
       "HLA-B0803        0.000000         0.309651    ...        0.968395   0.000000   \n",
       "H-2-KD           0.545455         0.348129    ...        0.815331   0.657718   \n",
       "HLA-B1503        0.537037         0.413124    ...        0.864784   0.592593   \n",
       "HLA-A0202        0.791209         0.641312    ...        0.890382   0.755556   \n",
       "HLA-A2902        0.644068         0.621376    ...        0.868946   0.644068   \n",
       "HLA-B5401        0.727273         0.311894    ...        0.816216   0.800000   \n",
       "HLA-B4002        0.758621         0.493522    ...        0.920635   0.758621   \n",
       "HLA-B4501        0.571429         0.238316    ...        1.000000   1.000000   \n",
       "\n",
       "           netmhc_tau  netmhcpan_auc  netmhcpan_f1  netmhcpan_tau  \\\n",
       "allele                                                              \n",
       "HLA-A2601    0.436646       0.942279      0.640000       0.451666   \n",
       "HLA-B0801    0.629519       0.942822      0.701671       0.606762   \n",
       "HLA-B5101    0.365307       0.948695      0.610169       0.406973   \n",
       "HLA-B5701    0.484975       0.883077      0.642623       0.460133   \n",
       "HLA-B0702    0.606014       0.913527      0.855011       0.604720   \n",
       "HLA-A3101    0.525026       0.859261      0.822335       0.528941   \n",
       "HLA-A1101    0.632199       0.945065      0.887671       0.625794   \n",
       "HLA-A0101    0.524866       0.894895      0.594286       0.498767   \n",
       "HLA-A0206    0.543184       0.910796      0.866258       0.535067   \n",
       "HLA-A6802    0.652332       0.964197      0.900293       0.653216   \n",
       "HLA-A3001    0.456482       0.871974      0.746114       0.459857   \n",
       "HLA-A0203    0.586911       0.974158      0.944578       0.591463   \n",
       "HLA-B3901    0.305138       0.980065      0.744186       0.316739   \n",
       "HLA-B1501    0.591735       0.935596      0.823245       0.589097   \n",
       "HLA-B1517    0.422497       0.934516      0.710280       0.448575   \n",
       "HLA-A2402    0.575710       0.892876      0.693069       0.600686   \n",
       "H-2-DB       0.600337       0.874574      0.577236       0.574262   \n",
       "H-2-KB       0.573199       0.825565      0.665354       0.486836   \n",
       "HLA-B3501    0.515205       0.836281      0.711027       0.514413   \n",
       "HLA-A6801    0.571403       0.948907      0.923944       0.599562   \n",
       "HLA-B0802    0.393833       0.989590      0.571429       0.410579   \n",
       "HLA-B1801    0.237118       0.789549      0.380952       0.234201   \n",
       "HLA-B5301    0.515585       0.885722      0.753623       0.559543   \n",
       "HLA-A3301    0.587112       0.931156      0.829175       0.612147   \n",
       "HLA-A6901    0.335339       0.944257      0.680851       0.327432   \n",
       "HLA-B1509    0.354311       0.922907      0.176471       0.448099   \n",
       "HLA-A3201    0.448284       0.907161      0.766423       0.480669   \n",
       "HLA-B5801    0.545132       0.882016      0.805195       0.531508   \n",
       "HLA-B2703    0.053658            NaN      0.000000       0.067451   \n",
       "HLA-A3002    0.342161       0.732749      0.611260       0.320577   \n",
       "HLA-A2501    0.151836       0.998540      0.888889       0.153968   \n",
       "HLA-A2602    0.525365       0.957811      0.839378       0.561093   \n",
       "HLA-B4402    0.574916       0.934554      0.607143       0.598960   \n",
       "HLA-B4001    0.635308       0.920678      0.845570       0.619418   \n",
       "HLA-A2301    0.605598       0.895229      0.788406       0.636944   \n",
       "Mamu-A02     0.598964       0.883087      0.748092       0.524622   \n",
       "HLA-A8001    0.182374       0.980799      0.434783       0.186524   \n",
       "HLA-B4403    0.541066       0.891047      0.775956       0.587824   \n",
       "HLA-B4601         NaN            NaN      0.000000            NaN   \n",
       "HLA-B3801    0.513269       0.980074      0.000000       0.652173   \n",
       "HLA-B2705    0.430561       0.943860      0.400000       0.377208   \n",
       "HLA-A2603    0.366317       0.934300      0.581818       0.426438   \n",
       "Mamu-A01     0.569423       0.912755      0.706897       0.539013   \n",
       "HLA-B0803    0.318249       0.952346      0.000000       0.315978   \n",
       "H-2-KD       0.403275       0.819189      0.645570       0.390333   \n",
       "HLA-B1503    0.468922       0.870064      0.588235       0.522577   \n",
       "HLA-A0202    0.627143       0.898697      0.769231       0.624280   \n",
       "HLA-A2902    0.619830       0.882377      0.633333       0.641257   \n",
       "HLA-B5401    0.300252       0.845946      0.727273       0.349826   \n",
       "HLA-B4002    0.456242       0.918651      0.909091       0.518720   \n",
       "HLA-B4501    0.263574       0.993333      0.800000       0.246426   \n",
       "\n",
       "           smmpmbec_cpp_auc  smmpmbec_cpp_f1  smmpmbec_cpp_tau  test_size  \n",
       "allele                                                                     \n",
       "HLA-A2601          0.927339         0.603053          0.422900       1333  \n",
       "HLA-B0801          0.940555         0.760000          0.618953        940  \n",
       "HLA-B5101          0.927621         0.395604          0.344534        854  \n",
       "HLA-B5701          0.929923         0.709091          0.569615        815  \n",
       "HLA-B0702          0.897928         0.857719          0.563960        813  \n",
       "HLA-A3101          0.851270         0.837037          0.502586        724  \n",
       "HLA-A1101          0.944205         0.882514          0.618388        723  \n",
       "HLA-A0101          0.832665         0.437811          0.428064        696  \n",
       "HLA-A0206          0.904317         0.878282          0.527571        682  \n",
       "HLA-A6802          0.965793         0.928775          0.629530        669  \n",
       "HLA-A3001          0.869935         0.692875          0.505244        660  \n",
       "HLA-A0203          0.972885         0.946746          0.583908        651  \n",
       "HLA-B3901          0.949708         0.708333          0.296639        641  \n",
       "HLA-B1501          0.936063         0.819048          0.590100        633  \n",
       "HLA-B1517          0.899620         0.652542          0.406582        582  \n",
       "HLA-A2402          0.848915         0.633416          0.537589        573  \n",
       "H-2-DB             0.884187         0.628571          0.571252        564  \n",
       "H-2-KB             0.915994         0.859967          0.589218        558  \n",
       "HLA-B3501          0.823734         0.705411          0.501630        542  \n",
       "HLA-A6801          0.944553         0.924347          0.556418        527  \n",
       "HLA-B0802          0.987214         0.105263          0.385838        509  \n",
       "HLA-B1801          0.787782         0.315789          0.215534        503  \n",
       "HLA-B5301          0.821328         0.734177          0.495308        485  \n",
       "HLA-A3301          0.907414         0.855098          0.572158        473  \n",
       "HLA-A6901          0.954868         0.689655          0.322498        470  \n",
       "HLA-B1509          0.894618         0.187500          0.364673        466  \n",
       "HLA-A3201          0.832013         0.580796          0.493693        449  \n",
       "HLA-B5801          0.905645         0.817757          0.582026        445  \n",
       "HLA-B2703               NaN         0.000000               NaN        441  \n",
       "HLA-A3002          0.754271         0.704104          0.354956        420  \n",
       "HLA-A2501          0.992701         0.727273          0.151997        416  \n",
       "HLA-A2602          0.942981         0.790244          0.531805        413  \n",
       "HLA-B4402          0.802018         0.570652          0.377900        411  \n",
       "HLA-B4001          0.862331         0.674487          0.520725        407  \n",
       "HLA-A2301          0.849381         0.745000          0.561259        391  \n",
       "Mamu-A02           0.810692         0.640000          0.478589        388  \n",
       "HLA-A8001          0.990015         0.518519          0.183987        379  \n",
       "HLA-B4403          0.796454         0.754902          0.432021        378  \n",
       "HLA-B4601               NaN         0.000000               NaN        378  \n",
       "HLA-B3801          0.949206         0.000000          0.547438        351  \n",
       "HLA-B2705          0.941682         0.304348          0.416884        314  \n",
       "HLA-A2603          0.843194         0.509091          0.306741        312  \n",
       "Mamu-A01           0.839390         0.609272          0.469975        274  \n",
       "HLA-B0803          0.973333         0.000000          0.314266        234  \n",
       "H-2-KD             0.753692         0.538462          0.365247        229  \n",
       "HLA-B1503          0.839417         0.523364          0.444917        165  \n",
       "HLA-A0202          0.882206         0.727273          0.606938        126  \n",
       "HLA-A2902          0.871795         0.655172          0.612364        118  \n",
       "HLA-B5401          0.839189         0.727273          0.333600         79  \n",
       "HLA-B4002          0.934524         0.758621          0.455548         74  \n",
       "HLA-B4501          1.000000         0.888889          0.251301         65  \n",
       "\n",
       "[51 rows x 35 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df = collections.defaultdict(list)\n",
    "predictors = validation_df_with_mhcflurry_results.columns[4:]\n",
    "\n",
    "for (allele, grouped) in validation_df_with_mhcflurry_results.groupby(\"allele\"):\n",
    "    scores_df[\"allele\"].append(allele)\n",
    "    scores_df[\"test_size\"].append(len(grouped.meas))\n",
    "    for predictor in predictors:\n",
    "        scores = make_scores(grouped.meas, grouped[predictor])\n",
    "        for (key, value) in scores.items():\n",
    "            scores_df[\"%s_%s\" % (predictor, key)].append(value)\n",
    "    \n",
    "scores_df = pandas.DataFrame(scores_df)\n",
    "scores_df.sort(\"test_size\", ascending=False, inplace=True)\n",
    "scores_df.index = scores_df.allele\n",
    "\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores_df.to_csv(\"../data/validation_scores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores_df.ix[\"HLA-A0201\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summary_df = scores_df[scores_df.allele.isin(alleles)]\n",
    "summary_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
