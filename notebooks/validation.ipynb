{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR (theano.sandbox.cuda): nvcc compiler not found on $PATH. Check your nvcc installation and try again.\n",
      "ERROR:theano.sandbox.cuda:nvcc compiler not found on $PATH. Check your nvcc installation and try again.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n",
      "gpu1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "/Users/tim/venvs/analysis-venv-2.7/lib/python2.7/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n",
      "  warnings.warn(self.msg_depr % (key, alt_key))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tim/venvs/analysis-venv-2.7/lib/python2.7/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"THEANO_FLAGS\"] = 'cuda.root=/usr/local/cuda,floatX=float32,device=gpu1,force_device=False,lib.cnmem=.75'\n",
    "\n",
    "import theano\n",
    "print(theano.config.device)\n",
    "\n",
    "import mhcflurry, seaborn, numpy, pandas, pickle, sklearn, collections, scipy, time\n",
    "import mhcflurry.dataset\n",
    "import fancyimpute, locale\n",
    "\n",
    "import sklearn.metrics\n",
    "import sklearn.cross_validation\n",
    "\n",
    "def print_full(x):\n",
    "    pandas.set_option('display.max_rows', len(x))\n",
    "    print(x)\n",
    "    pandas.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_ic50 = 50000\n",
    "min_peptides_to_consider_allele = 10\n",
    "data_dir = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_train_data = mhcflurry.dataset.Dataset.from_csv(data_dir + \"bdata.2009.mhci.public.1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 12235 peptides with <2 observations\n",
      "Dropping 9 alleles with <2 observations: ['ELA-A1', 'HLA-B2701', 'HLA-B3508', 'HLA-B44', 'HLA-E0101', 'Mamu-B04', 'Patr-A0602', 'Patr-B0901', 'Patr-B1701']\n",
      "[MICE] Completing matrix with shape (19304, 97)\n",
      "[MICE] Starting imputation round 1/300, elapsed time 0.060\n",
      "[MICE] Starting imputation round 2/300, elapsed time 6.151\n",
      "[MICE] Starting imputation round 3/300, elapsed time 12.110\n",
      "[MICE] Starting imputation round 4/300, elapsed time 16.391\n",
      "[MICE] Starting imputation round 5/300, elapsed time 20.525\n",
      "[MICE] Starting imputation round 6/300, elapsed time 25.179\n",
      "[MICE] Starting imputation round 7/300, elapsed time 31.370\n",
      "[MICE] Starting imputation round 8/300, elapsed time 36.467\n",
      "[MICE] Starting imputation round 9/300, elapsed time 42.110\n",
      "[MICE] Starting imputation round 10/300, elapsed time 47.532\n",
      "[MICE] Starting imputation round 11/300, elapsed time 52.988\n",
      "[MICE] Starting imputation round 12/300, elapsed time 58.399\n",
      "[MICE] Starting imputation round 13/300, elapsed time 64.018\n",
      "[MICE] Starting imputation round 14/300, elapsed time 69.693\n",
      "[MICE] Starting imputation round 15/300, elapsed time 75.895\n",
      "[MICE] Starting imputation round 16/300, elapsed time 81.729\n",
      "[MICE] Starting imputation round 17/300, elapsed time 86.923\n",
      "[MICE] Starting imputation round 18/300, elapsed time 92.986"
     ]
    }
   ],
   "source": [
    "imputed_train_data = all_train_data.impute_missing_values(\n",
    "    fancyimpute.MICE(n_imputations=250, n_burn_in=50),\n",
    "    min_observations_per_peptide=2,\n",
    "    min_observations_per_allele=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imputed_train_data.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "validation_df = pandas.read_csv(\"../data/combined_test_BLIND_dataset_from_kim2013.csv\")\n",
    "validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "validation_allele_counts = validation_df.allele.value_counts()\n",
    "train_allele_counts = all_train_data._df.allele.value_counts()\n",
    "print(validation_allele_counts)\n",
    "print(train_allele_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alleles = sorted(train_allele_counts.index[\n",
    "    (train_allele_counts >= min_peptides_to_consider_allele)\n",
    "    & (train_allele_counts.index.isin(validation_allele_counts.index))\n",
    "], key=lambda allele: -1 * train_allele_counts[allele])\n",
    "alleles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dropout_probabilities = [0.0, 0.5]\n",
    "embedding_output_dims_and_layer_sizes_list = [(32, [64]), (8, [4])]\n",
    "activations = [\"tanh\"]\n",
    "\n",
    "models_params_list = []\n",
    "\n",
    "for model_num in range(1):\n",
    "    for impute in [False, True]:\n",
    "        for dropout_probability in dropout_probabilities:\n",
    "            for (embedding_output_dim, layer_sizes) in embedding_output_dims_and_layer_sizes_list:\n",
    "                for activation in activations:\n",
    "                    models_params_list.append(dict(\n",
    "                        impute=impute,\n",
    "                        dropout_probability=dropout_probability,  \n",
    "                        embedding_output_dim=embedding_output_dim,\n",
    "                        layer_sizes=layer_sizes,\n",
    "                        activation=activation))\n",
    "\n",
    "print(\"%d models\" % len(models_params_list))\n",
    "models_params_explored = set.union(*[set(x) for x in models_params_list])\n",
    "models_params_explored\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_scores(ic50_y, ic50_y_pred, sample_weight=None, threshold_nm=500):     \n",
    "    y_pred = mhcflurry.regression_target.ic50_to_regression_target(ic50_y_pred, max_ic50)\n",
    "    try:\n",
    "        auc = sklearn.metrics.roc_auc_score(ic50_y <= threshold_nm, y_pred, sample_weight=sample_weight)\n",
    "    except ValueError:\n",
    "        auc = numpy.nan\n",
    "    try:\n",
    "        f1 = sklearn.metrics.f1_score(ic50_y <= threshold_nm, ic50_y_pred <= threshold_nm, sample_weight=sample_weight)\n",
    "    except ValueError:\n",
    "        f1 = numpy.nan\n",
    "    try:\n",
    "        tau = scipy.stats.kendalltau(ic50_y_pred, ic50_y)[0]\n",
    "    except ValueError:\n",
    "        tau = numpy.nan\n",
    "    \n",
    "    return dict(\n",
    "        auc=auc,\n",
    "        f1=f1,\n",
    "        tau=tau,\n",
    "    )    \n",
    "\n",
    "def mean_with_std(grouped_column, decimals=3):\n",
    "    pattern = \"%%0.%df\" % decimals\n",
    "    return pandas.Series([\n",
    "        (pattern + \" +/ \" + pattern) % (m, s) if not pandas.isnull(s) else pattern % m\n",
    "        for (m, s) in zip(grouped_column.mean(), grouped_column.std())\n",
    "    ], index = grouped_column.mean().index)\n",
    "\n",
    "def allele_data_to_df(data):\n",
    "    d = data._asdict()\n",
    "    d[\"X_index\"] = [x for x in d[\"X_index\"]]\n",
    "    d[\"X_binary\"] = [x for x in d[\"X_binary\"]]\n",
    "    df = pandas.DataFrame(d).set_index('peptides')\n",
    "    return df\n",
    "\n",
    "def make_2d_array(thing):\n",
    "    return numpy.array([list(x) for x in thing])\n",
    "\n",
    "def df_to_allele_data(df):\n",
    "    d = dict((col, df[col].values) for col in df)\n",
    "    d[\"X_index\"] = make_2d_array(d[\"X_index\"])\n",
    "    (d[\"max_ic50\"],) = list(df.max_ic50.unique())\n",
    "    return mhcflurry.data.AlleleData(peptides = df.index.values, **d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "models_and_scores = {}\n",
    "validation_df_with_mhcflurry = validation_df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train and test models, adding columns to validation_df_with_mhcflurry\n",
    "pandas.DataFrame(models_params_list).to_csv(\"../data/validation_models.csv\", index=False)\n",
    "\n",
    "def make_and_fit_model(allele, original_params):\n",
    "    params = dict(original_params)\n",
    "    impute = params[\"impute\"]\n",
    "    del params[\"impute\"]\n",
    "    model = mhcflurry.Class1BindingPredictor.from_hyperparameters(max_ic50=max_ic50, **params)\n",
    "    print(\"Fitting model for allele %s (%d + %d): %s\" % (\n",
    "            allele,\n",
    "            len(all_train_data.groupby_allele_dictionary()[allele]),\n",
    "            len(imputed_train_data.groupby_allele_dictionary()[allele]),\n",
    "            str(original_params)))\n",
    "    t = -time.time()\n",
    "    model.fit_dataset(\n",
    "        all_train_data.groupby_allele_dictionary()[allele],\n",
    "        pretraining_dataset=imputed_train_data.groupby_allele_dictionary()[allele] if impute else None,\n",
    "        verbose=False,\n",
    "        batch_size=128,\n",
    "        n_training_epochs=250)\n",
    "    t += time.time()\n",
    "    print(\"Trained in %d sec\" % t)\n",
    "    return model\n",
    "\n",
    "for (i, allele) in enumerate(alleles):\n",
    "    if allele not in validation_df_with_mhcflurry.allele.unique():\n",
    "        print(\"Skipping allele %s: not in test set\" % allele)\n",
    "        continue\n",
    "    if allele in models_and_scores:\n",
    "        print(\"Skipping allele %s: already done\" % allele)\n",
    "        continue\n",
    "    values_for_allele = []\n",
    "    for (j, params) in enumerate(models_params_list):\n",
    "        print(\"Allele %d model %d\" % (i, j))\n",
    "        model = make_and_fit_model(allele, params)\n",
    "        predictions = model.predict(\n",
    "            list(validation_df_with_mhcflurry.ix[validation_df_with_mhcflurry.allele == allele].peptide))\n",
    "        print(\"test set size: %d\" % len(predictions))\n",
    "        validation_df_with_mhcflurry.loc[(validation_df_with_mhcflurry.allele == allele),\n",
    "                                         (\"mhcflurry %d\" % j)] = predictions\n",
    "        scores = make_scores(validation_df_with_mhcflurry.ix[validation_df.allele == allele].meas,\n",
    "                            predictions)\n",
    "        print(scores)\n",
    "        values_for_allele.append((params, scores))\n",
    "        \n",
    "    models_and_scores[allele] = values_for_allele\n",
    "    \n",
    "    # Write out all data after each allele.\n",
    "    validation_df_with_mhcflurry_results = validation_df_with_mhcflurry.ix[validation_df_with_mhcflurry.allele.isin(models_and_scores)]\n",
    "    validation_df_with_mhcflurry_results.to_csv(\"../data/validation_predictions_full.csv\", index=False)\n",
    "    \n",
    "    scores_df = collections.defaultdict(list)\n",
    "    predictors = validation_df_with_mhcflurry_results.columns[4:]\n",
    "\n",
    "    for (allele, grouped) in validation_df_with_mhcflurry_results.groupby(\"allele\"):\n",
    "        scores_df[\"allele\"].append(allele)\n",
    "        scores_df[\"test_size\"].append(len(grouped.meas))\n",
    "        for predictor in predictors:\n",
    "            scores = make_scores(grouped.meas, grouped[predictor])\n",
    "            for (key, value) in scores.items():\n",
    "                scores_df[\"%s_%s\" % (predictor, key)].append(value)\n",
    "\n",
    "    scores_df = pandas.DataFrame(scores_df)\n",
    "    scores_df[\"train_size\"] = [\n",
    "        len(all_train_data.groupby_allele_dictionary()[a])\n",
    "        for a in scores_df.allele\n",
    "    ]\n",
    "\n",
    "    scores_df.index = scores_df.allele\n",
    "    scores_df.to_csv(\"../data/validation_scores.csv\", index=False)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_full(scores_df[[\"train_size\", \"test_size\"]].sort(\"train_size\", inplace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
