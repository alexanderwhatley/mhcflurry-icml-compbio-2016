{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 1: GeForce GTX TITAN X (CNMeM is enabled with initial size: 75.0% of memory, cuDNN 5004)\n",
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"THEANO_FLAGS\"] = 'cuda.root=/usr/local/cuda,floatX=float32,device=gpu1,force_device=False,lib.cnmem=.75'\n",
    "\n",
    "import theano\n",
    "print(theano.config.device)\n",
    "\n",
    "import mhcflurry, seaborn, numpy, pandas, pickle, sklearn, collections, scipy, time\n",
    "import mhcflurry.data\n",
    "import mhcflurry.imputation\n",
    "import fancyimpute, locale\n",
    "\n",
    "import sklearn.metrics\n",
    "import sklearn.cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_ic50 = 50000\n",
    "min_peptides_to_consider_allele = 10\n",
    "data_dir = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_train_data = mhcflurry.data.load_allele_datasets(data_dir + \"bdata.2009.mhci.public.1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 12235 peptides with <2 observations\n",
      "Dropping 9 alleles with <10 observations: ['ELA-A1', 'HLA-B2701', 'HLA-B3508', 'HLA-B44', 'HLA-E0101', 'Mamu-B04', 'Patr-A0602', 'Patr-B0901', 'Patr-B1701']\n",
      "[MICE] Completing matrix with shape (19304, 97)\n",
      "[MICE] Starting imputation round 1/30, elapsed time 0.023\n",
      "[MICE] Starting imputation round 2/30, elapsed time 1.485\n",
      "[MICE] Starting imputation round 3/30, elapsed time 2.801\n",
      "[MICE] Starting imputation round 4/30, elapsed time 4.116\n",
      "[MICE] Starting imputation round 5/30, elapsed time 5.425\n",
      "[MICE] Starting imputation round 6/30, elapsed time 6.738\n",
      "[MICE] Starting imputation round 7/30, elapsed time 8.054\n",
      "[MICE] Starting imputation round 8/30, elapsed time 9.368\n",
      "[MICE] Starting imputation round 9/30, elapsed time 10.672\n",
      "[MICE] Starting imputation round 10/30, elapsed time 11.988\n",
      "[MICE] Starting imputation round 11/30, elapsed time 13.300\n",
      "[MICE] Starting imputation round 12/30, elapsed time 14.606\n",
      "[MICE] Starting imputation round 13/30, elapsed time 15.922\n",
      "[MICE] Starting imputation round 14/30, elapsed time 17.235\n",
      "[MICE] Starting imputation round 15/30, elapsed time 18.539\n",
      "[MICE] Starting imputation round 16/30, elapsed time 19.852\n",
      "[MICE] Starting imputation round 17/30, elapsed time 21.166\n",
      "[MICE] Starting imputation round 18/30, elapsed time 22.470\n",
      "[MICE] Starting imputation round 19/30, elapsed time 23.782\n",
      "[MICE] Starting imputation round 20/30, elapsed time 25.097\n",
      "[MICE] Starting imputation round 21/30, elapsed time 26.405\n",
      "[MICE] Starting imputation round 22/30, elapsed time 27.722\n",
      "[MICE] Starting imputation round 23/30, elapsed time 29.037\n",
      "[MICE] Starting imputation round 24/30, elapsed time 30.345\n",
      "[MICE] Starting imputation round 25/30, elapsed time 31.657\n",
      "[MICE] Starting imputation round 26/30, elapsed time 32.974\n",
      "[MICE] Starting imputation round 27/30, elapsed time 34.285\n",
      "[MICE] Starting imputation round 28/30, elapsed time 35.593\n",
      "[MICE] Starting imputation round 29/30, elapsed time 36.905\n",
      "[MICE] Starting imputation round 30/30, elapsed time 38.211\n"
     ]
    }
   ],
   "source": [
    "imputed_train_data = mhcflurry.imputation.create_imputed_datasets(\n",
    "    all_train_data,\n",
    "    fancyimpute.MICE(n_imputations=25, n_burn_in=5),\n",
    "    min_observations_per_peptide=2,\n",
    "    min_observations_per_allele=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>allele</th>\n",
       "      <th>peptide</th>\n",
       "      <th>length</th>\n",
       "      <th>meas</th>\n",
       "      <th>netmhc</th>\n",
       "      <th>netmhcpan</th>\n",
       "      <th>smmpmbec_cpp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAACNVATA</td>\n",
       "      <td>9</td>\n",
       "      <td>657.657837</td>\n",
       "      <td>154.881662</td>\n",
       "      <td>711.213514</td>\n",
       "      <td>438.530698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAFEFVYV</td>\n",
       "      <td>8</td>\n",
       "      <td>30831.879502</td>\n",
       "      <td>6456.542290</td>\n",
       "      <td>785.235635</td>\n",
       "      <td>10351.421667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAFVNDYSL</td>\n",
       "      <td>9</td>\n",
       "      <td>77.446180</td>\n",
       "      <td>17.458222</td>\n",
       "      <td>7.516229</td>\n",
       "      <td>28.054336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAIANQAAV</td>\n",
       "      <td>9</td>\n",
       "      <td>1.999862</td>\n",
       "      <td>9.638290</td>\n",
       "      <td>9.749896</td>\n",
       "      <td>25.703958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAIANQAVV</td>\n",
       "      <td>9</td>\n",
       "      <td>1.517050</td>\n",
       "      <td>8.550667</td>\n",
       "      <td>8.336812</td>\n",
       "      <td>28.773984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAIENYVRF</td>\n",
       "      <td>9</td>\n",
       "      <td>37.844258</td>\n",
       "      <td>252.348077</td>\n",
       "      <td>114.815362</td>\n",
       "      <td>187.068214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAINFITTM</td>\n",
       "      <td>9</td>\n",
       "      <td>3.155005</td>\n",
       "      <td>199.986187</td>\n",
       "      <td>389.045145</td>\n",
       "      <td>200.909281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAIPAPPPI</td>\n",
       "      <td>9</td>\n",
       "      <td>3243.396173</td>\n",
       "      <td>1059.253725</td>\n",
       "      <td>493.173804</td>\n",
       "      <td>295.120923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAKLNRPPL</td>\n",
       "      <td>9</td>\n",
       "      <td>654.636174</td>\n",
       "      <td>66.374307</td>\n",
       "      <td>77.268059</td>\n",
       "      <td>38.459178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AALDMVDAL</td>\n",
       "      <td>9</td>\n",
       "      <td>229.614865</td>\n",
       "      <td>547.015963</td>\n",
       "      <td>597.035287</td>\n",
       "      <td>225.423921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AALQHLRSI</td>\n",
       "      <td>9</td>\n",
       "      <td>905.732601</td>\n",
       "      <td>1686.553025</td>\n",
       "      <td>2032.357011</td>\n",
       "      <td>698.232404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AALVRLTAL</td>\n",
       "      <td>9</td>\n",
       "      <td>1106.623784</td>\n",
       "      <td>435.511874</td>\n",
       "      <td>214.783047</td>\n",
       "      <td>378.442585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAMVPTGSL</td>\n",
       "      <td>9</td>\n",
       "      <td>1836.538343</td>\n",
       "      <td>4055.085354</td>\n",
       "      <td>5176.068320</td>\n",
       "      <td>1545.254440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AANSPWAPV</td>\n",
       "      <td>9</td>\n",
       "      <td>4325.138310</td>\n",
       "      <td>903.649474</td>\n",
       "      <td>1023.292992</td>\n",
       "      <td>557.185749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAPSGAAPL</td>\n",
       "      <td>9</td>\n",
       "      <td>2.844461</td>\n",
       "      <td>97.948999</td>\n",
       "      <td>501.187234</td>\n",
       "      <td>822.242650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAPVSEPTV</td>\n",
       "      <td>9</td>\n",
       "      <td>106.905488</td>\n",
       "      <td>2654.605562</td>\n",
       "      <td>1918.668741</td>\n",
       "      <td>1870.682140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAVANGFAA</td>\n",
       "      <td>9</td>\n",
       "      <td>83.368118</td>\n",
       "      <td>205.116218</td>\n",
       "      <td>228.559880</td>\n",
       "      <td>200.447203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAVLLGAPV</td>\n",
       "      <td>9</td>\n",
       "      <td>373.250158</td>\n",
       "      <td>320.626932</td>\n",
       "      <td>623.734835</td>\n",
       "      <td>286.417797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAVQLLFPA</td>\n",
       "      <td>9</td>\n",
       "      <td>347.536161</td>\n",
       "      <td>2494.594727</td>\n",
       "      <td>5152.286446</td>\n",
       "      <td>679.203633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAVTAGVAL</td>\n",
       "      <td>9</td>\n",
       "      <td>229.614865</td>\n",
       "      <td>1552.387010</td>\n",
       "      <td>2172.701179</td>\n",
       "      <td>1927.524913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAVVNRSLV</td>\n",
       "      <td>9</td>\n",
       "      <td>8.609938</td>\n",
       "      <td>26.302680</td>\n",
       "      <td>29.580125</td>\n",
       "      <td>63.826349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AAYVPADAV</td>\n",
       "      <td>9</td>\n",
       "      <td>331.894458</td>\n",
       "      <td>1757.923614</td>\n",
       "      <td>3206.269325</td>\n",
       "      <td>595.662144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>ACMVNLERL</td>\n",
       "      <td>9</td>\n",
       "      <td>737.904230</td>\n",
       "      <td>239.331576</td>\n",
       "      <td>679.203633</td>\n",
       "      <td>444.631267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>ADLVNHPPV</td>\n",
       "      <td>9</td>\n",
       "      <td>558.470195</td>\n",
       "      <td>1694.337800</td>\n",
       "      <td>1857.804455</td>\n",
       "      <td>325.087297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AEMEEALKG</td>\n",
       "      <td>9</td>\n",
       "      <td>78523.563461</td>\n",
       "      <td>41304.750199</td>\n",
       "      <td>46989.410861</td>\n",
       "      <td>440554.863507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AFFAFRYV</td>\n",
       "      <td>8</td>\n",
       "      <td>87902.251683</td>\n",
       "      <td>18967.059212</td>\n",
       "      <td>12882.495517</td>\n",
       "      <td>7816.278046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AGLDNKFYL</td>\n",
       "      <td>9</td>\n",
       "      <td>659.173895</td>\n",
       "      <td>357.272838</td>\n",
       "      <td>202.768272</td>\n",
       "      <td>260.015956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AGLLFVLL</td>\n",
       "      <td>8</td>\n",
       "      <td>87902.251683</td>\n",
       "      <td>14996.848355</td>\n",
       "      <td>9885.530947</td>\n",
       "      <td>26242.185434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AGLQVINL</td>\n",
       "      <td>8</td>\n",
       "      <td>59429.215862</td>\n",
       "      <td>25061.092530</td>\n",
       "      <td>12560.299637</td>\n",
       "      <td>35399.734108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>H-2-DB</td>\n",
       "      <td>AGLVSFNFL</td>\n",
       "      <td>9</td>\n",
       "      <td>210.862815</td>\n",
       "      <td>620.869034</td>\n",
       "      <td>2285.598803</td>\n",
       "      <td>1811.340093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27650</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YNTVCVIW</td>\n",
       "      <td>8</td>\n",
       "      <td>909.913273</td>\n",
       "      <td>734.513868</td>\n",
       "      <td>10023.052381</td>\n",
       "      <td>48.865236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27651</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YRHDGGNVL</td>\n",
       "      <td>9</td>\n",
       "      <td>78342.964277</td>\n",
       "      <td>1355.189412</td>\n",
       "      <td>22233.098907</td>\n",
       "      <td>1409.288798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27652</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YSEGQYMNTP</td>\n",
       "      <td>10</td>\n",
       "      <td>7362.070975</td>\n",
       "      <td>2958.012467</td>\n",
       "      <td>8709.635900</td>\n",
       "      <td>4477.133042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27653</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YSEVALNVTES</td>\n",
       "      <td>11</td>\n",
       "      <td>7709.034691</td>\n",
       "      <td>1682.674061</td>\n",
       "      <td>5105.050000</td>\n",
       "      <td>51.522864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27654</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YSLVFVILM</td>\n",
       "      <td>9</td>\n",
       "      <td>1256.029964</td>\n",
       "      <td>17.498467</td>\n",
       "      <td>49.431069</td>\n",
       "      <td>6.324119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27655</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YSQIGAGVY</td>\n",
       "      <td>9</td>\n",
       "      <td>1.999862</td>\n",
       "      <td>17.864876</td>\n",
       "      <td>15.346170</td>\n",
       "      <td>16.292960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27656</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YSRDLICEQS</td>\n",
       "      <td>10</td>\n",
       "      <td>1059.253725</td>\n",
       "      <td>1297.179271</td>\n",
       "      <td>3863.669771</td>\n",
       "      <td>220.292646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27657</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YSYKAFIKY</td>\n",
       "      <td>9</td>\n",
       "      <td>2.317395</td>\n",
       "      <td>4.285485</td>\n",
       "      <td>3.090295</td>\n",
       "      <td>1.778279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27658</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YSYKAFIKYP</td>\n",
       "      <td>10</td>\n",
       "      <td>4830.588020</td>\n",
       "      <td>1199.499303</td>\n",
       "      <td>1753.880502</td>\n",
       "      <td>3111.716337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27659</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YSYKAFIKYPE</td>\n",
       "      <td>11</td>\n",
       "      <td>4897.788194</td>\n",
       "      <td>671.428853</td>\n",
       "      <td>2666.858665</td>\n",
       "      <td>42.559841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27660</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTAFTLPSV</td>\n",
       "      <td>9</td>\n",
       "      <td>101.624869</td>\n",
       "      <td>32.210688</td>\n",
       "      <td>52.844525</td>\n",
       "      <td>91.833260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27661</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTAFTLPSVN</td>\n",
       "      <td>10</td>\n",
       "      <td>2529.297996</td>\n",
       "      <td>236.591970</td>\n",
       "      <td>1406.047524</td>\n",
       "      <td>14.487719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27662</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTDGSCNKQS</td>\n",
       "      <td>10</td>\n",
       "      <td>11091.748153</td>\n",
       "      <td>4345.102242</td>\n",
       "      <td>11246.049740</td>\n",
       "      <td>588.843655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27663</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTGDFDSVI</td>\n",
       "      <td>9</td>\n",
       "      <td>215.278173</td>\n",
       "      <td>354.813389</td>\n",
       "      <td>42.461956</td>\n",
       "      <td>40.550854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27664</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTPKVVGG</td>\n",
       "      <td>8</td>\n",
       "      <td>70145.529842</td>\n",
       "      <td>17947.336268</td>\n",
       "      <td>13995.873226</td>\n",
       "      <td>3140.508694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27665</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTPKVVGGIGG</td>\n",
       "      <td>11</td>\n",
       "      <td>70145.529842</td>\n",
       "      <td>9418.895965</td>\n",
       "      <td>10046.157903</td>\n",
       "      <td>387.257645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27666</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTSGPGIR</td>\n",
       "      <td>8</td>\n",
       "      <td>16865.530254</td>\n",
       "      <td>619.441075</td>\n",
       "      <td>2349.632821</td>\n",
       "      <td>80.723503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27667</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTSGPGIRYP</td>\n",
       "      <td>10</td>\n",
       "      <td>7128.530301</td>\n",
       "      <td>783.429643</td>\n",
       "      <td>5176.068320</td>\n",
       "      <td>1270.574105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27668</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTSGPGTRYP</td>\n",
       "      <td>10</td>\n",
       "      <td>3097.419299</td>\n",
       "      <td>483.058802</td>\n",
       "      <td>5420.008904</td>\n",
       "      <td>797.994687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27669</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTSGPGTRYPM</td>\n",
       "      <td>11</td>\n",
       "      <td>25.941794</td>\n",
       "      <td>39.174188</td>\n",
       "      <td>7.744618</td>\n",
       "      <td>76.383578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27670</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTTGGTSR</td>\n",
       "      <td>8</td>\n",
       "      <td>70145.529842</td>\n",
       "      <td>357.272838</td>\n",
       "      <td>5701.642723</td>\n",
       "      <td>100.230524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27671</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTTGGTSRN</td>\n",
       "      <td>9</td>\n",
       "      <td>36897.759857</td>\n",
       "      <td>186.637969</td>\n",
       "      <td>6025.595861</td>\n",
       "      <td>119.674053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27672</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTTTGASRN</td>\n",
       "      <td>9</td>\n",
       "      <td>587.489353</td>\n",
       "      <td>135.518941</td>\n",
       "      <td>2844.461107</td>\n",
       "      <td>87.096359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27673</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTYEAYVRYP</td>\n",
       "      <td>10</td>\n",
       "      <td>2624.218543</td>\n",
       "      <td>1552.387010</td>\n",
       "      <td>3872.576449</td>\n",
       "      <td>1954.339456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27674</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YTYEAYVRYPE</td>\n",
       "      <td>11</td>\n",
       "      <td>1905.460718</td>\n",
       "      <td>864.967919</td>\n",
       "      <td>6998.419960</td>\n",
       "      <td>32.734069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27675</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YVADALAAF</td>\n",
       "      <td>9</td>\n",
       "      <td>15.381546</td>\n",
       "      <td>453.941617</td>\n",
       "      <td>71.285303</td>\n",
       "      <td>108.143395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27676</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YVFPVIFSK</td>\n",
       "      <td>9</td>\n",
       "      <td>39264.493540</td>\n",
       "      <td>2098.939884</td>\n",
       "      <td>5610.479760</td>\n",
       "      <td>901.571138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27677</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YVFPVIFSR</td>\n",
       "      <td>9</td>\n",
       "      <td>36728.230050</td>\n",
       "      <td>2333.458062</td>\n",
       "      <td>10046.157903</td>\n",
       "      <td>2600.159563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27678</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YVPCHIRQI</td>\n",
       "      <td>9</td>\n",
       "      <td>10764.652136</td>\n",
       "      <td>21134.890398</td>\n",
       "      <td>6039.486294</td>\n",
       "      <td>10568.175092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27679</th>\n",
       "      <td>Mamu-A02</td>\n",
       "      <td>YVVQMLARL</td>\n",
       "      <td>9</td>\n",
       "      <td>152.405275</td>\n",
       "      <td>232.273680</td>\n",
       "      <td>739.605275</td>\n",
       "      <td>105.681751</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27680 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         allele      peptide  length          meas        netmhc  \\\n",
       "0        H-2-DB    AAACNVATA       9    657.657837    154.881662   \n",
       "1        H-2-DB     AAFEFVYV       8  30831.879502   6456.542290   \n",
       "2        H-2-DB    AAFVNDYSL       9     77.446180     17.458222   \n",
       "3        H-2-DB    AAIANQAAV       9      1.999862      9.638290   \n",
       "4        H-2-DB    AAIANQAVV       9      1.517050      8.550667   \n",
       "5        H-2-DB    AAIENYVRF       9     37.844258    252.348077   \n",
       "6        H-2-DB    AAINFITTM       9      3.155005    199.986187   \n",
       "7        H-2-DB    AAIPAPPPI       9   3243.396173   1059.253725   \n",
       "8        H-2-DB    AAKLNRPPL       9    654.636174     66.374307   \n",
       "9        H-2-DB    AALDMVDAL       9    229.614865    547.015963   \n",
       "10       H-2-DB    AALQHLRSI       9    905.732601   1686.553025   \n",
       "11       H-2-DB    AALVRLTAL       9   1106.623784    435.511874   \n",
       "12       H-2-DB    AAMVPTGSL       9   1836.538343   4055.085354   \n",
       "13       H-2-DB    AANSPWAPV       9   4325.138310    903.649474   \n",
       "14       H-2-DB    AAPSGAAPL       9      2.844461     97.948999   \n",
       "15       H-2-DB    AAPVSEPTV       9    106.905488   2654.605562   \n",
       "16       H-2-DB    AAVANGFAA       9     83.368118    205.116218   \n",
       "17       H-2-DB    AAVLLGAPV       9    373.250158    320.626932   \n",
       "18       H-2-DB    AAVQLLFPA       9    347.536161   2494.594727   \n",
       "19       H-2-DB    AAVTAGVAL       9    229.614865   1552.387010   \n",
       "20       H-2-DB    AAVVNRSLV       9      8.609938     26.302680   \n",
       "21       H-2-DB    AAYVPADAV       9    331.894458   1757.923614   \n",
       "22       H-2-DB    ACMVNLERL       9    737.904230    239.331576   \n",
       "23       H-2-DB    ADLVNHPPV       9    558.470195   1694.337800   \n",
       "24       H-2-DB    AEMEEALKG       9  78523.563461  41304.750199   \n",
       "25       H-2-DB     AFFAFRYV       8  87902.251683  18967.059212   \n",
       "26       H-2-DB    AGLDNKFYL       9    659.173895    357.272838   \n",
       "27       H-2-DB     AGLLFVLL       8  87902.251683  14996.848355   \n",
       "28       H-2-DB     AGLQVINL       8  59429.215862  25061.092530   \n",
       "29       H-2-DB    AGLVSFNFL       9    210.862815    620.869034   \n",
       "...         ...          ...     ...           ...           ...   \n",
       "27650  Mamu-A02     YNTVCVIW       8    909.913273    734.513868   \n",
       "27651  Mamu-A02    YRHDGGNVL       9  78342.964277   1355.189412   \n",
       "27652  Mamu-A02   YSEGQYMNTP      10   7362.070975   2958.012467   \n",
       "27653  Mamu-A02  YSEVALNVTES      11   7709.034691   1682.674061   \n",
       "27654  Mamu-A02    YSLVFVILM       9   1256.029964     17.498467   \n",
       "27655  Mamu-A02    YSQIGAGVY       9      1.999862     17.864876   \n",
       "27656  Mamu-A02   YSRDLICEQS      10   1059.253725   1297.179271   \n",
       "27657  Mamu-A02    YSYKAFIKY       9      2.317395      4.285485   \n",
       "27658  Mamu-A02   YSYKAFIKYP      10   4830.588020   1199.499303   \n",
       "27659  Mamu-A02  YSYKAFIKYPE      11   4897.788194    671.428853   \n",
       "27660  Mamu-A02    YTAFTLPSV       9    101.624869     32.210688   \n",
       "27661  Mamu-A02   YTAFTLPSVN      10   2529.297996    236.591970   \n",
       "27662  Mamu-A02   YTDGSCNKQS      10  11091.748153   4345.102242   \n",
       "27663  Mamu-A02    YTGDFDSVI       9    215.278173    354.813389   \n",
       "27664  Mamu-A02     YTPKVVGG       8  70145.529842  17947.336268   \n",
       "27665  Mamu-A02  YTPKVVGGIGG      11  70145.529842   9418.895965   \n",
       "27666  Mamu-A02     YTSGPGIR       8  16865.530254    619.441075   \n",
       "27667  Mamu-A02   YTSGPGIRYP      10   7128.530301    783.429643   \n",
       "27668  Mamu-A02   YTSGPGTRYP      10   3097.419299    483.058802   \n",
       "27669  Mamu-A02  YTSGPGTRYPM      11     25.941794     39.174188   \n",
       "27670  Mamu-A02     YTTGGTSR       8  70145.529842    357.272838   \n",
       "27671  Mamu-A02    YTTGGTSRN       9  36897.759857    186.637969   \n",
       "27672  Mamu-A02    YTTTGASRN       9    587.489353    135.518941   \n",
       "27673  Mamu-A02   YTYEAYVRYP      10   2624.218543   1552.387010   \n",
       "27674  Mamu-A02  YTYEAYVRYPE      11   1905.460718    864.967919   \n",
       "27675  Mamu-A02    YVADALAAF       9     15.381546    453.941617   \n",
       "27676  Mamu-A02    YVFPVIFSK       9  39264.493540   2098.939884   \n",
       "27677  Mamu-A02    YVFPVIFSR       9  36728.230050   2333.458062   \n",
       "27678  Mamu-A02    YVPCHIRQI       9  10764.652136  21134.890398   \n",
       "27679  Mamu-A02    YVVQMLARL       9    152.405275    232.273680   \n",
       "\n",
       "          netmhcpan   smmpmbec_cpp  \n",
       "0        711.213514     438.530698  \n",
       "1        785.235635   10351.421667  \n",
       "2          7.516229      28.054336  \n",
       "3          9.749896      25.703958  \n",
       "4          8.336812      28.773984  \n",
       "5        114.815362     187.068214  \n",
       "6        389.045145     200.909281  \n",
       "7        493.173804     295.120923  \n",
       "8         77.268059      38.459178  \n",
       "9        597.035287     225.423921  \n",
       "10      2032.357011     698.232404  \n",
       "11       214.783047     378.442585  \n",
       "12      5176.068320    1545.254440  \n",
       "13      1023.292992     557.185749  \n",
       "14       501.187234     822.242650  \n",
       "15      1918.668741    1870.682140  \n",
       "16       228.559880     200.447203  \n",
       "17       623.734835     286.417797  \n",
       "18      5152.286446     679.203633  \n",
       "19      2172.701179    1927.524913  \n",
       "20        29.580125      63.826349  \n",
       "21      3206.269325     595.662144  \n",
       "22       679.203633     444.631267  \n",
       "23      1857.804455     325.087297  \n",
       "24     46989.410861  440554.863507  \n",
       "25     12882.495517    7816.278046  \n",
       "26       202.768272     260.015956  \n",
       "27      9885.530947   26242.185434  \n",
       "28     12560.299637   35399.734108  \n",
       "29      2285.598803    1811.340093  \n",
       "...             ...            ...  \n",
       "27650  10023.052381      48.865236  \n",
       "27651  22233.098907    1409.288798  \n",
       "27652   8709.635900    4477.133042  \n",
       "27653   5105.050000      51.522864  \n",
       "27654     49.431069       6.324119  \n",
       "27655     15.346170      16.292960  \n",
       "27656   3863.669771     220.292646  \n",
       "27657      3.090295       1.778279  \n",
       "27658   1753.880502    3111.716337  \n",
       "27659   2666.858665      42.559841  \n",
       "27660     52.844525      91.833260  \n",
       "27661   1406.047524      14.487719  \n",
       "27662  11246.049740     588.843655  \n",
       "27663     42.461956      40.550854  \n",
       "27664  13995.873226    3140.508694  \n",
       "27665  10046.157903     387.257645  \n",
       "27666   2349.632821      80.723503  \n",
       "27667   5176.068320    1270.574105  \n",
       "27668   5420.008904     797.994687  \n",
       "27669      7.744618      76.383578  \n",
       "27670   5701.642723     100.230524  \n",
       "27671   6025.595861     119.674053  \n",
       "27672   2844.461107      87.096359  \n",
       "27673   3872.576449    1954.339456  \n",
       "27674   6998.419960      32.734069  \n",
       "27675     71.285303     108.143395  \n",
       "27676   5610.479760     901.571138  \n",
       "27677  10046.157903    2600.159563  \n",
       "27678   6039.486294   10568.175092  \n",
       "27679    739.605275     105.681751  \n",
       "\n",
       "[27680 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df = pandas.read_csv(\"../data/combined_test_BLIND_dataset_from_kim2013.csv\")\n",
    "validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HLA-A0201    2126\n",
       "HLA-A2601    1333\n",
       "HLA-B0801     940\n",
       "HLA-B5101     854\n",
       "HLA-B5701     815\n",
       "HLA-B0702     813\n",
       "HLA-A0301     811\n",
       "HLA-A3101     724\n",
       "HLA-A1101     723\n",
       "HLA-A0101     696\n",
       "HLA-A0206     682\n",
       "HLA-A6802     669\n",
       "HLA-A3001     660\n",
       "HLA-A0203     651\n",
       "HLA-B3901     641\n",
       "HLA-B1501     633\n",
       "HLA-B1517     582\n",
       "HLA-A2402     573\n",
       "H-2-DB        564\n",
       "H-2-KB        558\n",
       "HLA-B3501     542\n",
       "HLA-A6801     527\n",
       "HLA-B0802     509\n",
       "HLA-B1801     503\n",
       "HLA-B5301     485\n",
       "HLA-A3301     473\n",
       "HLA-A6901     470\n",
       "HLA-B1509     466\n",
       "HLA-A3201     449\n",
       "HLA-B5801     445\n",
       "HLA-B2703     441\n",
       "HLA-A3002     420\n",
       "HLA-A2501     416\n",
       "HLA-A2602     413\n",
       "HLA-B4402     411\n",
       "HLA-B4001     407\n",
       "HLA-A2301     391\n",
       "Mamu-A02      388\n",
       "HLA-A8001     379\n",
       "HLA-B4601     378\n",
       "HLA-B4403     378\n",
       "HLA-B3801     351\n",
       "HLA-B2705     314\n",
       "HLA-A2603     312\n",
       "Mamu-A01      274\n",
       "HLA-B0803     234\n",
       "H-2-KD        229\n",
       "HLA-B1503     165\n",
       "HLA-A0202     126\n",
       "HLA-A2902     118\n",
       "HLA-B5401      79\n",
       "HLA-B4002      74\n",
       "HLA-B4501      65\n",
       "Name: allele, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_allele_counts = validation_df.allele.value_counts()\n",
    "validation_allele_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['HLA-A0201',\n",
       " 'HLA-A0301',\n",
       " 'HLA-A0203',\n",
       " 'HLA-A1101',\n",
       " 'HLA-A0206',\n",
       " 'HLA-A3101',\n",
       " 'HLA-A6802',\n",
       " 'HLA-A0202',\n",
       " 'HLA-A0101',\n",
       " 'HLA-B0702',\n",
       " 'H-2-KB',\n",
       " 'H-2-DB',\n",
       " 'HLA-B1501',\n",
       " 'HLA-A6801',\n",
       " 'HLA-A3301',\n",
       " 'HLA-B2705',\n",
       " 'HLA-A2601',\n",
       " 'HLA-B4001',\n",
       " 'HLA-B5801',\n",
       " 'HLA-A2402',\n",
       " 'HLA-A2902',\n",
       " 'HLA-B3501',\n",
       " 'HLA-B0801',\n",
       " 'Mamu-A01',\n",
       " 'HLA-A6901',\n",
       " 'HLA-B1801',\n",
       " 'HLA-A3001',\n",
       " 'HLA-A2301',\n",
       " 'HLA-B5701',\n",
       " 'HLA-B5101',\n",
       " 'HLA-B4402',\n",
       " 'HLA-A3002',\n",
       " 'HLA-B4601',\n",
       " 'HLA-B5401',\n",
       " 'HLA-B5301',\n",
       " 'Mamu-A02',\n",
       " 'HLA-B4403',\n",
       " 'HLA-B4501',\n",
       " 'HLA-B3901',\n",
       " 'HLA-B4002',\n",
       " 'HLA-B1517',\n",
       " 'HLA-A8001',\n",
       " 'HLA-A3201',\n",
       " 'HLA-A2501',\n",
       " 'HLA-B0802',\n",
       " 'H-2-KD',\n",
       " 'HLA-B2703',\n",
       " 'HLA-B1503',\n",
       " 'HLA-B1509',\n",
       " 'HLA-B0803',\n",
       " 'HLA-A2603',\n",
       " 'HLA-A2602',\n",
       " 'HLA-B3801']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alleles = sorted((\n",
    "    allele for allele in all_train_data\n",
    "    if (len(set(all_train_data[allele].original_peptides)) >= min_peptides_to_consider_allele)\n",
    "                 and allele in validation_allele_counts.index),\n",
    "                 key=lambda allele: -1 * len(set(all_train_data[allele].original_peptides)))\n",
    "alleles\n",
    "\n",
    "print(len(alleles))\n",
    "alleles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 models\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'activation',\n",
       " 'dropout_probability',\n",
       " 'embedding_output_dim',\n",
       " 'impute',\n",
       " 'layer_sizes'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout_probabilities = [0.0, 0.5]\n",
    "embedding_output_dims_and_layer_sizes_list = [(32, [64]), (8, [4])]\n",
    "activations = [\"tanh\"]\n",
    "\n",
    "models_params_list = []\n",
    "\n",
    "for model_num in range(10):\n",
    "    for impute in [False, True]:\n",
    "        for dropout_probability in dropout_probabilities:\n",
    "            for (embedding_output_dim, layer_sizes) in embedding_output_dims_and_layer_sizes_list:\n",
    "                for activation in activations:\n",
    "                    models_params_list.append(dict(\n",
    "                        impute=impute,\n",
    "                        dropout_probability=dropout_probability,  \n",
    "                        embedding_output_dim=embedding_output_dim,\n",
    "                        layer_sizes=layer_sizes,\n",
    "                        activation=activation))\n",
    "\n",
    "print(\"%d models\" % len(models_params_list))\n",
    "models_params_explored = set.union(*[set(x) for x in models_params_list])\n",
    "models_params_explored\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_to_ic50(log_value):\n",
    "        \"\"\"\n",
    "        Convert neural network output to IC50 values between 0.0 and\n",
    "        self.max_ic50 (typically 5000, 20000 or 50000)\n",
    "        \"\"\"\n",
    "        return max_ic50 ** (1.0 - log_value)\n",
    "\n",
    "def make_scores(ic50_y, ic50_y_pred, sample_weight=None, threshold_nm=500):     \n",
    "    y_pred = mhcflurry.common.ic50_to_regression_target(ic50_y_pred, max_ic50)\n",
    "    try:\n",
    "        auc = sklearn.metrics.roc_auc_score(ic50_y <= threshold_nm, y_pred, sample_weight=sample_weight)\n",
    "    except ValueError:\n",
    "        auc = numpy.nan\n",
    "    try:\n",
    "        f1 = sklearn.metrics.f1_score(ic50_y <= threshold_nm, ic50_y_pred <= threshold_nm, sample_weight=sample_weight)\n",
    "    except ValueError:\n",
    "        f1 = numpy.nan\n",
    "    try:\n",
    "        tau = scipy.stats.kendalltau(ic50_y_pred, ic50_y)[0]\n",
    "    except ValueError:\n",
    "        tau = numpy.nan\n",
    "    \n",
    "    return dict(\n",
    "        auc=auc,\n",
    "        f1=f1,\n",
    "        tau=tau,\n",
    "    )    \n",
    "\n",
    "def mean_with_std(grouped_column, decimals=3):\n",
    "    pattern = \"%%0.%df\" % decimals\n",
    "    return pandas.Series([\n",
    "        (pattern + \" +/ \" + pattern) % (m, s) if not pandas.isnull(s) else pattern % m\n",
    "        for (m, s) in zip(grouped_column.mean(), grouped_column.std())\n",
    "    ], index = grouped_column.mean().index)\n",
    "\n",
    "def allele_data_to_df(data):\n",
    "    d = data._asdict()\n",
    "    d[\"X_index\"] = [x for x in d[\"X_index\"]]\n",
    "    d[\"X_binary\"] = [x for x in d[\"X_binary\"]]\n",
    "    df = pandas.DataFrame(d).set_index('peptides')\n",
    "    return df\n",
    "\n",
    "def make_2d_array(thing):\n",
    "    return numpy.array([list(x) for x in thing])\n",
    "\n",
    "def df_to_allele_data(df):\n",
    "    d = dict((col, df[col].values) for col in df)\n",
    "    d[\"X_index\"] = make_2d_array(d[\"X_index\"])\n",
    "    (d[\"max_ic50\"],) = list(df.max_ic50.unique())\n",
    "    return mhcflurry.data.AlleleData(peptides = df.index.values, **d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "models_and_scores = {}\n",
    "validation_df_with_mhcflurry = validation_df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allele 0 model 0\n",
      "Fitting model for allele HLA-A0201 (32876 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "Trained in 38 sec\n",
      "test set size: 2126\n",
      "{'tau': 0.57053898906984879, 'auc': 0.89706610392848241, 'f1': 0.83501979762428502}\n",
      "Allele 0 model 1\n",
      "Fitting model for allele HLA-A0201 (32876 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "Trained in 34 sec\n",
      "test set size: 2126\n",
      "{'tau': 0.62680697037722621, 'auc': 0.92704437796060835, 'f1': 0.8823782852218871}\n",
      "Allele 0 model 2\n",
      "Fitting model for allele HLA-A0201 (32876 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "Trained in 44 sec\n",
      "test set size: 2126\n",
      "{'tau': 0.62270182136721697, 'auc': 0.92546657406418076, 'f1': 0.87851275399913542}\n",
      "Allele 0 model 3\n",
      "Fitting model for allele HLA-A0201 (32876 + 80965): {'activation': 'tanh', 'impute': False, 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "Trained in 40 sec\n",
      "test set size: 2126\n",
      "{'tau': 0.6013483670730686, 'auc': 0.91610838052498489, 'f1': 0.85547050877982889}\n",
      "Allele 0 model 4\n",
      "Fitting model for allele HLA-A0201 (32876 + 80965): {'activation': 'tanh', 'impute': True, 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n"
     ]
    }
   ],
   "source": [
    "# train and test models, adding columns to validation_df_with_mhcflurry\n",
    "def make_and_fit_model(allele, original_params):\n",
    "    params = dict(original_params)\n",
    "    impute = params[\"impute\"]\n",
    "    del params[\"impute\"]\n",
    "    model = mhcflurry.Class1BindingPredictor.from_hyperparameters(max_ic50=max_ic50, **params)\n",
    "    print(\"Fitting model for allele %s (%d + %d): %s\" % (\n",
    "            allele, len(all_train_data[allele].Y), len(imputed_train_data[allele].Y), str(original_params)))\n",
    "    t = -time.time()\n",
    "    model.fit(all_train_data[allele].X_index,\n",
    "              all_train_data[allele].Y,\n",
    "              sample_weights=all_train_data[allele].weights,\n",
    "              X_pretrain=imputed_train_data[allele].X_index if impute else None,\n",
    "              Y_pretrain=imputed_train_data[allele].Y if impute else None,\n",
    "              sample_weights_pretrain=imputed_train_data[allele].weights if impute else None,\n",
    "              verbose=False,\n",
    "              batch_size=128,\n",
    "              n_training_epochs=250)\n",
    "    t += time.time()\n",
    "    print(\"Trained in %d sec\" % t)\n",
    "    return model\n",
    "\n",
    "for (i, allele) in enumerate(alleles):\n",
    "    if allele not in validation_df_with_mhcflurry.allele.unique():\n",
    "        print(\"Skipping allele %s: not in test set\" % allele)\n",
    "        continue\n",
    "    if allele in models_and_scores:\n",
    "        print(\"Skipping allele %s: already done\" % allele)\n",
    "        continue\n",
    "    values_for_allele = []\n",
    "    for (j, params) in enumerate(models_params_list):\n",
    "        print(\"Allele %d model %d\" % (i, j))\n",
    "        model = make_and_fit_model(allele, params)\n",
    "        predictions = model.predict_peptides_ic50(\n",
    "            list(validation_df_with_mhcflurry.ix[validation_df_with_mhcflurry.allele == allele].peptide))\n",
    "        print(\"test set size: %d\" % len(predictions))\n",
    "        validation_df_with_mhcflurry.loc[(validation_df_with_mhcflurry.allele == allele),\n",
    "                                         (\"mhcflurry %d\" % j)] = predictions\n",
    "        scores = make_scores(validation_df_with_mhcflurry.ix[validation_df.allele == allele].meas,\n",
    "                            predictions)\n",
    "        print(scores)\n",
    "        values_for_allele.append((params, scores))\n",
    "        \n",
    "    models_and_scores[allele] = values_for_allele\n",
    "    \n",
    "    # Write out all data after each allele.\n",
    "    validation_df_with_mhcflurry_results = validation_df_with_mhcflurry.ix[validation_df_with_mhcflurry.allele.isin(models_and_scores)]\n",
    "    validation_df_with_mhcflurry_results.to_csv(\"../data/validation_predictions_full.csv\", index=False)\n",
    "    \n",
    "    scores_df = collections.defaultdict(list)\n",
    "    predictors = validation_df_with_mhcflurry_results.columns[4:]\n",
    "\n",
    "    for (allele, grouped) in validation_df_with_mhcflurry_results.groupby(\"allele\"):\n",
    "        scores_df[\"allele\"].append(allele)\n",
    "        scores_df[\"test_size\"].append(len(grouped.meas))\n",
    "        for predictor in predictors:\n",
    "            scores = make_scores(grouped.meas, grouped[predictor])\n",
    "            for (key, value) in scores.items():\n",
    "                scores_df[\"%s_%s\" % (predictor, key)].append(value)\n",
    "\n",
    "    scores_df = pandas.DataFrame(scores_df)\n",
    "    scores_df[\"train_size\"] = [len(set(all_train_data[a].original_peptides)) for a in scores_df.allele]\n",
    "\n",
    "    scores_df.index = scores_df.allele\n",
    "    scores_df.to_csv(\"../data/validation_scores.csv\", index=False)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pandas.DataFrame(models_params_list).to_csv(\"../data/validation_models.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
