{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"THEANO_FLAGS\"] = 'cuda.root=/usr/local/cuda,floatX=float32,device=gpu0,force_device=False,lib.cnmem=.75'\n",
    "\n",
    "import theano\n",
    "print(theano.config.device)\n",
    "\n",
    "import mhcflurry, seaborn, numpy, pandas, pickle, sklearn, collections, scipy, time, logging\n",
    "import mhcflurry.data\n",
    "import mhcflurry.imputation\n",
    "import fancyimpute\n",
    "\n",
    "import sklearn.metrics\n",
    "import sklearn.cross_validation\n",
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_peptides_to_consider_allele = 50\n",
    "max_ic50 = 50000\n",
    "data_dir = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_train_data = mhcflurry.data.load_allele_datasets(\n",
    "    data_dir + \"bdata.2009.mhci.public.1.txt\",\n",
    "    use_multiple_peptide_lengths=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alleles = [\n",
    "    \"HLA-A0201\",\n",
    "    # \"HLA-A0301\",\n",
    "    \"HLA-A0203\",\n",
    "    # \"HLA-A2602\",\n",
    "    # \"HLA-A2603\",\n",
    "    # 'HLA-B7301',\n",
    "]\n",
    "#alleles = alleles[:1] + alleles[-1:]\n",
    "#alleles = [allele for allele in all_train_data if len(all_train_data[allele].Y) >= min_peptides_to_consider_allele]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36749263596306014"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_data[alleles[0]].weights.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{8, 9, 10, 11, 12, 13, 14, 15}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(len(x) for x in all_train_data[alleles[0]].original_peptides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{9}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(len(x) for x in all_train_data[alleles[0]].peptides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 2 / 106 alleles\n"
     ]
    }
   ],
   "source": [
    "#train_data = dict((allele, data)\n",
    "#                  for (allele, data) in all_train_data.items()\n",
    "#                  if len(data.Y) >= min_peptides_to_consider_allele)\n",
    "train_data = dict((allele, all_train_data[allele]) for allele in alleles)\n",
    "print(\"Training data: %d / %d alleles\" % (len(train_data), len(all_train_data)))\n",
    "\n",
    "#test_data = mhcflurry.data.load_allele_datasets(\"/Users/tim/sinai/git/mhcflurry/bdata.2013.mhci.public.blind.1.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def log_to_ic50(log_value):\n",
    "        \"\"\"\n",
    "        Convert neural network output to IC50 values between 0.0 and\n",
    "        self.max_ic50 (typically 5000, 20000 or 50000)\n",
    "        \"\"\"\n",
    "        return max_ic50 ** (1.0 - log_value)\n",
    "\n",
    "def make_scores(y, y_pred, weights=None, sample_weight=None, threshold_nm=500):\n",
    "    ic50_y = log_to_ic50(y)\n",
    "    ic50_y_pred = log_to_ic50(y_pred) \n",
    "    return dict(\n",
    "        auc=sklearn.metrics.roc_auc_score(ic50_y <= threshold_nm, y_pred, sample_weight=sample_weight),\n",
    "        f1=sklearn.metrics.f1_score(ic50_y <= threshold_nm, ic50_y_pred <= threshold_nm, sample_weight=sample_weight),\n",
    "        tau=scipy.stats.kendalltau(y_pred, y)[0],\n",
    "    )    \n",
    "\n",
    "def mean_with_std(grouped_column, decimals=3):\n",
    "    pattern = \"%%0.%df\" % decimals\n",
    "    return pandas.Series([\n",
    "        (pattern + \" +/ \" + pattern) % (m, s) if not pandas.isnull(s) else pattern % m\n",
    "        for (m, s) in zip(grouped_column.mean(), grouped_column.std())\n",
    "    ], index = grouped_column.mean().index)\n",
    "\n",
    "def allele_data_to_df(data):\n",
    "    d = data._asdict()\n",
    "    d[\"X_index\"] = [x for x in d[\"X_index\"]]\n",
    "    d[\"X_binary\"] = [x for x in d[\"X_binary\"]]\n",
    "    df = pandas.DataFrame(d).set_index('peptides')\n",
    "    return df\n",
    "\n",
    "def make_2d_array(thing):\n",
    "    return numpy.array([list(x) for x in thing])\n",
    "\n",
    "def df_to_allele_data(df):\n",
    "    d = dict((col, df[col].values) for col in df)\n",
    "    d[\"X_index\"] = make_2d_array(d[\"X_index\"])\n",
    "    (d[\"max_ic50\"],) = list(df.max_ic50.unique())\n",
    "    return mhcflurry.data.AlleleData(peptides = df.index.values, **d)\n",
    "\n",
    "def get_shuffled_fields(allele_data):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    allele_data : mhcflurry.data.AlleleData\n",
    "    \n",
    "    Returns shuffled array fields\n",
    "    \"\"\"\n",
    "    original_peptides = data.original_peptides\n",
    "    X = data.X_index \n",
    "    Y = data.Y\n",
    "    weights = data.weights\n",
    "    \n",
    "    # shuffle all the  samples!\n",
    "    shuffle_indices = np.arange(len(X))\n",
    "    np.random.shuffle(shuffle_indices)\n",
    "    \n",
    "    X = X[shuffle_indices]\n",
    "    Y = Y[shuffle_indices]\n",
    "    weights = weights[shuffle_indices]\n",
    "    original_peptides = original_peptides[shuffle_indices]\n",
    "    \n",
    "    return X, Y, weights, original_peptides\n",
    "\n",
    "def collapse_9mer_affinities(Y_9mer_true, Y_9mer_pred, original_peptides):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    Y_9mer_true : np.array of float\n",
    "        True regression target values for 9mers extracted from longer/shorter peptides\n",
    "    \n",
    "    Y_9mer_pred : np.array of float\n",
    "        Predicted values for 9mers\n",
    "    \n",
    "    original_peptides : np.array of str\n",
    "        Original peptides of varying length that 9mers were extracted from.\n",
    "    \"\"\"\n",
    "    # collapse multiple 9mer predictions and measured values into \n",
    "    # smaller set of predictions for peptides of varying lengths\n",
    "    peptide_to_true_affinity_dict = defaultdict(list)\n",
    "    peptide_to_predicted_affinity_dict = defaultdict(list)\n",
    "    for i, p in enumerate(original_peptides):\n",
    "        peptide_to_true_affinity_dict[p].append(Y_9mer_true[i])\n",
    "        peptide_to_predicted_affinity_dict[p].append(Y_9mer_pred[i])\n",
    "\n",
    "    unique_peptides = list(sorted(set(peptide_to_predicted_affinity_dict.keys())))\n",
    "    print(\"-- # unique peptides = %d\" % (len(unique_peptides),))\n",
    "    Y_true = np.array([\n",
    "            np.mean(peptide_to_true_affinity_dict[p]) for p in unique_peptides ])\n",
    "    Y_pred = np.array([\n",
    "            np.mean(peptide_to_predicted_affinity_dict[p]) for p in unique_peptides])\n",
    "    return Y_true, Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 models\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'activation', 'dropout_probability', 'embedding_output_dim', 'layer_sizes'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout_probabilities = [0.0, 0.5]\n",
    "\n",
    "embedding_output_dims = [32, 8]\n",
    "#embedding_output_dims = [4, 32]\n",
    "\n",
    "#layer_sizes = [[4], [8], [16], [64], [128]]\n",
    "layer_sizes_list = [[128], [4]]\n",
    "\n",
    "activations = [\"tanh\"]\n",
    "\n",
    "models_params_list = []\n",
    "for dropout_probability in dropout_probabilities:\n",
    "    for embedding_output_dim in embedding_output_dims:\n",
    "        for layer_sizes in layer_sizes_list:\n",
    "            for activation in activations:\n",
    "                models_params_list.append(dict(\n",
    "                    dropout_probability=dropout_probability,  \n",
    "                    embedding_output_dim=embedding_output_dim,\n",
    "                    layer_sizes=layer_sizes,\n",
    "                    activation=activation))\n",
    "\n",
    "print(\"%d models\" % len(models_params_list))\n",
    "models_params_explored = set.union(*[set(x) for x in models_params_list])\n",
    "models_params_explored\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allele: HLA-A0201\n",
      "-- fold #1/3\n",
      " HLA-A0201 fold   0 [  0 /   8] train_size=21917 test_size=10959 impute=False model={'activation': 'tanh', 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [128]}\n",
      "-- # unique peptides = 6377\n",
      "-- # unique peptides = 3188\n",
      "train tau: 0.689418\n",
      "train auc: 0.980339\n",
      "train f1: 0.898028\n",
      "test tau: 0.593120\n",
      "test auc: 0.936387\n",
      "test f1: 0.794369\n",
      " HLA-A0201 fold   0 [  1 /   8] train_size=21917 test_size=10959 impute=False model={'activation': 'tanh', 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "-- # unique peptides = 6377\n",
      "-- # unique peptides = 3188\n",
      "train tau: 0.636077\n",
      "train auc: 0.959210\n",
      "train f1: 0.850818\n",
      "test tau: 0.618440\n",
      "test auc: 0.947425\n",
      "test f1: 0.818637\n",
      " HLA-A0201 fold   0 [  2 /   8] train_size=21917 test_size=10959 impute=False model={'activation': 'tanh', 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "-- # unique peptides = 6377\n",
      "-- # unique peptides = 3188\n",
      "train tau: 0.612900\n",
      "train auc: 0.951196\n",
      "train f1: 0.835620\n",
      "test tau: 0.609103\n",
      "test auc: 0.946499\n",
      "test f1: 0.818502\n",
      " HLA-A0201 fold   0 [  3 /   8] train_size=21917 test_size=10959 impute=False model={'activation': 'tanh', 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [128]}\n",
      "-- # unique peptides = 6377\n",
      "-- # unique peptides = 3188\n",
      "train tau: 0.756294\n",
      "train auc: 0.993607\n",
      "train f1: 0.942386\n",
      "test tau: 0.573385\n",
      "test auc: 0.924151\n",
      "test f1: 0.773585\n",
      " HLA-A0201 fold   0 [  4 /   8] train_size=21917 test_size=10959 impute=False model={'activation': 'tanh', 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "-- # unique peptides = 6377\n",
      "-- # unique peptides = 3188\n",
      "train tau: 0.595929\n",
      "train auc: 0.942209\n",
      "train f1: 0.800302\n",
      "test tau: 0.594130\n",
      "test auc: 0.937414\n",
      "test f1: 0.776956\n",
      " HLA-A0201 fold   0 [  5 /   8] train_size=21917 test_size=10959 impute=False model={'activation': 'tanh', 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "-- # unique peptides = 6377\n",
      "-- # unique peptides = 3188\n",
      "train tau: 0.651938\n",
      "train auc: 0.966531\n",
      "train f1: 0.863958\n",
      "test tau: 0.608516\n",
      "test auc: 0.946092\n",
      "test f1: 0.819348\n",
      " HLA-A0201 fold   0 [  6 /   8] train_size=21917 test_size=10959 impute=False model={'activation': 'tanh', 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [128]}\n",
      "-- # unique peptides = 6377\n",
      "-- # unique peptides = 3188\n",
      "train tau: 0.614779\n",
      "train auc: 0.951413\n",
      "train f1: 0.838246\n",
      "test tau: 0.610973\n",
      "test auc: 0.946660\n",
      "test f1: 0.820180\n",
      " HLA-A0201 fold   0 [  7 /   8] train_size=21917 test_size=10959 impute=False model={'activation': 'tanh', 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [128]}\n",
      "-- # unique peptides = 6377\n",
      "-- # unique peptides = 3188\n",
      "train tau: 0.600546\n",
      "train auc: 0.944210\n",
      "train f1: 0.814146\n",
      "test tau: 0.598405\n",
      "test auc: 0.938844\n",
      "test f1: 0.796706\n",
      "-- fold #2/3\n",
      " HLA-A0201 fold   1 [  0 /   8] train_size=21917 test_size=10959 impute=False model={'activation': 'tanh', 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [128]}\n",
      "-- # unique peptides = 6376\n",
      "-- # unique peptides = 3189\n",
      "train tau: 0.764580\n",
      "train auc: 0.993560\n",
      "train f1: 0.944716\n",
      "test tau: 0.565897\n",
      "test auc: 0.928619\n",
      "test f1: 0.786155\n",
      " HLA-A0201 fold   1 [  1 /   8] train_size=21917 test_size=10959 impute=False model={'activation': 'tanh', 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [128]}\n",
      "-- # unique peptides = 6376\n",
      "-- # unique peptides = 3189\n",
      "train tau: 0.605306\n",
      "train auc: 0.942652\n",
      "train f1: 0.796619\n",
      "test tau: 0.593166\n",
      "test auc: 0.944114\n",
      "test f1: 0.804175\n",
      " HLA-A0201 fold   1 [  2 /   8] train_size=21917 test_size=10959 impute=False model={'activation': 'tanh', 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "-- # unique peptides = 6376\n",
      "-- # unique peptides = 3189\n",
      "train tau: 0.652449\n",
      "train auc: 0.963767\n",
      "train f1: 0.854192\n",
      "test tau: 0.599184\n",
      "test auc: 0.951386\n",
      "test f1: 0.834683\n",
      " HLA-A0201 fold   1 [  3 /   8] train_size=21917 test_size=10959 impute=False model={'activation': 'tanh', 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [128]}\n",
      "-- # unique peptides = 6376\n",
      "-- # unique peptides = 3189\n",
      "train tau: 0.620571\n",
      "train auc: 0.950763\n",
      "train f1: 0.828168\n",
      "test tau: 0.605044\n",
      "test auc: 0.950678\n",
      "test f1: 0.828986\n",
      " HLA-A0201 fold   1 [  4 /   8] train_size=21917 test_size=10959 impute=False model={'activation': 'tanh', 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "-- # unique peptides = 6376\n",
      "-- # unique peptides = 3189\n",
      "train tau: 0.601807\n",
      "train auc: 0.941037\n",
      "train f1: 0.803225\n",
      "test tau: 0.589987\n",
      "test auc: 0.942599\n",
      "test f1: 0.804711\n",
      " HLA-A0201 fold   1 [  5 /   8] train_size=21917 test_size=10959 impute=False model={'activation': 'tanh', 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "-- # unique peptides = 6376\n",
      "-- # unique peptides = 3189\n",
      "train tau: 0.620054\n",
      "train auc: 0.951944\n",
      "train f1: 0.834692\n",
      "test tau: 0.604458\n",
      "test auc: 0.950674\n",
      "test f1: 0.837500\n",
      " HLA-A0201 fold   1 [  6 /   8] train_size=21917 test_size=10959 impute=False model={'activation': 'tanh', 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [128]}\n",
      "-- # unique peptides = 6376\n",
      "-- # unique peptides = 3189\n",
      "train tau: 0.677204\n",
      "train auc: 0.974023\n",
      "train f1: 0.880309\n",
      "test tau: 0.601365\n",
      "test auc: 0.947718\n",
      "test f1: 0.827362\n",
      " HLA-A0201 fold   1 [  7 /   8] train_size=21917 test_size=10959 impute=False model={'activation': 'tanh', 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "-- # unique peptides = 6376\n",
      "-- # unique peptides = 3189\n",
      "train tau: 0.641459\n",
      "train auc: 0.958561\n",
      "train f1: 0.846869\n",
      "test tau: 0.612538\n",
      "test auc: 0.954074\n",
      "test f1: 0.841104\n",
      "-- fold #3/3\n",
      " HLA-A0201 fold   2 [  0 /   8] train_size=21918 test_size=10958 impute=False model={'activation': 'tanh', 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "-- # unique peptides = 6377\n",
      "-- # unique peptides = 3188\n",
      "train tau: 0.632399\n",
      "train auc: 0.960931\n",
      "train f1: 0.851591\n",
      "test tau: 0.612703\n",
      "test auc: 0.948553\n",
      "test f1: 0.824654\n",
      " HLA-A0201 fold   2 [  1 /   8] train_size=21918 test_size=10958 impute=False model={'activation': 'tanh', 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "-- # unique peptides = 6377\n",
      "-- # unique peptides = 3188\n",
      "train tau: 0.616235\n",
      "train auc: 0.953054\n",
      "train f1: 0.833251\n",
      "test tau: 0.611002\n",
      "test auc: 0.947506\n",
      "test f1: 0.823186\n",
      " HLA-A0201 fold   2 [  2 /   8] train_size=21918 test_size=10958 impute=False model={'activation': 'tanh', 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [128]}\n",
      "-- # unique peptides = 6377\n",
      "-- # unique peptides = 3188\n",
      "train tau: 0.755682\n",
      "train auc: 0.993622\n",
      "train f1: 0.945570\n",
      "test tau: 0.571184\n",
      "test auc: 0.926755\n",
      "test f1: 0.788973\n",
      " HLA-A0201 fold   2 [  3 /   8] train_size=21918 test_size=10958 impute=False model={'activation': 'tanh', 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [128]}\n",
      "-- # unique peptides = 6377\n",
      "-- # unique peptides = 3188\n",
      "train tau: 0.676646\n",
      "train auc: 0.976603\n",
      "train f1: 0.887770\n",
      "test tau: 0.606301\n",
      "test auc: 0.945763\n",
      "test f1: 0.815962\n",
      " HLA-A0201 fold   2 [  4 /   8] train_size=21918 test_size=10958 impute=False model={'activation': 'tanh', 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [128]}\n",
      "-- # unique peptides = 6377\n",
      "-- # unique peptides = 3188\n",
      "train tau: 0.600343\n",
      "train auc: 0.944059\n",
      "train f1: 0.805556\n",
      "test tau: 0.601706\n",
      "test auc: 0.941839\n",
      "test f1: 0.803794\n",
      " HLA-A0201 fold   2 [  5 /   8] train_size=21918 test_size=10958 impute=False model={'activation': 'tanh', 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [128]}\n",
      "-- # unique peptides = 6377\n",
      "-- # unique peptides = 3188\n",
      "train tau: 0.617038\n",
      "train auc: 0.953064\n",
      "train f1: 0.831233\n",
      "test tau: 0.613119\n",
      "test auc: 0.947674\n",
      "test f1: 0.825474\n",
      " HLA-A0201 fold   2 [  6 /   8] train_size=21918 test_size=10958 impute=False model={'activation': 'tanh', 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "-- # unique peptides = 6377\n",
      "-- # unique peptides = 3188\n",
      "train tau: 0.650748\n",
      "train auc: 0.967210\n",
      "train f1: 0.866100\n",
      "test tau: 0.607308\n",
      "test auc: 0.944436\n",
      "test f1: 0.817173\n",
      " HLA-A0201 fold   2 [  7 /   8] train_size=21918 test_size=10958 impute=False model={'activation': 'tanh', 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "-- # unique peptides = 6377\n",
      "-- # unique peptides = 3188\n",
      "train tau: 0.599491\n",
      "train auc: 0.944287\n",
      "train f1: 0.801827\n",
      "test tau: 0.598530\n",
      "test auc: 0.940864\n",
      "test f1: 0.802198\n",
      "Allele: HLA-A0203\n",
      "-- fold #1/3\n",
      " HLA-A0203 fold   0 [  0 /   8] train_size=13252 test_size=6627 impute=False model={'activation': 'tanh', 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "-- # unique peptides = 3694\n",
      "-- # unique peptides = 1848\n",
      "train tau: 0.606821\n",
      "train auc: 0.917251\n",
      "train f1: 0.782676\n",
      "test tau: 0.614793\n",
      "test auc: 0.919640\n",
      "test f1: 0.779661\n",
      " HLA-A0203 fold   0 [  1 /   8] train_size=13252 test_size=6627 impute=False model={'activation': 'tanh', 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [128]}\n",
      "-- # unique peptides = 3694\n",
      "-- # unique peptides = 1848\n",
      "train tau: 0.606775\n",
      "train auc: 0.916913\n",
      "train f1: 0.783920\n",
      "test tau: 0.615366\n",
      "test auc: 0.920612\n",
      "test f1: 0.783784\n",
      " HLA-A0203 fold   0 [  2 /   8] train_size=13252 test_size=6627 impute=False model={'activation': 'tanh', 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [128]}\n",
      "-- # unique peptides = 3694\n",
      "-- # unique peptides = 1848\n",
      "train tau: 0.626517\n",
      "train auc: 0.928959\n",
      "train f1: 0.805745\n",
      "test tau: 0.631430\n",
      "test auc: 0.931166\n",
      "test f1: 0.807126\n",
      " HLA-A0203 fold   0 [  3 /   8] train_size=13252 test_size=6627 impute=False model={'activation': 'tanh', 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "-- # unique peptides = 3694\n",
      "-- # unique peptides = 1848\n",
      "train tau: 0.683477\n",
      "train auc: 0.959783\n",
      "train f1: 0.846332\n",
      "test tau: 0.617045\n",
      "test auc: 0.926688\n",
      "test f1: 0.791033\n",
      " HLA-A0203 fold   0 [  4 /   8] train_size=13252 test_size=6627 impute=False model={'activation': 'tanh', 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "-- # unique peptides = 3694\n",
      "-- # unique peptides = 1848\n",
      "train tau: 0.627758\n",
      "train auc: 0.929875\n",
      "train f1: 0.800000\n",
      "test tau: 0.631454\n",
      "test auc: 0.930441\n",
      "test f1: 0.799368\n",
      " HLA-A0203 fold   0 [  5 /   8] train_size=13252 test_size=6627 impute=False model={'activation': 'tanh', 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "-- # unique peptides = 3694\n",
      "-- # unique peptides = 1848\n",
      "train tau: 0.657255\n",
      "train auc: 0.947986\n",
      "train f1: 0.831452\n",
      "test tau: 0.628262\n",
      "test auc: 0.931942\n",
      "test f1: 0.792575\n",
      " HLA-A0203 fold   0 [  6 /   8] train_size=13252 test_size=6627 impute=False model={'activation': 'tanh', 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [128]}\n",
      "-- # unique peptides = 3694\n",
      "-- # unique peptides = 1848\n",
      "train tau: 0.781838\n",
      "train auc: 0.989722\n",
      "train f1: 0.929533\n",
      "test tau: 0.584572\n",
      "test auc: 0.910715\n",
      "test f1: 0.759016\n",
      " HLA-A0203 fold   0 [  7 /   8] train_size=13252 test_size=6627 impute=False model={'activation': 'tanh', 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [128]}\n",
      "-- # unique peptides = 3694\n",
      "-- # unique peptides = 1848\n",
      "train tau: 0.678256\n",
      "train auc: 0.954726\n",
      "train f1: 0.847165\n",
      "test tau: 0.631472\n",
      "test auc: 0.933126\n",
      "test f1: 0.799380\n",
      "-- fold #2/3\n",
      " HLA-A0203 fold   1 [  0 /   8] train_size=13253 test_size=6626 impute=False model={'activation': 'tanh', 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "-- # unique peptides = 3695\n",
      "-- # unique peptides = 1847\n",
      "train tau: 0.661833\n",
      "train auc: 0.949147\n",
      "train f1: 0.830562\n",
      "test tau: 0.627735\n",
      "test auc: 0.921516\n",
      "test f1: 0.787185\n",
      " HLA-A0203 fold   1 [  1 /   8] train_size=13253 test_size=6626 impute=False model={'activation': 'tanh', 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [128]}\n",
      "-- # unique peptides = 3695\n",
      "-- # unique peptides = 1847\n",
      "train tau: 0.614392\n",
      "train auc: 0.924185\n",
      "train f1: 0.790024\n",
      "test tau: 0.599493\n",
      "test auc: 0.906639\n",
      "test f1: 0.766486\n",
      " HLA-A0203 fold   1 [  2 /   8] train_size=13253 test_size=6626 impute=False model={'activation': 'tanh', 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "-- # unique peptides = 3695\n",
      "-- # unique peptides = 1847\n",
      "train tau: 0.678649\n",
      "train auc: 0.959813\n",
      "train f1: 0.853630\n",
      "test tau: 0.603913\n",
      "test auc: 0.912352\n",
      "test f1: 0.772036\n",
      " HLA-A0203 fold   1 [  3 /   8] train_size=13253 test_size=6626 impute=False model={'activation': 'tanh', 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [128]}\n",
      "-- # unique peptides = 3695\n",
      "-- # unique peptides = 1847\n",
      "train tau: 0.717725\n",
      "train auc: 0.974737\n",
      "train f1: 0.868954\n",
      "test tau: 0.598688\n",
      "test auc: 0.908988\n",
      "test f1: 0.759274\n",
      " HLA-A0203 fold   1 [  4 /   8] train_size=13253 test_size=6626 impute=False model={'activation': 'tanh', 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "-- # unique peptides = 3695\n",
      "-- # unique peptides = 1847\n",
      "train tau: 0.615065\n",
      "train auc: 0.924163\n",
      "train f1: 0.795285\n",
      "test tau: 0.599694\n",
      "test auc: 0.906781\n",
      "test f1: 0.770865\n",
      " HLA-A0203 fold   1 [  5 /   8] train_size=13253 test_size=6626 impute=False model={'activation': 'tanh', 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [128]}\n",
      "-- # unique peptides = 3695\n",
      "-- # unique peptides = 1847\n",
      "train tau: 0.634643\n",
      "train auc: 0.936135\n",
      "train f1: 0.812036\n",
      "test tau: 0.613060\n",
      "test auc: 0.915715\n",
      "test f1: 0.781955\n",
      " HLA-A0203 fold   1 [  6 /   8] train_size=13253 test_size=6626 impute=False model={'activation': 'tanh', 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "-- # unique peptides = 3695\n",
      "-- # unique peptides = 1847\n",
      "train tau: 0.636664\n",
      "train auc: 0.936914\n",
      "train f1: 0.817021\n",
      "test tau: 0.615471\n",
      "test auc: 0.916830\n",
      "test f1: 0.791078\n",
      " HLA-A0203 fold   1 [  7 /   8] train_size=13253 test_size=6626 impute=False model={'activation': 'tanh', 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [128]}\n",
      "-- # unique peptides = 3695\n",
      "-- # unique peptides = 1847\n",
      "train tau: 0.779462\n",
      "train auc: 0.988924\n",
      "train f1: 0.931322\n",
      "test tau: 0.590762\n",
      "test auc: 0.903446\n",
      "test f1: 0.757644\n",
      "-- fold #3/3\n",
      " HLA-A0203 fold   2 [  0 /   8] train_size=13253 test_size=6626 impute=False model={'activation': 'tanh', 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "-- # unique peptides = 3695\n",
      "-- # unique peptides = 1847\n",
      "train tau: 0.679339\n",
      "train auc: 0.955270\n",
      "train f1: 0.843373\n",
      "test tau: 0.606250\n",
      "test auc: 0.930061\n",
      "test f1: 0.800940\n",
      " HLA-A0203 fold   2 [  1 /   8] train_size=13253 test_size=6626 impute=False model={'activation': 'tanh', 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "-- # unique peptides = 3695\n",
      "-- # unique peptides = 1847\n",
      "train tau: 0.661309\n",
      "train auc: 0.944735\n",
      "train f1: 0.829942\n",
      "test tau: 0.629710\n",
      "test auc: 0.939081\n",
      "test f1: 0.820827\n",
      " HLA-A0203 fold   2 [  2 /   8] train_size=13253 test_size=6626 impute=False model={'activation': 'tanh', 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [128]}\n",
      "-- # unique peptides = 3695\n",
      "-- # unique peptides = 1847\n",
      "train tau: 0.780921\n",
      "train auc: 0.988310\n",
      "train f1: 0.925741\n",
      "test tau: 0.584453\n",
      "test auc: 0.920559\n",
      "test f1: 0.788754\n",
      " HLA-A0203 fold   2 [  3 /   8] train_size=13253 test_size=6626 impute=False model={'activation': 'tanh', 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [128]}\n",
      "-- # unique peptides = 3695\n",
      "-- # unique peptides = 1847\n",
      "train tau: 0.692024\n",
      "train auc: 0.958840\n",
      "train f1: 0.857585\n",
      "test tau: 0.620042\n",
      "test auc: 0.934271\n",
      "test f1: 0.795012\n",
      " HLA-A0203 fold   2 [  4 /   8] train_size=13253 test_size=6626 impute=False model={'activation': 'tanh', 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "-- # unique peptides = 3695\n",
      "-- # unique peptides = 1847\n",
      "train tau: 0.634529\n",
      "train auc: 0.930270\n",
      "train f1: 0.801413\n",
      "test tau: 0.615135\n",
      "test auc: 0.933175\n",
      "test f1: 0.807541\n",
      " HLA-A0203 fold   2 [  5 /   8] train_size=13253 test_size=6626 impute=False model={'activation': 'tanh', 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [128]}\n",
      "-- # unique peptides = 3695\n",
      "-- # unique peptides = 1847\n",
      "train tau: 0.615054\n",
      "train auc: 0.918310\n",
      "train f1: 0.782643\n",
      "test tau: 0.601473\n",
      "test auc: 0.922263\n",
      "test f1: 0.790845\n",
      " HLA-A0203 fold   2 [  6 /   8] train_size=13253 test_size=6626 impute=False model={'activation': 'tanh', 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "-- # unique peptides = 3695\n",
      "-- # unique peptides = 1847\n",
      "train tau: 0.611912\n",
      "train auc: 0.915988\n",
      "train f1: 0.772908\n",
      "test tau: 0.598595\n",
      "test auc: 0.921551\n",
      "test f1: 0.785256\n",
      " HLA-A0203 fold   2 [  7 /   8] train_size=13253 test_size=6626 impute=False model={'activation': 'tanh', 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [128]}\n",
      "-- # unique peptides = 3695\n",
      "-- # unique peptides = 1847\n",
      "train tau: 0.633083\n",
      "train auc: 0.929472\n",
      "train f1: 0.799688\n",
      "test tau: 0.615735\n",
      "test auc: 0.932234\n",
      "test f1: 0.806299\n",
      "1179.00166893\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activation</th>\n",
       "      <th>allele</th>\n",
       "      <th>allele_size</th>\n",
       "      <th>dropout_probability</th>\n",
       "      <th>embedding_output_dim</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>layer_sizes</th>\n",
       "      <th>model_params</th>\n",
       "      <th>test_auc</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>test_tau</th>\n",
       "      <th>train_auc</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>train_size</th>\n",
       "      <th>train_tau</th>\n",
       "      <th>layer0_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201</td>\n",
       "      <td>32876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>25.061758</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'activation': u'tanh', u'embedding_output_di...</td>\n",
       "      <td>0.936387</td>\n",
       "      <td>0.794369</td>\n",
       "      <td>0.593120</td>\n",
       "      <td>0.980339</td>\n",
       "      <td>0.898028</td>\n",
       "      <td>21917</td>\n",
       "      <td>0.689418</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201</td>\n",
       "      <td>32876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>23.286498</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'activation': u'tanh', u'embedding_output_di...</td>\n",
       "      <td>0.947425</td>\n",
       "      <td>0.818637</td>\n",
       "      <td>0.618440</td>\n",
       "      <td>0.959210</td>\n",
       "      <td>0.850818</td>\n",
       "      <td>21917</td>\n",
       "      <td>0.636077</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201</td>\n",
       "      <td>32876</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>28.607714</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'activation': u'tanh', u'embedding_output_di...</td>\n",
       "      <td>0.946499</td>\n",
       "      <td>0.818502</td>\n",
       "      <td>0.609103</td>\n",
       "      <td>0.951196</td>\n",
       "      <td>0.835620</td>\n",
       "      <td>21917</td>\n",
       "      <td>0.612900</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201</td>\n",
       "      <td>32876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>26.448946</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'activation': u'tanh', u'embedding_output_di...</td>\n",
       "      <td>0.924151</td>\n",
       "      <td>0.773585</td>\n",
       "      <td>0.573385</td>\n",
       "      <td>0.993607</td>\n",
       "      <td>0.942386</td>\n",
       "      <td>21917</td>\n",
       "      <td>0.756294</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201</td>\n",
       "      <td>32876</td>\n",
       "      <td>0.5</td>\n",
       "      <td>8</td>\n",
       "      <td>26.492861</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'activation': u'tanh', u'embedding_output_di...</td>\n",
       "      <td>0.937414</td>\n",
       "      <td>0.776956</td>\n",
       "      <td>0.594130</td>\n",
       "      <td>0.942209</td>\n",
       "      <td>0.800302</td>\n",
       "      <td>21917</td>\n",
       "      <td>0.595929</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201</td>\n",
       "      <td>32876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>24.556165</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'activation': u'tanh', u'embedding_output_di...</td>\n",
       "      <td>0.946092</td>\n",
       "      <td>0.819348</td>\n",
       "      <td>0.608516</td>\n",
       "      <td>0.966531</td>\n",
       "      <td>0.863958</td>\n",
       "      <td>21917</td>\n",
       "      <td>0.651938</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201</td>\n",
       "      <td>32876</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>31.096893</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'activation': u'tanh', u'embedding_output_di...</td>\n",
       "      <td>0.946660</td>\n",
       "      <td>0.820180</td>\n",
       "      <td>0.610973</td>\n",
       "      <td>0.951413</td>\n",
       "      <td>0.838246</td>\n",
       "      <td>21917</td>\n",
       "      <td>0.614779</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201</td>\n",
       "      <td>32876</td>\n",
       "      <td>0.5</td>\n",
       "      <td>8</td>\n",
       "      <td>28.702486</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'activation': u'tanh', u'embedding_output_di...</td>\n",
       "      <td>0.938844</td>\n",
       "      <td>0.796706</td>\n",
       "      <td>0.598405</td>\n",
       "      <td>0.944210</td>\n",
       "      <td>0.814146</td>\n",
       "      <td>21917</td>\n",
       "      <td>0.600546</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201</td>\n",
       "      <td>32876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>26.469361</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'activation': u'tanh', u'embedding_output_di...</td>\n",
       "      <td>0.928619</td>\n",
       "      <td>0.786155</td>\n",
       "      <td>0.565897</td>\n",
       "      <td>0.993560</td>\n",
       "      <td>0.944716</td>\n",
       "      <td>21917</td>\n",
       "      <td>0.764580</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201</td>\n",
       "      <td>32876</td>\n",
       "      <td>0.5</td>\n",
       "      <td>8</td>\n",
       "      <td>28.772157</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'activation': u'tanh', u'embedding_output_di...</td>\n",
       "      <td>0.944114</td>\n",
       "      <td>0.804175</td>\n",
       "      <td>0.593166</td>\n",
       "      <td>0.942652</td>\n",
       "      <td>0.796619</td>\n",
       "      <td>21917</td>\n",
       "      <td>0.605306</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201</td>\n",
       "      <td>32876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>25.660596</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'activation': u'tanh', u'embedding_output_di...</td>\n",
       "      <td>0.951386</td>\n",
       "      <td>0.834683</td>\n",
       "      <td>0.599184</td>\n",
       "      <td>0.963767</td>\n",
       "      <td>0.854192</td>\n",
       "      <td>21917</td>\n",
       "      <td>0.652449</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201</td>\n",
       "      <td>32876</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>32.758711</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'activation': u'tanh', u'embedding_output_di...</td>\n",
       "      <td>0.950678</td>\n",
       "      <td>0.828986</td>\n",
       "      <td>0.605044</td>\n",
       "      <td>0.950763</td>\n",
       "      <td>0.828168</td>\n",
       "      <td>21917</td>\n",
       "      <td>0.620571</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201</td>\n",
       "      <td>32876</td>\n",
       "      <td>0.5</td>\n",
       "      <td>8</td>\n",
       "      <td>32.099770</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'activation': u'tanh', u'embedding_output_di...</td>\n",
       "      <td>0.942599</td>\n",
       "      <td>0.804711</td>\n",
       "      <td>0.589987</td>\n",
       "      <td>0.941037</td>\n",
       "      <td>0.803225</td>\n",
       "      <td>21917</td>\n",
       "      <td>0.601807</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201</td>\n",
       "      <td>32876</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>30.385494</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'activation': u'tanh', u'embedding_output_di...</td>\n",
       "      <td>0.950674</td>\n",
       "      <td>0.837500</td>\n",
       "      <td>0.604458</td>\n",
       "      <td>0.951944</td>\n",
       "      <td>0.834692</td>\n",
       "      <td>21917</td>\n",
       "      <td>0.620054</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201</td>\n",
       "      <td>32876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>27.885062</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'activation': u'tanh', u'embedding_output_di...</td>\n",
       "      <td>0.947718</td>\n",
       "      <td>0.827362</td>\n",
       "      <td>0.601365</td>\n",
       "      <td>0.974023</td>\n",
       "      <td>0.880309</td>\n",
       "      <td>21917</td>\n",
       "      <td>0.677204</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201</td>\n",
       "      <td>32876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>25.542518</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'activation': u'tanh', u'embedding_output_di...</td>\n",
       "      <td>0.954074</td>\n",
       "      <td>0.841104</td>\n",
       "      <td>0.612538</td>\n",
       "      <td>0.958561</td>\n",
       "      <td>0.846869</td>\n",
       "      <td>21917</td>\n",
       "      <td>0.641459</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201</td>\n",
       "      <td>32876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>24.283030</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'activation': u'tanh', u'embedding_output_di...</td>\n",
       "      <td>0.948553</td>\n",
       "      <td>0.824654</td>\n",
       "      <td>0.612703</td>\n",
       "      <td>0.960931</td>\n",
       "      <td>0.851591</td>\n",
       "      <td>21918</td>\n",
       "      <td>0.632399</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201</td>\n",
       "      <td>32876</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>29.371298</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'activation': u'tanh', u'embedding_output_di...</td>\n",
       "      <td>0.947506</td>\n",
       "      <td>0.823186</td>\n",
       "      <td>0.611002</td>\n",
       "      <td>0.953054</td>\n",
       "      <td>0.833251</td>\n",
       "      <td>21918</td>\n",
       "      <td>0.616235</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201</td>\n",
       "      <td>32876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>26.635670</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'activation': u'tanh', u'embedding_output_di...</td>\n",
       "      <td>0.926755</td>\n",
       "      <td>0.788973</td>\n",
       "      <td>0.571184</td>\n",
       "      <td>0.993622</td>\n",
       "      <td>0.945570</td>\n",
       "      <td>21918</td>\n",
       "      <td>0.755682</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201</td>\n",
       "      <td>32876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>24.868186</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'activation': u'tanh', u'embedding_output_di...</td>\n",
       "      <td>0.945763</td>\n",
       "      <td>0.815962</td>\n",
       "      <td>0.606301</td>\n",
       "      <td>0.976603</td>\n",
       "      <td>0.887770</td>\n",
       "      <td>21918</td>\n",
       "      <td>0.676646</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201</td>\n",
       "      <td>32876</td>\n",
       "      <td>0.5</td>\n",
       "      <td>8</td>\n",
       "      <td>28.872226</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'activation': u'tanh', u'embedding_output_di...</td>\n",
       "      <td>0.941839</td>\n",
       "      <td>0.803794</td>\n",
       "      <td>0.601706</td>\n",
       "      <td>0.944059</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>21918</td>\n",
       "      <td>0.600343</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201</td>\n",
       "      <td>32876</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>31.321089</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'activation': u'tanh', u'embedding_output_di...</td>\n",
       "      <td>0.947674</td>\n",
       "      <td>0.825474</td>\n",
       "      <td>0.613119</td>\n",
       "      <td>0.953064</td>\n",
       "      <td>0.831233</td>\n",
       "      <td>21918</td>\n",
       "      <td>0.617038</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201</td>\n",
       "      <td>32876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>24.663473</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'activation': u'tanh', u'embedding_output_di...</td>\n",
       "      <td>0.944436</td>\n",
       "      <td>0.817173</td>\n",
       "      <td>0.607308</td>\n",
       "      <td>0.967210</td>\n",
       "      <td>0.866100</td>\n",
       "      <td>21918</td>\n",
       "      <td>0.650748</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0201</td>\n",
       "      <td>32876</td>\n",
       "      <td>0.5</td>\n",
       "      <td>8</td>\n",
       "      <td>26.786619</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'activation': u'tanh', u'embedding_output_di...</td>\n",
       "      <td>0.940864</td>\n",
       "      <td>0.802198</td>\n",
       "      <td>0.598530</td>\n",
       "      <td>0.944287</td>\n",
       "      <td>0.801827</td>\n",
       "      <td>21918</td>\n",
       "      <td>0.599491</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0203</td>\n",
       "      <td>19879</td>\n",
       "      <td>0.5</td>\n",
       "      <td>8</td>\n",
       "      <td>25.331706</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'activation': u'tanh', u'embedding_output_di...</td>\n",
       "      <td>0.919640</td>\n",
       "      <td>0.779661</td>\n",
       "      <td>0.614793</td>\n",
       "      <td>0.917251</td>\n",
       "      <td>0.782676</td>\n",
       "      <td>13252</td>\n",
       "      <td>0.606821</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0203</td>\n",
       "      <td>19879</td>\n",
       "      <td>0.5</td>\n",
       "      <td>8</td>\n",
       "      <td>26.331401</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'activation': u'tanh', u'embedding_output_di...</td>\n",
       "      <td>0.920612</td>\n",
       "      <td>0.783784</td>\n",
       "      <td>0.615366</td>\n",
       "      <td>0.916913</td>\n",
       "      <td>0.783920</td>\n",
       "      <td>13252</td>\n",
       "      <td>0.606775</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0203</td>\n",
       "      <td>19879</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>27.737688</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'activation': u'tanh', u'embedding_output_di...</td>\n",
       "      <td>0.931166</td>\n",
       "      <td>0.807126</td>\n",
       "      <td>0.631430</td>\n",
       "      <td>0.928959</td>\n",
       "      <td>0.805745</td>\n",
       "      <td>13252</td>\n",
       "      <td>0.626517</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0203</td>\n",
       "      <td>19879</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>23.273205</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'activation': u'tanh', u'embedding_output_di...</td>\n",
       "      <td>0.926688</td>\n",
       "      <td>0.791033</td>\n",
       "      <td>0.617045</td>\n",
       "      <td>0.959783</td>\n",
       "      <td>0.846332</td>\n",
       "      <td>13252</td>\n",
       "      <td>0.683477</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0203</td>\n",
       "      <td>19879</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>26.301128</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'activation': u'tanh', u'embedding_output_di...</td>\n",
       "      <td>0.930441</td>\n",
       "      <td>0.799368</td>\n",
       "      <td>0.631454</td>\n",
       "      <td>0.929875</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>13252</td>\n",
       "      <td>0.627758</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0203</td>\n",
       "      <td>19879</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>22.473019</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'activation': u'tanh', u'embedding_output_di...</td>\n",
       "      <td>0.931942</td>\n",
       "      <td>0.792575</td>\n",
       "      <td>0.628262</td>\n",
       "      <td>0.947986</td>\n",
       "      <td>0.831452</td>\n",
       "      <td>13252</td>\n",
       "      <td>0.657255</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0203</td>\n",
       "      <td>19879</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>24.632706</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'activation': u'tanh', u'embedding_output_di...</td>\n",
       "      <td>0.910715</td>\n",
       "      <td>0.759016</td>\n",
       "      <td>0.584572</td>\n",
       "      <td>0.989722</td>\n",
       "      <td>0.929533</td>\n",
       "      <td>13252</td>\n",
       "      <td>0.781838</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0203</td>\n",
       "      <td>19879</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>23.257946</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'activation': u'tanh', u'embedding_output_di...</td>\n",
       "      <td>0.933126</td>\n",
       "      <td>0.799380</td>\n",
       "      <td>0.631472</td>\n",
       "      <td>0.954726</td>\n",
       "      <td>0.847165</td>\n",
       "      <td>13252</td>\n",
       "      <td>0.678256</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0203</td>\n",
       "      <td>19879</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>22.774164</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'activation': u'tanh', u'embedding_output_di...</td>\n",
       "      <td>0.921516</td>\n",
       "      <td>0.787185</td>\n",
       "      <td>0.627735</td>\n",
       "      <td>0.949147</td>\n",
       "      <td>0.830562</td>\n",
       "      <td>13253</td>\n",
       "      <td>0.661833</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0203</td>\n",
       "      <td>19879</td>\n",
       "      <td>0.5</td>\n",
       "      <td>8</td>\n",
       "      <td>25.936212</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'activation': u'tanh', u'embedding_output_di...</td>\n",
       "      <td>0.906639</td>\n",
       "      <td>0.766486</td>\n",
       "      <td>0.599493</td>\n",
       "      <td>0.924185</td>\n",
       "      <td>0.790024</td>\n",
       "      <td>13253</td>\n",
       "      <td>0.614392</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0203</td>\n",
       "      <td>19879</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>15.288314</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'activation': u'tanh', u'embedding_output_di...</td>\n",
       "      <td>0.912352</td>\n",
       "      <td>0.772036</td>\n",
       "      <td>0.603913</td>\n",
       "      <td>0.959813</td>\n",
       "      <td>0.853630</td>\n",
       "      <td>13253</td>\n",
       "      <td>0.678649</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0203</td>\n",
       "      <td>19879</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>15.407130</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'activation': u'tanh', u'embedding_output_di...</td>\n",
       "      <td>0.908988</td>\n",
       "      <td>0.759274</td>\n",
       "      <td>0.598688</td>\n",
       "      <td>0.974737</td>\n",
       "      <td>0.868954</td>\n",
       "      <td>13253</td>\n",
       "      <td>0.717725</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0203</td>\n",
       "      <td>19879</td>\n",
       "      <td>0.5</td>\n",
       "      <td>8</td>\n",
       "      <td>16.507124</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'activation': u'tanh', u'embedding_output_di...</td>\n",
       "      <td>0.906781</td>\n",
       "      <td>0.770865</td>\n",
       "      <td>0.599694</td>\n",
       "      <td>0.924163</td>\n",
       "      <td>0.795285</td>\n",
       "      <td>13253</td>\n",
       "      <td>0.615065</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0203</td>\n",
       "      <td>19879</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>19.163187</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'activation': u'tanh', u'embedding_output_di...</td>\n",
       "      <td>0.915715</td>\n",
       "      <td>0.781955</td>\n",
       "      <td>0.613060</td>\n",
       "      <td>0.936135</td>\n",
       "      <td>0.812036</td>\n",
       "      <td>13253</td>\n",
       "      <td>0.634643</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0203</td>\n",
       "      <td>19879</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>17.605578</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'activation': u'tanh', u'embedding_output_di...</td>\n",
       "      <td>0.916830</td>\n",
       "      <td>0.791078</td>\n",
       "      <td>0.615471</td>\n",
       "      <td>0.936914</td>\n",
       "      <td>0.817021</td>\n",
       "      <td>13253</td>\n",
       "      <td>0.636664</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0203</td>\n",
       "      <td>19879</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>16.329000</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'activation': u'tanh', u'embedding_output_di...</td>\n",
       "      <td>0.903446</td>\n",
       "      <td>0.757644</td>\n",
       "      <td>0.590762</td>\n",
       "      <td>0.988924</td>\n",
       "      <td>0.931322</td>\n",
       "      <td>13253</td>\n",
       "      <td>0.779462</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0203</td>\n",
       "      <td>19879</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>15.248185</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'activation': u'tanh', u'embedding_output_di...</td>\n",
       "      <td>0.930061</td>\n",
       "      <td>0.800940</td>\n",
       "      <td>0.606250</td>\n",
       "      <td>0.955270</td>\n",
       "      <td>0.843373</td>\n",
       "      <td>13253</td>\n",
       "      <td>0.679339</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0203</td>\n",
       "      <td>19879</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>14.493280</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'activation': u'tanh', u'embedding_output_di...</td>\n",
       "      <td>0.939081</td>\n",
       "      <td>0.820827</td>\n",
       "      <td>0.629710</td>\n",
       "      <td>0.944735</td>\n",
       "      <td>0.829942</td>\n",
       "      <td>13253</td>\n",
       "      <td>0.661309</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0203</td>\n",
       "      <td>19879</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>16.323694</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'activation': u'tanh', u'embedding_output_di...</td>\n",
       "      <td>0.920559</td>\n",
       "      <td>0.788754</td>\n",
       "      <td>0.584453</td>\n",
       "      <td>0.988310</td>\n",
       "      <td>0.925741</td>\n",
       "      <td>13253</td>\n",
       "      <td>0.780921</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0203</td>\n",
       "      <td>19879</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>15.371938</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'activation': u'tanh', u'embedding_output_di...</td>\n",
       "      <td>0.934271</td>\n",
       "      <td>0.795012</td>\n",
       "      <td>0.620042</td>\n",
       "      <td>0.958840</td>\n",
       "      <td>0.857585</td>\n",
       "      <td>13253</td>\n",
       "      <td>0.692024</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0203</td>\n",
       "      <td>19879</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>17.658805</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'activation': u'tanh', u'embedding_output_di...</td>\n",
       "      <td>0.933175</td>\n",
       "      <td>0.807541</td>\n",
       "      <td>0.615135</td>\n",
       "      <td>0.930270</td>\n",
       "      <td>0.801413</td>\n",
       "      <td>13253</td>\n",
       "      <td>0.634529</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0203</td>\n",
       "      <td>19879</td>\n",
       "      <td>0.5</td>\n",
       "      <td>8</td>\n",
       "      <td>17.781813</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'activation': u'tanh', u'embedding_output_di...</td>\n",
       "      <td>0.922263</td>\n",
       "      <td>0.790845</td>\n",
       "      <td>0.601473</td>\n",
       "      <td>0.918310</td>\n",
       "      <td>0.782643</td>\n",
       "      <td>13253</td>\n",
       "      <td>0.615054</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0203</td>\n",
       "      <td>19879</td>\n",
       "      <td>0.5</td>\n",
       "      <td>8</td>\n",
       "      <td>16.497927</td>\n",
       "      <td>[4]</td>\n",
       "      <td>{u'activation': u'tanh', u'embedding_output_di...</td>\n",
       "      <td>0.921551</td>\n",
       "      <td>0.785256</td>\n",
       "      <td>0.598595</td>\n",
       "      <td>0.915988</td>\n",
       "      <td>0.772908</td>\n",
       "      <td>13253</td>\n",
       "      <td>0.611912</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>tanh</td>\n",
       "      <td>HLA-A0203</td>\n",
       "      <td>19879</td>\n",
       "      <td>0.5</td>\n",
       "      <td>32</td>\n",
       "      <td>19.272132</td>\n",
       "      <td>[128]</td>\n",
       "      <td>{u'activation': u'tanh', u'embedding_output_di...</td>\n",
       "      <td>0.932234</td>\n",
       "      <td>0.806299</td>\n",
       "      <td>0.615735</td>\n",
       "      <td>0.929472</td>\n",
       "      <td>0.799688</td>\n",
       "      <td>13253</td>\n",
       "      <td>0.633083</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   activation     allele  allele_size  dropout_probability  \\\n",
       "0        tanh  HLA-A0201        32876                  0.0   \n",
       "1        tanh  HLA-A0201        32876                  0.0   \n",
       "2        tanh  HLA-A0201        32876                  0.5   \n",
       "3        tanh  HLA-A0201        32876                  0.0   \n",
       "4        tanh  HLA-A0201        32876                  0.5   \n",
       "5        tanh  HLA-A0201        32876                  0.0   \n",
       "6        tanh  HLA-A0201        32876                  0.5   \n",
       "7        tanh  HLA-A0201        32876                  0.5   \n",
       "8        tanh  HLA-A0201        32876                  0.0   \n",
       "9        tanh  HLA-A0201        32876                  0.5   \n",
       "10       tanh  HLA-A0201        32876                  0.0   \n",
       "11       tanh  HLA-A0201        32876                  0.5   \n",
       "12       tanh  HLA-A0201        32876                  0.5   \n",
       "13       tanh  HLA-A0201        32876                  0.5   \n",
       "14       tanh  HLA-A0201        32876                  0.0   \n",
       "15       tanh  HLA-A0201        32876                  0.0   \n",
       "16       tanh  HLA-A0201        32876                  0.0   \n",
       "17       tanh  HLA-A0201        32876                  0.5   \n",
       "18       tanh  HLA-A0201        32876                  0.0   \n",
       "19       tanh  HLA-A0201        32876                  0.0   \n",
       "20       tanh  HLA-A0201        32876                  0.5   \n",
       "21       tanh  HLA-A0201        32876                  0.5   \n",
       "22       tanh  HLA-A0201        32876                  0.0   \n",
       "23       tanh  HLA-A0201        32876                  0.5   \n",
       "24       tanh  HLA-A0203        19879                  0.5   \n",
       "25       tanh  HLA-A0203        19879                  0.5   \n",
       "26       tanh  HLA-A0203        19879                  0.5   \n",
       "27       tanh  HLA-A0203        19879                  0.0   \n",
       "28       tanh  HLA-A0203        19879                  0.5   \n",
       "29       tanh  HLA-A0203        19879                  0.0   \n",
       "30       tanh  HLA-A0203        19879                  0.0   \n",
       "31       tanh  HLA-A0203        19879                  0.0   \n",
       "32       tanh  HLA-A0203        19879                  0.0   \n",
       "33       tanh  HLA-A0203        19879                  0.5   \n",
       "34       tanh  HLA-A0203        19879                  0.0   \n",
       "35       tanh  HLA-A0203        19879                  0.0   \n",
       "36       tanh  HLA-A0203        19879                  0.5   \n",
       "37       tanh  HLA-A0203        19879                  0.5   \n",
       "38       tanh  HLA-A0203        19879                  0.5   \n",
       "39       tanh  HLA-A0203        19879                  0.0   \n",
       "40       tanh  HLA-A0203        19879                  0.0   \n",
       "41       tanh  HLA-A0203        19879                  0.0   \n",
       "42       tanh  HLA-A0203        19879                  0.0   \n",
       "43       tanh  HLA-A0203        19879                  0.0   \n",
       "44       tanh  HLA-A0203        19879                  0.5   \n",
       "45       tanh  HLA-A0203        19879                  0.5   \n",
       "46       tanh  HLA-A0203        19879                  0.5   \n",
       "47       tanh  HLA-A0203        19879                  0.5   \n",
       "\n",
       "    embedding_output_dim   fit_time layer_sizes  \\\n",
       "0                      8  25.061758       [128]   \n",
       "1                      8  23.286498         [4]   \n",
       "2                     32  28.607714         [4]   \n",
       "3                     32  26.448946       [128]   \n",
       "4                      8  26.492861         [4]   \n",
       "5                     32  24.556165         [4]   \n",
       "6                     32  31.096893       [128]   \n",
       "7                      8  28.702486       [128]   \n",
       "8                     32  26.469361       [128]   \n",
       "9                      8  28.772157       [128]   \n",
       "10                    32  25.660596         [4]   \n",
       "11                    32  32.758711       [128]   \n",
       "12                     8  32.099770         [4]   \n",
       "13                    32  30.385494         [4]   \n",
       "14                     8  27.885062       [128]   \n",
       "15                     8  25.542518         [4]   \n",
       "16                     8  24.283030         [4]   \n",
       "17                    32  29.371298         [4]   \n",
       "18                    32  26.635670       [128]   \n",
       "19                     8  24.868186       [128]   \n",
       "20                     8  28.872226       [128]   \n",
       "21                    32  31.321089       [128]   \n",
       "22                    32  24.663473         [4]   \n",
       "23                     8  26.786619         [4]   \n",
       "24                     8  25.331706         [4]   \n",
       "25                     8  26.331401       [128]   \n",
       "26                    32  27.737688       [128]   \n",
       "27                    32  23.273205         [4]   \n",
       "28                    32  26.301128         [4]   \n",
       "29                     8  22.473019         [4]   \n",
       "30                    32  24.632706       [128]   \n",
       "31                     8  23.257946       [128]   \n",
       "32                     8  22.774164         [4]   \n",
       "33                     8  25.936212       [128]   \n",
       "34                    32  15.288314         [4]   \n",
       "35                     8  15.407130       [128]   \n",
       "36                     8  16.507124         [4]   \n",
       "37                    32  19.163187       [128]   \n",
       "38                    32  17.605578         [4]   \n",
       "39                    32  16.329000       [128]   \n",
       "40                    32  15.248185         [4]   \n",
       "41                     8  14.493280         [4]   \n",
       "42                    32  16.323694       [128]   \n",
       "43                     8  15.371938       [128]   \n",
       "44                    32  17.658805         [4]   \n",
       "45                     8  17.781813       [128]   \n",
       "46                     8  16.497927         [4]   \n",
       "47                    32  19.272132       [128]   \n",
       "\n",
       "                                         model_params  test_auc   test_f1  \\\n",
       "0   {u'activation': u'tanh', u'embedding_output_di...  0.936387  0.794369   \n",
       "1   {u'activation': u'tanh', u'embedding_output_di...  0.947425  0.818637   \n",
       "2   {u'activation': u'tanh', u'embedding_output_di...  0.946499  0.818502   \n",
       "3   {u'activation': u'tanh', u'embedding_output_di...  0.924151  0.773585   \n",
       "4   {u'activation': u'tanh', u'embedding_output_di...  0.937414  0.776956   \n",
       "5   {u'activation': u'tanh', u'embedding_output_di...  0.946092  0.819348   \n",
       "6   {u'activation': u'tanh', u'embedding_output_di...  0.946660  0.820180   \n",
       "7   {u'activation': u'tanh', u'embedding_output_di...  0.938844  0.796706   \n",
       "8   {u'activation': u'tanh', u'embedding_output_di...  0.928619  0.786155   \n",
       "9   {u'activation': u'tanh', u'embedding_output_di...  0.944114  0.804175   \n",
       "10  {u'activation': u'tanh', u'embedding_output_di...  0.951386  0.834683   \n",
       "11  {u'activation': u'tanh', u'embedding_output_di...  0.950678  0.828986   \n",
       "12  {u'activation': u'tanh', u'embedding_output_di...  0.942599  0.804711   \n",
       "13  {u'activation': u'tanh', u'embedding_output_di...  0.950674  0.837500   \n",
       "14  {u'activation': u'tanh', u'embedding_output_di...  0.947718  0.827362   \n",
       "15  {u'activation': u'tanh', u'embedding_output_di...  0.954074  0.841104   \n",
       "16  {u'activation': u'tanh', u'embedding_output_di...  0.948553  0.824654   \n",
       "17  {u'activation': u'tanh', u'embedding_output_di...  0.947506  0.823186   \n",
       "18  {u'activation': u'tanh', u'embedding_output_di...  0.926755  0.788973   \n",
       "19  {u'activation': u'tanh', u'embedding_output_di...  0.945763  0.815962   \n",
       "20  {u'activation': u'tanh', u'embedding_output_di...  0.941839  0.803794   \n",
       "21  {u'activation': u'tanh', u'embedding_output_di...  0.947674  0.825474   \n",
       "22  {u'activation': u'tanh', u'embedding_output_di...  0.944436  0.817173   \n",
       "23  {u'activation': u'tanh', u'embedding_output_di...  0.940864  0.802198   \n",
       "24  {u'activation': u'tanh', u'embedding_output_di...  0.919640  0.779661   \n",
       "25  {u'activation': u'tanh', u'embedding_output_di...  0.920612  0.783784   \n",
       "26  {u'activation': u'tanh', u'embedding_output_di...  0.931166  0.807126   \n",
       "27  {u'activation': u'tanh', u'embedding_output_di...  0.926688  0.791033   \n",
       "28  {u'activation': u'tanh', u'embedding_output_di...  0.930441  0.799368   \n",
       "29  {u'activation': u'tanh', u'embedding_output_di...  0.931942  0.792575   \n",
       "30  {u'activation': u'tanh', u'embedding_output_di...  0.910715  0.759016   \n",
       "31  {u'activation': u'tanh', u'embedding_output_di...  0.933126  0.799380   \n",
       "32  {u'activation': u'tanh', u'embedding_output_di...  0.921516  0.787185   \n",
       "33  {u'activation': u'tanh', u'embedding_output_di...  0.906639  0.766486   \n",
       "34  {u'activation': u'tanh', u'embedding_output_di...  0.912352  0.772036   \n",
       "35  {u'activation': u'tanh', u'embedding_output_di...  0.908988  0.759274   \n",
       "36  {u'activation': u'tanh', u'embedding_output_di...  0.906781  0.770865   \n",
       "37  {u'activation': u'tanh', u'embedding_output_di...  0.915715  0.781955   \n",
       "38  {u'activation': u'tanh', u'embedding_output_di...  0.916830  0.791078   \n",
       "39  {u'activation': u'tanh', u'embedding_output_di...  0.903446  0.757644   \n",
       "40  {u'activation': u'tanh', u'embedding_output_di...  0.930061  0.800940   \n",
       "41  {u'activation': u'tanh', u'embedding_output_di...  0.939081  0.820827   \n",
       "42  {u'activation': u'tanh', u'embedding_output_di...  0.920559  0.788754   \n",
       "43  {u'activation': u'tanh', u'embedding_output_di...  0.934271  0.795012   \n",
       "44  {u'activation': u'tanh', u'embedding_output_di...  0.933175  0.807541   \n",
       "45  {u'activation': u'tanh', u'embedding_output_di...  0.922263  0.790845   \n",
       "46  {u'activation': u'tanh', u'embedding_output_di...  0.921551  0.785256   \n",
       "47  {u'activation': u'tanh', u'embedding_output_di...  0.932234  0.806299   \n",
       "\n",
       "    test_tau  train_auc  train_f1  train_size  train_tau  layer0_size  \n",
       "0   0.593120   0.980339  0.898028       21917   0.689418          128  \n",
       "1   0.618440   0.959210  0.850818       21917   0.636077            4  \n",
       "2   0.609103   0.951196  0.835620       21917   0.612900            4  \n",
       "3   0.573385   0.993607  0.942386       21917   0.756294          128  \n",
       "4   0.594130   0.942209  0.800302       21917   0.595929            4  \n",
       "5   0.608516   0.966531  0.863958       21917   0.651938            4  \n",
       "6   0.610973   0.951413  0.838246       21917   0.614779          128  \n",
       "7   0.598405   0.944210  0.814146       21917   0.600546          128  \n",
       "8   0.565897   0.993560  0.944716       21917   0.764580          128  \n",
       "9   0.593166   0.942652  0.796619       21917   0.605306          128  \n",
       "10  0.599184   0.963767  0.854192       21917   0.652449            4  \n",
       "11  0.605044   0.950763  0.828168       21917   0.620571          128  \n",
       "12  0.589987   0.941037  0.803225       21917   0.601807            4  \n",
       "13  0.604458   0.951944  0.834692       21917   0.620054            4  \n",
       "14  0.601365   0.974023  0.880309       21917   0.677204          128  \n",
       "15  0.612538   0.958561  0.846869       21917   0.641459            4  \n",
       "16  0.612703   0.960931  0.851591       21918   0.632399            4  \n",
       "17  0.611002   0.953054  0.833251       21918   0.616235            4  \n",
       "18  0.571184   0.993622  0.945570       21918   0.755682          128  \n",
       "19  0.606301   0.976603  0.887770       21918   0.676646          128  \n",
       "20  0.601706   0.944059  0.805556       21918   0.600343          128  \n",
       "21  0.613119   0.953064  0.831233       21918   0.617038          128  \n",
       "22  0.607308   0.967210  0.866100       21918   0.650748            4  \n",
       "23  0.598530   0.944287  0.801827       21918   0.599491            4  \n",
       "24  0.614793   0.917251  0.782676       13252   0.606821            4  \n",
       "25  0.615366   0.916913  0.783920       13252   0.606775          128  \n",
       "26  0.631430   0.928959  0.805745       13252   0.626517          128  \n",
       "27  0.617045   0.959783  0.846332       13252   0.683477            4  \n",
       "28  0.631454   0.929875  0.800000       13252   0.627758            4  \n",
       "29  0.628262   0.947986  0.831452       13252   0.657255            4  \n",
       "30  0.584572   0.989722  0.929533       13252   0.781838          128  \n",
       "31  0.631472   0.954726  0.847165       13252   0.678256          128  \n",
       "32  0.627735   0.949147  0.830562       13253   0.661833            4  \n",
       "33  0.599493   0.924185  0.790024       13253   0.614392          128  \n",
       "34  0.603913   0.959813  0.853630       13253   0.678649            4  \n",
       "35  0.598688   0.974737  0.868954       13253   0.717725          128  \n",
       "36  0.599694   0.924163  0.795285       13253   0.615065            4  \n",
       "37  0.613060   0.936135  0.812036       13253   0.634643          128  \n",
       "38  0.615471   0.936914  0.817021       13253   0.636664            4  \n",
       "39  0.590762   0.988924  0.931322       13253   0.779462          128  \n",
       "40  0.606250   0.955270  0.843373       13253   0.679339            4  \n",
       "41  0.629710   0.944735  0.829942       13253   0.661309            4  \n",
       "42  0.584453   0.988310  0.925741       13253   0.780921          128  \n",
       "43  0.620042   0.958840  0.857585       13253   0.692024          128  \n",
       "44  0.615135   0.930270  0.801413       13253   0.634529            4  \n",
       "45  0.601473   0.918310  0.782643       13253   0.615054          128  \n",
       "46  0.598595   0.915988  0.772908       13253   0.611912            4  \n",
       "47  0.615735   0.929472  0.799688       13253   0.633083          128  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "cv_df = defaultdict(list)\n",
    "start = time.time()\n",
    "\n",
    "for (allele, data) in train_data.items():\n",
    "    print(\"Allele: %s\" % allele)\n",
    "    # data_df = allele_data_to_df(data)\n",
    "    X, Y, weights, original_peptides = get_shuffled_fields(data)\n",
    "            \n",
    "    cv = sklearn.cross_validation.LabelKFold(original_peptides, n_folds = 3)\n",
    "    for (fold_num, (train_indices, test_indices)) in enumerate(cv):\n",
    "        print(\"-- fold #%d/3\" % (fold_num + 1,))\n",
    "        X_cv_train = X[train_indices]\n",
    "        X_cv_test = X[test_indices]\n",
    "        \n",
    "        Y_cv_train = Y[train_indices]\n",
    "        Y_cv_test = Y[test_indices]\n",
    "        \n",
    "        weights_cv_train = weights[train_indices]\n",
    "        weights_cv_test = weights[test_indices]\n",
    "        \n",
    "        original_peptides_train = original_peptides[train_indices]\n",
    "        original_peptides_test = original_peptides[test_indices]\n",
    "        impute = False\n",
    "        \n",
    "        np.random.shuffle(models_params_list)\n",
    "        for (i, model_params) in enumerate(models_params_list):\n",
    "            print(\"%10s fold %3d [%3d / %3d] train_size=%d test_size=%d impute=%s model=%s\" %\n",
    "                  (allele, fold_num, i, len(models_params_list), len(train_indices), len(test_indices), impute, model_params))\n",
    "            sys.stdout.flush()\n",
    "            predictor = mhcflurry.Class1BindingPredictor.from_hyperparameters(\n",
    "                max_ic50=max_ic50,\n",
    "                **model_params)\n",
    "\n",
    "            fit_time = -time.time()\n",
    "            predictor.fit(\n",
    "                X_cv_train,\n",
    "                Y_cv_train,\n",
    "                sample_weights=weights_cv_train,\n",
    "                verbose=False,\n",
    "                batch_size=128,\n",
    "                n_training_epochs=250)\n",
    "            fit_time += time.time()\n",
    "    \n",
    "            Y_cv_train_9mer_predictions = predictor.predict(X_cv_train)\n",
    "            Y_cv_test_9mer_predictions = predictor.predict(X_cv_test)\n",
    "            \n",
    "            Y_train_true, Y_train_pred = collapse_9mer_affinities(\n",
    "                Y_9mer_true=Y_cv_train,\n",
    "                Y_9mer_pred=Y_cv_train_9mer_predictions,\n",
    "                original_peptides=original_peptides_train)\n",
    "            \n",
    "            Y_test_true, Y_test_pred = collapse_9mer_affinities(\n",
    "                Y_9mer_true=Y_cv_test,\n",
    "                Y_9mer_pred=Y_cv_test_9mer_predictions,\n",
    "                original_peptides=original_peptides_test)\n",
    "            \n",
    "            cv_df[\"allele\"].append(allele)\n",
    "            cv_df[\"allele_size\"].append(len(Y))\n",
    "            cv_df[\"train_size\"].append(len(Y_cv_train))\n",
    "            cv_df[\"model_params\"].append(model_params)\n",
    "            cv_df[\"fit_time\"].append(fit_time)\n",
    "\n",
    "            for (param, param_value) in model_params.items():\n",
    "                cv_df[param].append(param_value)\n",
    "            \n",
    "            for (key, value) in make_scores(Y_train_true, Y_train_pred).items():\n",
    "                cv_df[\"train_%s\" % key].append(value)\n",
    "                print(\"train %s: %f\" % (key, value))\n",
    "            \n",
    "            for (key, value) in make_scores(\n",
    "                    Y_test_true, \n",
    "                    Y_test_pred).items():\n",
    "                cv_df[\"test_%s\" % key].append(value)\n",
    "                print(\"test %s: %f\" % (key, value))\n",
    "\n",
    "\n",
    "cv_df = pandas.DataFrame(cv_df)\n",
    "cv_df[\"layer0_size\"] = [x[0] for x in cv_df.layer_sizes]\n",
    "print(time.time() - start)\n",
    "cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_df[\"combined\"] = cv_df.test_auc + cv_df.test_f1 + cv_df.test_tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'activation', u'allele', u'allele_size', u'dropout_probability',\n",
      "       u'embedding_output_dim', u'fit_time', u'layer_sizes', u'model_params',\n",
      "       u'test_auc', u'test_f1', u'test_tau', u'train_auc', u'train_f1',\n",
      "       u'train_size', u'train_tau', u'layer0_size', u'combined'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/anaconda2/envs/standard-2.7/lib/python2.7/site-packages/ipykernel/__main__.py:9: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "cv_df_str = cv_df.copy()\n",
    "print(cv_df_str.columns)\n",
    "del cv_df_str['model_params']\n",
    "del cv_df_str['fit_time']\n",
    "\n",
    "for col in [\"layer_sizes\"]:\n",
    "    cv_df_str[col] = [str(x) for x in cv_df_str[col]]\n",
    "summary = cv_df_str.groupby(list(cv_df_str.columns[:6])).mean() #.reset_index()\n",
    "summary.sort(\"combined\", ascending=False, inplace=True)\n",
    "summary.to_csv(\"../data/cv_hla0201_summary_128.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_df.sort(\"combined\", ascending=False, inplace=True)\n",
    "cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cv_df = pandas.DataFrame(cv_df)\n",
    "cv_df[\"layer0_size\"] = [x[0] for x in cv_df.layer_sizes]\n",
    "cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cv_df.to_csv(\"cv5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "group_columns = [\"allele\", \"allele_size\", \"impute\"]\n",
    "group_columns.extend(models_params_explored)\n",
    "group_columns.append(\"layer0_size\")\n",
    "group_columns.remove(\"layer_sizes\")\n",
    "print(mean_with_std(cv_df.groupby(group_columns).test_auc)) #.sort(inplace=False, ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def best_by(score):\n",
    "    means = cv_df.groupby(group_columns)[score].mean().reset_index()\n",
    "    max_rows = []\n",
    "    for allele in means.allele.unique():\n",
    "        max_rows.append(means.ix[means.allele == allele][score].argmax())\n",
    "    return means.ix[max_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_by('test_auc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_by('test_tau')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_by('test_f1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
