{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"THEANO_FLAGS\"] = 'cuda.root=/usr/local/cuda,floatX=float32,device=gpu0,force_device=False,lib.cnmem=.75'\n",
    "\n",
    "import theano\n",
    "print(theano.config.device)\n",
    "\n",
    "import mhcflurry, seaborn, numpy, pandas, pickle, sklearn, collections, scipy, time, logging\n",
    "import mhcflurry.data\n",
    "import mhcflurry.imputation\n",
    "import fancyimpute\n",
    "\n",
    "import sklearn.metrics\n",
    "import sklearn.cross_validation\n",
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_peptides_to_consider_allele = 50\n",
    "max_ic50 = 50000\n",
    "data_dir = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_train_data = mhcflurry.data.load_allele_datasets(\n",
    "    data_dir + \"bdata.2009.mhci.public.1.txt\",\n",
    "    use_multiple_peptide_lengths=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alleles = [\n",
    "    \"HLA-A0201\",\n",
    "    # \"HLA-A0301\",\n",
    "    \"HLA-A0203\",\n",
    "    # \"HLA-A2602\",\n",
    "    # \"HLA-A2603\",\n",
    "    # 'HLA-B7301',\n",
    "]\n",
    "#alleles = alleles[:1] + alleles[-1:]\n",
    "#alleles = [allele for allele in all_train_data if len(all_train_data[allele].Y) >= min_peptides_to_consider_allele]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36749263596306014"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_data[alleles[0]].weights.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{8, 9, 10, 11, 12, 13, 14, 15}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(len(x) for x in all_train_data[alleles[0]].original_peptides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{9}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(len(x) for x in all_train_data[alleles[0]].peptides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 2 / 106 alleles\n"
     ]
    }
   ],
   "source": [
    "#train_data = dict((allele, data)\n",
    "#                  for (allele, data) in all_train_data.items()\n",
    "#                  if len(data.Y) >= min_peptides_to_consider_allele)\n",
    "train_data = dict((allele, all_train_data[allele]) for allele in alleles)\n",
    "print(\"Training data: %d / %d alleles\" % (len(train_data), len(all_train_data)))\n",
    "\n",
    "#test_data = mhcflurry.data.load_allele_datasets(\"/Users/tim/sinai/git/mhcflurry/bdata.2013.mhci.public.blind.1.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def log_to_ic50(log_value):\n",
    "        \"\"\"\n",
    "        Convert neural network output to IC50 values between 0.0 and\n",
    "        self.max_ic50 (typically 5000, 20000 or 50000)\n",
    "        \"\"\"\n",
    "        return max_ic50 ** (1.0 - log_value)\n",
    "\n",
    "def make_scores(y, y_pred, weights=None, sample_weight=None, threshold_nm=500):\n",
    "    ic50_y = log_to_ic50(y)\n",
    "    ic50_y_pred = log_to_ic50(y_pred) \n",
    "    return dict(\n",
    "        auc=sklearn.metrics.roc_auc_score(ic50_y <= threshold_nm, y_pred, sample_weight=sample_weight),\n",
    "        f1=sklearn.metrics.f1_score(ic50_y <= threshold_nm, ic50_y_pred <= threshold_nm, sample_weight=sample_weight),\n",
    "        tau=scipy.stats.kendalltau(y_pred, y)[0],\n",
    "    )    \n",
    "\n",
    "def mean_with_std(grouped_column, decimals=3):\n",
    "    pattern = \"%%0.%df\" % decimals\n",
    "    return pandas.Series([\n",
    "        (pattern + \" +/ \" + pattern) % (m, s) if not pandas.isnull(s) else pattern % m\n",
    "        for (m, s) in zip(grouped_column.mean(), grouped_column.std())\n",
    "    ], index = grouped_column.mean().index)\n",
    "\n",
    "def allele_data_to_df(data):\n",
    "    d = data._asdict()\n",
    "    d[\"X_index\"] = [x for x in d[\"X_index\"]]\n",
    "    d[\"X_binary\"] = [x for x in d[\"X_binary\"]]\n",
    "    df = pandas.DataFrame(d).set_index('peptides')\n",
    "    return df\n",
    "\n",
    "def make_2d_array(thing):\n",
    "    return numpy.array([list(x) for x in thing])\n",
    "\n",
    "def df_to_allele_data(df):\n",
    "    d = dict((col, df[col].values) for col in df)\n",
    "    d[\"X_index\"] = make_2d_array(d[\"X_index\"])\n",
    "    (d[\"max_ic50\"],) = list(df.max_ic50.unique())\n",
    "    return mhcflurry.data.AlleleData(peptides = df.index.values, **d)\n",
    "\n",
    "def get_shuffled_fields(allele_data):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    allele_data : mhcflurry.data.AlleleData\n",
    "    \n",
    "    Returns shuffled array fields\n",
    "    \"\"\"\n",
    "    original_peptides = data.original_peptides\n",
    "    X = data.X_index \n",
    "    Y = data.Y\n",
    "    weights = data.weights\n",
    "    \n",
    "    # shuffle all the  samples!\n",
    "    shuffle_indices = np.arange(len(X))\n",
    "    np.random.shuffle(shuffle_indices)\n",
    "    \n",
    "    X = X[shuffle_indices]\n",
    "    Y = Y[shuffle_indices]\n",
    "    weights = weights[shuffle_indices]\n",
    "    original_peptides = original_peptides[shuffle_indices]\n",
    "    \n",
    "    return X, Y, weights, original_peptides\n",
    "\n",
    "def collapse_9mer_affinities(Y_9mer_true, Y_9mer_pred, original_peptides):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    Y_9mer_true : np.array of float\n",
    "        True regression target values for 9mers extracted from longer/shorter peptides\n",
    "    \n",
    "    Y_9mer_pred : np.array of float\n",
    "        Predicted values for 9mers\n",
    "    \n",
    "    original_peptides : np.array of str\n",
    "        Original peptides of varying length that 9mers were extracted from.\n",
    "    \"\"\"\n",
    "    # collapse multiple 9mer predictions and measured values into \n",
    "    # smaller set of predictions for peptides of varying lengths\n",
    "    peptide_to_true_affinity_dict = defaultdict(list)\n",
    "    peptide_to_predicted_affinity_dict = defaultdict(list)\n",
    "    for i, p in enumerate(original_peptides):\n",
    "        peptide_to_true_affinity_dict[p].append(Y_9mer_true[i])\n",
    "        peptide_to_predicted_affinity_dict[p].append(Y_9mer_pred[i])\n",
    "\n",
    "    unique_peptides = list(sorted(set(peptide_to_predicted_affinity_dict.keys())))\n",
    "    print(\"-- # unique peptides = %d\" % (len(unique_peptides),))\n",
    "    Y_true = np.array([\n",
    "            np.mean(peptide_to_true_affinity_dict[p]) for p in unique_peptides ])\n",
    "    Y_pred = np.array([\n",
    "            np.mean(peptide_to_predicted_affinity_dict[p]) for p in unique_peptides])\n",
    "    return Y_true, Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 models\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'activation', 'dropout_probability', 'embedding_output_dim', 'layer_sizes'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout_probabilities = [0.0, 0.5]\n",
    "\n",
    "embedding_output_dims = [8, 32]\n",
    "#embedding_output_dims = [4, 32]\n",
    "\n",
    "#layer_sizes = [[4], [8], [16], [64], [128]]\n",
    "layer_sizes_list = [[4], [64]]\n",
    "\n",
    "activations = [\"tanh\"]\n",
    "\n",
    "models_params_list = []\n",
    "for dropout_probability in dropout_probabilities:\n",
    "    for embedding_output_dim in embedding_output_dims:\n",
    "        for layer_sizes in layer_sizes_list:\n",
    "            for activation in activations:\n",
    "                models_params_list.append(dict(\n",
    "                    dropout_probability=dropout_probability,  \n",
    "                    embedding_output_dim=embedding_output_dim,\n",
    "                    layer_sizes=layer_sizes,\n",
    "                    activation=activation))\n",
    "\n",
    "print(\"%d models\" % len(models_params_list))\n",
    "models_params_explored = set.union(*[set(x) for x in models_params_list])\n",
    "models_params_explored\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allele: HLA-A0201\n",
      "-- fold #1/3\n",
      " HLA-A0201 fold   0 [  0 /   8] train_size=21917 test_size=10959 impute=False model={'activation': 'tanh', 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "-- # unique peptides = 6377\n",
      "-- # unique peptides = 3188\n",
      "train tau: 0.613550\n",
      "train auc: 0.951816\n",
      "train f1: 0.835915\n",
      "test tau: 0.609978\n",
      "test auc: 0.946921\n",
      "test f1: 0.815041\n",
      " HLA-A0201 fold   0 [  1 /   8] train_size=21917 test_size=10959 impute=False model={'activation': 'tanh', 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "-- # unique peptides = 6377\n",
      "-- # unique peptides = 3188\n",
      "train tau: 0.651792\n",
      "train auc: 0.966812\n",
      "train f1: 0.867430\n",
      "test tau: 0.615680\n",
      "test auc: 0.947475\n",
      "test f1: 0.822502\n",
      " HLA-A0201 fold   0 [  2 /   8] train_size=21917 test_size=10959 impute=False model={'activation': 'tanh', 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "-- # unique peptides = 6377\n",
      "-- # unique peptides = 3188\n",
      "train tau: 0.599223\n",
      "train auc: 0.943256\n",
      "train f1: 0.815050\n",
      "test tau: 0.596902\n",
      "test auc: 0.938335\n",
      "test f1: 0.802254\n",
      " HLA-A0201 fold   0 [  3 /   8] train_size=21917 test_size=10959 impute=False model={'activation': 'tanh', 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n",
      "-- # unique peptides = 6377\n",
      "-- # unique peptides = 3188\n",
      "train tau: 0.634809\n",
      "train auc: 0.960581\n",
      "train f1: 0.854858\n",
      "test tau: 0.617178\n",
      "test auc: 0.948313\n",
      "test f1: 0.820513\n",
      " HLA-A0201 fold   0 [  4 /   8] train_size=21917 test_size=10959 impute=False model={'activation': 'tanh', 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "-- # unique peptides = 6377\n",
      "-- # unique peptides = 3188\n",
      "train tau: 0.743866\n",
      "train auc: 0.992378\n",
      "train f1: 0.939870\n",
      "test tau: 0.574697\n",
      "test auc: 0.924848\n",
      "test f1: 0.786220\n",
      " HLA-A0201 fold   0 [  5 /   8] train_size=21917 test_size=10959 impute=False model={'activation': 'tanh', 'embedding_output_dim': 32, 'dropout_probability': 0.5, 'layer_sizes': [64]}\n",
      "-- # unique peptides = 6377\n",
      "-- # unique peptides = 3188\n",
      "train tau: 0.614747\n",
      "train auc: 0.951539\n",
      "train f1: 0.835850\n",
      "test tau: 0.611162\n",
      "test auc: 0.946422\n",
      "test f1: 0.819820\n",
      " HLA-A0201 fold   0 [  6 /   8] train_size=21917 test_size=10959 impute=False model={'activation': 'tanh', 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "-- # unique peptides = 6377\n",
      "-- # unique peptides = 3188\n",
      "train tau: 0.670543\n",
      "train auc: 0.974431\n",
      "train f1: 0.885944\n",
      "test tau: 0.604751\n",
      "test auc: 0.940551\n",
      "test f1: 0.798625\n",
      " HLA-A0201 fold   0 [  7 /   8] train_size=21917 test_size=10959 impute=False model={'activation': 'tanh', 'embedding_output_dim': 8, 'dropout_probability': 0.5, 'layer_sizes': [4]}\n",
      "-- # unique peptides = 6377\n",
      "-- # unique peptides = 3188\n",
      "train tau: 0.594933\n",
      "train auc: 0.942468\n",
      "train f1: 0.787957\n",
      "test tau: 0.593855\n",
      "test auc: 0.936922\n",
      "test f1: 0.769978\n",
      "-- fold #2/3\n",
      " HLA-A0201 fold   1 [  0 /   8] train_size=21917 test_size=10959 impute=False model={'activation': 'tanh', 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "-- # unique peptides = 6376\n",
      "-- # unique peptides = 3189\n",
      "train tau: 0.743463\n",
      "train auc: 0.989781\n",
      "train f1: 0.931043\n",
      "test tau: 0.562648\n",
      "test auc: 0.928437\n",
      "test f1: 0.802953\n",
      " HLA-A0201 fold   1 [  1 /   8] train_size=21917 test_size=10959 impute=False model={'activation': 'tanh', 'embedding_output_dim': 8, 'dropout_probability': 0.0, 'layer_sizes': [64]}\n",
      "-- # unique peptides = 6376\n",
      "-- # unique peptides = 3189\n",
      "train tau: 0.666998\n",
      "train auc: 0.970110\n",
      "train f1: 0.869776\n",
      "test tau: 0.603703\n",
      "test auc: 0.949141\n",
      "test f1: 0.835610\n",
      " HLA-A0201 fold   1 [  2 /   8] train_size=21917 test_size=10959 impute=False model={'activation': 'tanh', 'embedding_output_dim': 32, 'dropout_probability': 0.0, 'layer_sizes': [4]}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "cv_df = defaultdict(list)\n",
    "start = time.time()\n",
    "\n",
    "for (allele, data) in train_data.items():\n",
    "    print(\"Allele: %s\" % allele)\n",
    "    # data_df = allele_data_to_df(data)\n",
    "    X, Y, weights, original_peptides = get_shuffled_fields(data)\n",
    "            \n",
    "    cv = sklearn.cross_validation.LabelKFold(original_peptides, n_folds = 3)\n",
    "    for (fold_num, (train_indices, test_indices)) in enumerate(cv):\n",
    "        print(\"-- fold #%d/3\" % (fold_num + 1,))\n",
    "        X_cv_train = X[train_indices]\n",
    "        X_cv_test = X[test_indices]\n",
    "        \n",
    "        Y_cv_train = Y[train_indices]\n",
    "        Y_cv_test = Y[test_indices]\n",
    "        \n",
    "        weights_cv_train = weights[train_indices]\n",
    "        weights_cv_test = weights[test_indices]\n",
    "        \n",
    "        original_peptides_train = original_peptides[train_indices]\n",
    "        original_peptides_test = original_peptides[test_indices]\n",
    "        impute = False\n",
    "        \n",
    "        np.random.shuffle(models_params_list)\n",
    "        for (i, model_params) in enumerate(models_params_list):\n",
    "            print(\"%10s fold %3d [%3d / %3d] train_size=%d test_size=%d impute=%s model=%s\" %\n",
    "                  (allele, fold_num, i, len(models_params_list), len(train_indices), len(test_indices), impute, model_params))\n",
    "            sys.stdout.flush()\n",
    "            predictor = mhcflurry.Class1BindingPredictor.from_hyperparameters(\n",
    "                max_ic50=max_ic50,\n",
    "                **model_params)\n",
    "\n",
    "            fit_time = -time.time()\n",
    "            predictor.fit(\n",
    "                X_cv_train,\n",
    "                Y_cv_train,\n",
    "                sample_weights=weights_cv_train,\n",
    "                verbose=False,\n",
    "                batch_size=128,\n",
    "                n_training_epochs=250)\n",
    "            fit_time += time.time()\n",
    "    \n",
    "            Y_cv_train_9mer_predictions = predictor.predict(X_cv_train)\n",
    "            Y_cv_test_9mer_predictions = predictor.predict(X_cv_test)\n",
    "            \n",
    "            Y_train_true, Y_train_pred = collapse_9mer_affinities(\n",
    "                Y_9mer_true=Y_cv_train,\n",
    "                Y_9mer_pred=Y_cv_train_9mer_predictions,\n",
    "                original_peptides=original_peptides_train)\n",
    "            \n",
    "            Y_test_true, Y_test_pred = collapse_9mer_affinities(\n",
    "                Y_9mer_true=Y_cv_test,\n",
    "                Y_9mer_pred=Y_cv_test_9mer_predictions,\n",
    "                original_peptides=original_peptides_test)\n",
    "            \n",
    "            cv_df[\"allele\"].append(allele)\n",
    "            cv_df[\"allele_size\"].append(len(Y))\n",
    "            cv_df[\"train_size\"].append(len(Y_cv_train))\n",
    "            cv_df[\"model_params\"].append(model_params)\n",
    "            cv_df[\"fit_time\"].append(fit_time)\n",
    "\n",
    "            for (param, param_value) in model_params.items():\n",
    "                cv_df[param].append(param_value)\n",
    "            \n",
    "            for (key, value) in make_scores(Y_train_true, Y_train_pred).items():\n",
    "                cv_df[\"train_%s\" % key].append(value)\n",
    "                print(\"train %s: %f\" % (key, value))\n",
    "            \n",
    "            for (key, value) in make_scores(\n",
    "                    Y_test_true, \n",
    "                    Y_test_pred).items():\n",
    "                cv_df[\"test_%s\" % key].append(value)\n",
    "                print(\"test %s: %f\" % (key, value))\n",
    "\n",
    "\n",
    "cv_df = pandas.DataFrame(cv_df)\n",
    "cv_df[\"layer0_size\"] = [x[0] for x in cv_df.layer_sizes]\n",
    "print(time.time() - start)\n",
    "cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_df_str = cv_df.copy()\n",
    "print(cv_df_str.columns)\n",
    "del cv_df_str['model_params']\n",
    "del cv_df_str['fit_time']\n",
    "\n",
    "for col in [\"layer_sizes\"]:\n",
    "    cv_df_str[col] = [str(x) for x in cv_df_str[col]]\n",
    "summary = cv_df_str.groupby(list(cv_df_str.columns[:6])).mean() #.reset_index()\n",
    "summary.sort(\"combined\", ascending=False, inplace=True)\n",
    "summary.to_csv(\"../data/cv_hla0201_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_df[\"combined\"] = cv_df.test_auc + cv_df.test_f1 + cv_df.test_tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_df.sort(\"combined\", ascending=False, inplace=True)\n",
    "cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cv_df = pandas.DataFrame(cv_df)\n",
    "cv_df[\"layer0_size\"] = [x[0] for x in cv_df.layer_sizes]\n",
    "cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cv_df.to_csv(\"cv5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "group_columns = [\"allele\", \"allele_size\", \"impute\"]\n",
    "group_columns.extend(models_params_explored)\n",
    "group_columns.append(\"layer0_size\")\n",
    "group_columns.remove(\"layer_sizes\")\n",
    "print(mean_with_std(cv_df.groupby(group_columns).test_auc)) #.sort(inplace=False, ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def best_by(score):\n",
    "    means = cv_df.groupby(group_columns)[score].mean().reset_index()\n",
    "    max_rows = []\n",
    "    for allele in means.allele.unique():\n",
    "        max_rows.append(means.ix[means.allele == allele][score].argmax())\n",
    "    return means.ix[max_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_by('test_auc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_by('test_tau')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_by('test_f1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
