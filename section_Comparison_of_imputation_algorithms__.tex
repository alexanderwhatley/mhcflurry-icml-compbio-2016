\section{Comparison of imputation algorithms as predictors}

A dataset of peptide-MHC affinities for $n$ peptides and $a$ alleles may be thought of as a $n \times a$ matrix where peptide/allele pairs without measurements are missing values. The task of predicting values at these positions is known as matrix completion, and various heuristic and optimization-based algorithms have been proposed. We first investigated the performance of several matrix completion algorithms as a standalone solution to the peptide-MHC affinity prediction problem. The algorithms considered were:

\begin{itemize}
\item {\bf meanFill}: Replace each missing pMHC binding affinity with the mean affinity for that allele. This is a very simple imputation method which serves as a baseline against which other methods can be compared. 

\item {\bf knnImpute}~\cite{Troyanskaya_2001}: Each missing entry $X_{ij}$ is imputed using the values in the $k$ closest columns with observation in row $i$.  Similarity between alleles is computed as $e^{-d_{st}^2}$, where $d_{st}$ is the mean squared difference between observed entries of alleles $s$ and $t$. 

\item {\bf svdImpute}~\cite{Troyanskaya_2001}: Imputation using iterative fixed rank SVD decomposition. 

\item {\bf softImpute}~\cite{Mazumder2010SpectralMatrices}: A singular value thresholding method which iteratively estimates a low-rank matrix completion without forcing the pre-specification of a particular solution rank. Instead, the {\it softImpute} method is parameterized by a shrinkage value $\lambda$ that is subtracted from each singular value. 

\item {\bf MICE}~\cite{Azur_2011}: Average multiple imputations generated using Gibbs sampling from the joint distribution of columns. 
\end{itemize}

We evaluated the performance of these methods using three-fold cross validation on BD2009 (~\ref{tab:imputation}), only considering peptides which occurred in at least three alleles and excluding alleles with less than five measurements. All imputation methods were implemented in the \textit{fancyimpute} Python library~\cite{fancyimpute-0-0-16}. Since MICE outperformed the other methods on two of the three predictor metrics, we selected it for the subsequent neural network experiments.

\begin{table}[htbp]
\centering
\begin{tabular}{cl||ccc}
\toprule
Imputation Method & Parameter & AUC & $F_1$ score & Kendall's $\tau$ \\
\midrule 
meanFill &  0.676654 &  0.049500 &  0.176754 \\
\midrule
knnImpute & $k = 1$ &  0.809074 &  0.579518 &  0.402011 \\
  & $k = 3$  &  0.831888 &  0.575937 &  0.420860 \\
  & $k = 5$ &  0.831026 &  0.561185 &  0.417029 \\
\midrule
MICE & $n = 25$  &  0.858611 &  0.575969 &  0.449778 \\
MICE & $n = 50$  &  0.861272 &  0.565275 &  0.459442 \\
\midrule
softImpute & $\lambda=5$ &  0.789808 &  0.391585 &  0.334080 \\
& $\lambda=10$ &  0.832483 &  0.535746 &  0.397632 \\
& $\lambda=20$ &  0.856079 &  0.605987 &  0.437538 \\
             
\midrule
svdImpute & rank = 5  &  0.823050 &  0.570396 &  0.391171 \\
& rank = 10  &  0.836672 &  0.584327 &  0.400482 \\
& rank = 20  &  0.829862 &  0.570383 &  0.388172 \\
\bottomrule[1.25pt]
\end{tabular}
\begin{center}
\caption{Cross validation performance of several matrix completion algorithms on BD2009} \label{tab:imputation}
\end{center}
\end{table}

