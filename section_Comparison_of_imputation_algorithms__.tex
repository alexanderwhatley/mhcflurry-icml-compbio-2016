\section{Comparison of imputation algorithms as predictors}

A dataset of peptide-MHC affinities for $n$ peptides and $a$ alleles may be thought of as a $n \times a$ matrix where peptide/allele pairs without measurements are missing values. The task of predicting values at these positions is known as matrix completion or imputation (depending on the community and data source).  We investigated the performance of several imputation algorithms as a standalone solution to the peptide-MHC affinity prediction problem. The algorithms considered were:

\begin{itemize}
\item {\bf meanFill}: Replace each missing pMHC binding affinity with the mean affinity for that allele. This is a very simple imputation method which serves as a baseline against which other methods can be compared. 

\item {\bf knnImpute}~\cite{Troyanskaya_2001}: Each missing entry $X_{ij}$ is imputed using the values in the $k$ closest columns with observation in row $i$.  Similarity between alleles is computed as $e^{-d_{st}^2}$, where $d_{st}$ is the mean squared difference between observed entries of alleles $s$ and $t$. 

\item {\bf svdImpute}~\cite{Troyanskaya_2001}: Imputation using iterative fixed rank SVD decomposition. 

\item {\bf softImpute}~\cite{Mazumder2010SpectralMatrices}: A singular value thresholding method which iteratively estimates a low-rank matrix completion without forcing the pre-specification of a particular solution rank. Instead, the {\it softImpute} method is parameterized by a shrinkage value $\lambda$ that is subtracted from each singular value. 

\item {\bf MICE}~\cite{Azur_2011}: Average multiple imputations generated using Gibbs sampling from the joint distribution of columns. 
\end{itemize}

We evaluated the performance of these methods using three-fold cross validation on BD2009, only considering peptides which occurred in at least three alleles and excluding alleles with less than five measurements (Table~\ref{tab:imputation}). All imputation methods were implemented in the \textit{fancyimpute} Python library~\cite{fancyimpute-0-0-16}. Since MICE outperformed the other methods on two of the three predictor metrics, we selected it for the subsequent neural network experiments.

\begin{table}[htbp]
\centering
\begin{tabular}{cl||ccc}
\toprule
Imputation Method & Parameter & AUC & $F_1$ score & Kendall's $\tau$ \\
\midrule 
meanFill & &   0.67665 &  0.04950 &  0.17675 \\
\midrule
knnImpute & $k = 1$ &  0.80907 &  0.57952 &  0.40201 \\
  & $k = 3$  &  0.83189 &  0.57594 &  0.42086 \\
  & $k = 5$ &  0.83103 &  0.56118 &  0.41703 \\
\midrule
MICE & $n = 25$  &  0.85861 &  0.57597 &  0.44978 \\
     & $n = 50$  &  {\bf 0.86127} &  0.56527 &  {\bf 0.45944} \\
\midrule
softImpute & $\lambda=5$ &  0.78981 &  0.39158 &  0.33408 \\
& $\lambda=10$ &  0.83248 &  0.53575 &  0.39763 \\
& $\lambda=20$ &  0.85608 &  {\bf 0.60599} &  0.43754 \\
             
\midrule
svdImpute & rank = 5  &  0.82305 &  0.57040 &  0.39117 \\
& rank = 10  &  0.83667 &  0.58433 &  0.40048 \\
& rank = 20  &  0.82986 &  0.57038 &  0.38817 \\
\bottomrule[1.25pt]
\end{tabular}
\begin{center}
\caption{Cross-validation performance of imputation algorithms on BD2009 dataset} \label{tab:imputation}
\end{center}
\end{table}

