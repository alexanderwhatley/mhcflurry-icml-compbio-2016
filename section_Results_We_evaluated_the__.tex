\section{Results}
We evaluated the effect of imputation, using only the training set, by down-sampling the three alleles with the most training data to a range of simulated training set sizes and testing on the remaining data (Figure ~\ref{fig:imputecomparison}). We find that imputation gives a modest improvement up to approximately 100 training samples. With more training data there is no benefit to imputation. The results are similar for the two other performance metrics (not shown).

% The figure doesn't show up in the preview but does show up if you export to pdf.
\begin{figure}[hb]
\includegraphics{figures/impute_comparison.pdf}
\caption{\label{fig:imputecomparison} MHCflurry performance on down-sampled training data with and without imputation}
\end{figure}

We compared the performance of two MHCFlurry-based models, ``mhcflurry ensemble'' and ``mhcflurry single,'' against netMHC, netMHCpan, and smmpmbec on the blind test data. The ``mhcflurry single'' model is one predictor with the architecture described previously. The ``MHCFlurry ensemble'' model is an ensemble of 10 predictors, each identical to the single predictor with different random initial weights.

\begin{table}[hr]
\centering
\begin{tabular}{llll}
\toprule
{} &               AUC &       $F_1$ score &  Kendall's $\tau$ \\
\midrule
MHCflurry ensemble            &           0.93260 &           0.78459 &  \textbf{0.58686} \\
MHCflurry single              &           0.93225 &           0.78106 &           0.58572 \\
NetMHC                        &           0.93234 &  \textbf{0.80722} &           0.58633 \\
NetMHCpan                     &  \textbf{0.93264} &           0.79957 &           0.58138 \\
SMM-PMBEC                     &           0.92134 &           0.79026 &           0.56488 \\
\bottomrule
\end{tabular}

\caption{Performance on BLIND}
\label{tab:measurementweighted}
\end{table}


The MHCFlurry ensemble predictor is competitive with NetMHC, but slightly worse than NetMHCpan. After running these experiments, we realized that the BD2009 / BLIND train and test datasets do not contain any alleles with fewer than 200 training observations. Since imputation only seems to help for alleles with fewer than 100 observations, this benchmark may not significantly benefit from the imputation approach.

\section{Future Work}
Imputing training data shows promise in cross-validation as a way to improve performance on alleles with few observations, but only seems to help for very small training sizes ($\leq 100$), not the alleles in the intermediate regime of $>200$ present in the BLIND benchmark dataset. Further work is required to assess the accuracy of MHCFlurry and other predictors on alleles with very few training examples.


% These are generated in the 'paper plots' notebook; do not edit by hand.
